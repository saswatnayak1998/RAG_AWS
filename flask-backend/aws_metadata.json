[{"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=Landing%20Pages&topic_url=https://docs.aws.amazon.com/", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=EC2&topic_url=https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "What is Amazon EC2? - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/index.html", "content": "What is Amazon EC2? PDF RSS Amazon Elastic Compute Cloud (Amazon EC2) provides on-demand, scalable computing capacity in the Amazon Web\n\t\tServices (AWS) Cloud. Using Amazon EC2 reduces hardware costs so you can develop and deploy\n\t\tapplications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you\n\t\tneed, configure security and networking, and manage storage. You can add capacity (scale up)\n\t\tto handle compute-heavy tasks, such as monthly or yearly processes, or spikes in website\n\t\ttraffic. When usage decreases, you can reduce capacity (scale down) again. An EC2 instance is a virtual server in the AWS Cloud. When you launch an EC2 instance,\n    \tthe instance type that you specify determines the hardware available to your instance. \n    \tEach instance type offers a different balance of compute, memory, network, and storage \n    \tresources. For more information, see the Amazon EC2 Instance Types Guide . Features of Amazon EC2 Amazon EC2 provides the following high-level features: Instances Virtual servers. Amazon Machine Images (AMIs) Preconfigured templates for your instances that package the components you\n\t\t\t\t\t\tneed for your server (including the operating system and additional\n\t\t\t\t\t\tsoftware). Instance types Various configurations of CPU, memory, storage, networking capacity, and\n\t\t\t\t\t\tgraphics hardware for your instances. Amazon EBS volumes Persistent storage volumes for your data using Amazon Elastic Block Store (Amazon EBS). Instance store volumes Storage volumes for temporary data that is deleted when you stop,\n\t\t\t\t\t\thibernate, or terminate your instance. Key pairs Secure login information for your instances. AWS stores the public key\n\t\t\t\t\t\tand you store the private key in a secure place. Security groups A virtual firewall that allows you to specify the protocols, ports, and\n\t\t\t\t\t\tsource IP ranges that can reach your instances, and the destination IP\n\t\t\t\t\t\tranges to which your instances can connect. Amazon EC2 supports the processing, storage, and transmission \nof credit card data by a merchant or service provider, and has been \nvalidated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). \nFor more information about PCI DSS, including how to request a copy of the AWS PCI Compliance Package, \nsee PCI DSS Level 1 . Related services Services to use with Amazon EC2 You can use other AWS services with the instances that you deploy using Amazon EC2. Amazon EC2 Auto Scaling Helps ensure you have the correct number of Amazon EC2 instances available to\n        \t\t\t\thandle the load for your application. AWS Backup Automate backing up your Amazon EC2 instances and the Amazon EBS volumes attached to\n\t\t\t\t\t\tthem. Amazon CloudWatch Monitor your instances and Amazon EBS volumes. Elastic Load Balancing Automatically distribute incoming application traffic across multiple\n\t\t\t\t\t\tinstances. Amazon GuardDuty Detect potentially unauthorized or malicious use of your EC2 instances. EC2 Image Builder Automate the creation, management, and deployment of customized, secure, and\n\t\t\t\t\t\tup-to-date server images. AWS Launch Wizard Size, configure, and deploy AWS resources for third-party applications\n\t\t\t\t\t\twithout having to manually identify and provision individual AWS\n\t\t\t\t\t\tresources. AWS Systems Manager Perform operations at scale on EC2 instances with this secure end-to-end\n\t\t\t\t\t\tmanagement solution. Additional compute services You can launch instances using another AWS compute service instead of using Amazon EC2. Amazon Lightsail Build websites or web applications using Amazon Lightsail, a cloud platform\n\t\t\t\t\t\tthat provides the resources that you need to deploy your project quickly, for\n\t\t\t\t\t\ta low, predictable monthly price. To compare Amazon EC2 and Lightsail, see Amazon Lightsail or Amazon EC2 . Amazon Elastic Container Service (Amazon ECS) Deploy, manage, and scale containerized applications on a cluster of EC2\n\t\t\t\t\t\tinstances. For more information, see Choosing an AWS container service . Amazon Elastic Kubernetes Service (Amazon EKS) Run your Kubernetes applications on AWS.  For more information, see Choosing an AWS container service . Access Amazon EC2 You can create and manage your Amazon EC2 instances using the following interfaces: Amazon EC2 console A simple web interface to create and manage Amazon EC2 instances and resources.\n\t\t\t\t\t\tIf you've signed up for an AWS account, you can access the Amazon EC2 console\n\t\t\t\t\t\tby signing into the AWS Management Console and selecting EC2 from\n\t\t\t\t\t\tthe console home page. AWS Command Line Interface Enables you to interact with AWS services using commands in your command-line shell. It\n\t\t\t\t\t\tis supported on Windows, Mac, and Linux. For more information about the\n\t\t\t\t\t\tAWS CLI , see AWS Command Line Interface User Guide . You can find the Amazon EC2 commands in the AWS CLI Command Reference . AWS CloudFormation Amazon EC2 supports creating resources using AWS CloudFormation. You create a template, in JSON or YAML\n\t\t\t\t\t\tformat, that describes your AWS resources, and AWS CloudFormation provisions and\n\t\t\t\t\t\tconfigures those resources for you. You can reuse your CloudFormation\n\t\t\t\t\t\ttemplates to provision the same resources multiple times, whether in the\n\t\t\t\t\t\tsame Region and account or in multiple Regions and accounts. For more\n\t\t\t\t\t\tinformation about supported resource types and properties for Amazon EC2, see EC2 resource type\n\t\t\t\t\t\t\treference in the AWS CloudFormation User Guide . AWS SDKs If you prefer to build applications using language-specific APIs instead\n\t\t\t\t\t\tof submitting a request over HTTP or HTTPS, AWS provides libraries, sample\n\t\t\t\t\t\tcode, tutorials, and other resources for software developers. These\n\t\t\t\t\t\tlibraries provide basic functions that automate tasks such as\n\t\t\t\t\t\tcryptographically signing your requests, retrying requests, and handling\n\t\t\t\t\t\terror responses, making it easier for you to get started. For more\n\t\t\t\t\t\tinformation, see Tools to Build\n\t\t\t\t\t\t\t\ton AWS . AWS Tools for PowerShell A set of PowerShell modules that are built on the functionality exposed by\n\t\t\t\t\t\tthe AWS SDK for .NET. The Tools for PowerShell enable you to script operations on your AWS\n\t\t\t\t\t\tresources from the PowerShell command line. To get started, see the AWS Tools for Windows PowerShell User Guide . You can find the cmdlets for Amazon EC2, in the AWS Tools for PowerShell Cmdlet Reference . Query API Amazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that\n\t\t\t\t\t\tuse the HTTP verbs GET or POST and a Query parameter named Action . For more information about the API actions for\n\t\t\t\t\t\tAmazon EC2, see Actions in the Amazon EC2 API Reference . Pricing for Amazon EC2 Amazon EC2 provides the following pricing options: Free Tier You can get started with Amazon EC2 for free. To explore the Free Tier options,\n\t\t\t\t\t\tsee AWS Free Tier . On-Demand Instances Pay for the instances that you use by the second, with a minimum of 60\n\t\t\t\t\t\tseconds, with no long-term commitments or upfront payments. Savings Plans You can reduce your Amazon EC2 costs by making a commitment to a consistent\n\t\t\t\t\t\tamount of usage, in USD per hour, for a term of 1 or 3 years. Reserved Instances You can reduce your Amazon EC2 costs by making a commitment to a specific\n\t\t\t\t\t\tinstance configuration, including instance type and Region, for a term of 1\n\t\t\t\t\t\tor 3 years. Spot Instances Request unused EC2 instances, which can reduce your Amazon EC2 costs\n\t\t\t\t\t\tsignificantly. Dedicated Hosts Reduce costs by using a physical EC2 server that is fully dedicated for\n\t\t\t\t\t\tyour use, either On-Demand or as part of a Savings Plan. You can use your\n\t\t\t\t\t\texisting server-bound software licenses and get help meeting compliance\n\t\t\t\t\t\trequirements. On-Demand Capacity Reservations Reserve compute capacity for your EC2 instances in a specific Availability\n\t\t\t\t\t\tZone for any duration of time. Per-second billing Removes the cost of unused minutes and seconds from your bill. For a complete list of charges and prices for Amazon EC2 and more information about the purchase\n\t\t\tmodels, see Amazon EC2 pricing . Estimates, billing, and cost\n\t\t\t\t\toptimization To create estimates for your AWS use cases, use the AWS Pricing Calculator . To estimate the cost of transforming Microsoft\n\t\t\t\t\tworkloads to a modern architecture that uses open source and\n\t\t\t\tcloud-native services deployed on AWS, use the AWS\n\t\t\t\t\tModernization Calculator for Microsoft Workloads . To see your bill, go to the Billing and Cost Management\n\t\t\t\t\tDashboard in the AWS Billing and Cost Management\n\t\t\t\t\tconsole . Your bill contains links to usage reports that provide details\n\t\t\t\tabout your bill. To learn more about AWS account billing, see AWS Billing and Cost Management User\n\t\t\t\tGuide . If you have questions concerning AWS billing, accounts, and events, contact AWS Support . To calculate the cost of a sample provisioned\n\t\t\t\t\tenvironment, see Cloud Economics\n\t\t\t\t\t\tCenter . When calculating the cost of a provisioned\n\t\t\t\tenvironment, remember to include incidental costs such as snapshot storage for EBS\n\t\t\t\tvolumes. You can optimize the cost, security, and performance of your AWS environment\n\t\t\t\tusing AWS Trusted Advisor . You can use AWS Cost Explorer to analyze the cost and usage of your EC2 instances. You can view \n\t\t\t\tdata up to the last 13 months, and forecast how much you are likely to spend for the next \n\t\t\t\t12 months. For more information, see Analyzing your costs with\n\t\t\t\t\tAWS Cost Explorer in the AWS Cost Management User Guide . Resources Amazon EC2 features AWS re:Post AWS Skill Builder AWS Support Hands-on Tutorials Web Hosting Windows on AWS Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Get started tutorial"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-ug.pdf", "content": "No main content found."}, {"title": "Amazon EC2 instance types - Amazon EC2", "url": "https://docs.aws.amazon.com/ec2/latest/instancetypes/index.html", "content": "Amazon EC2 instance types PDF RSS When you launch an EC2 instance, the instance type that you specify\n        determines the hardware of the host computer used for your instance. Each instance type\n        offers different compute, memory, and storage capabilities, and is grouped in an instance\n        family based on these capabilities. Select an instance type based on the requirements of the\n        application or software that you plan to run on your instance. Amazon EC2 dedicates some resources of the host computer, such as CPU, memory, and instance\n        storage, to a particular instance. Amazon EC2 shares other resources of the host computer, such\n        as the network and the disk subsystem, among instances. If each instance on a host computer\n        tries to use as much of one of these shared resources as possible, each receives an equal\n        share of that resource. However, when a resource is underused, an instance can consume a\n        higher share of that resource while it's available. Each instance type provides higher or lower minimum performance from a shared resource.\n        For example, instance types with high I/O performance have a larger allocation of shared resources. \n        Allocating a larger share of shared resources also reduces the variance of I/O performance. \n        For most applications, moderate I/O performance is more than enough. However, for\n        applications that require greater or more consistent I/O performance, consider\n        an instance type with higher I/O performance. Contents Current generation instances Previous generation instances Amazon EC2 instance type naming conventions Amazon EC2 instance type specifications Instances built on the AWS Nitro System Amazon EC2 instance type quotas Current generation instances For the best performance, we recommend that you use the following instance types\n            when you launch new instances. For more information, see Amazon EC2 Instance Types . General purpose: M5 | M5a | M5ad | M5d | M5dn | M5n | M5zn | M6a | M6g | M6gd | M6i | M6id | M6idn | M6in | M7a | M7g | M7gd | M7i | M7i-flex | M8g | Mac1 | Mac2 | Mac2-m1ultra | Mac2-m2 | Mac2-m2pro | T2 | T3 | T3a | T4g Compute optimized: C5 | C5a | C5ad | C5d | C5n | C6a | C6g | C6gd | C6gn | C6i | C6id | C6in | C7a | C7g | C7gd | C7gn | C7i | C7i-flex | C8g Memory optimized: R5 | R5a | R5ad | R5b | R5d | R5dn | R5n | R6a | R6g | R6gd | R6i | R6idn | R6in | R6id | R7a | R7g | R7gd | R7i | R7iz | R8g | U-3tb1 | U-6tb1 | U-9tb1 | U-12tb1 | U-18tb1 | U-24tb1 | U7i-12tb | U7in-16tb | U7in-24tb | U7in-32tb | X1 | X2gd | X2idn | X2iedn | X2iezn | X1e | X8g | z1d Storage optimized: D2 | D3 | D3en | H1 | I3 | I3en | I4g | I4i | Im4gn | Is4gen Accelerated computing: DL1 | DL2q | F1 | G4ad | G4dn | G5 | G5g | G6 | G6e | Gr6 | Inf1 | Inf2 | P2 | P3 | P3dn | P4d | P4de | P5 | P5e | Trn1 | Trn1n | VT1 High-performance computing: Hpc6a | Hpc6id | Hpc7a | Hpc7g Previous generation instances Amazon Web Services offers previous generation instance types for users who have optimized their\n            applications around them and have yet to upgrade. We encourage you to use current generation \n            instance types to get the best performance, but we continue to support the following previous \n            generation instance types. For more information about which current \n                generation instance type would be a suitable upgrade, see Previous Generation Instances . General purpose : A1 | M1 | M2 | M3 | M4 | T1 Compute optimized : C1 | C3 | C4 Memory optimized : R3 | R4 Storage optimized : I2 Accelerated computing : G3 Instance performance Fixed performance instances Fixed performance instances provide fixed CPU resources. These instances can\n                deliver and sustain full CPU performance at any time, and for as long as a workload\n                needs it. If you need consistently high CPU performance for applications such as\n                video encoding, high volume websites, or HPC applications, we recommend that you use\n                fixed performance instances. Burstable performance instances Burstable performance ( T ) instances provide a baseline level of CPU\n                performance with the ability to burst above the baseline. The baseline CPU is\n                designed to meet the needs of the majority of general purpose workloads, such as\n                large-scale micro-services, web servers, small and medium databases, data logging,\n                code repositories, virtual desktops, and development and test environments. The baseline utilization and ability to burst are governed by CPU credits. Each\n            burstable performance instance continuously earns credits when it stays below the CPU\n            baseline, and continuously spends credits when it bursts above the baseline. For more\n            information, see Burstable\n                performance instances in the Amazon EC2 User Guide . Flex instances M7i-flex and C7i-flex instances offer a balance of compute, memory, and network\n                resources, and they provide the most cost-effective way to run a broad spectrum of\n                general purpose applications. These instances provide reliable CPU resources to\n                deliver a baseline CPU performance of 40 percent, which is designed to meet the\n                compute requirements for a majority of general purpose workloads. When more\n                performance is needed, these instances provide the ability to exceed the baseline\n                CPU performance and deliver up to 100 percent CPU performance for 95 percent of the\n                time over a 24-hour window. M7i-flex and C7i-flex instances running at a high CPU utilization that is consistently\n            above the baseline for long periods of time might see a gradual reduction in the maximum\n            burst CPU throughput. For more information, see M7i-flex instances and C7i-flex instances . Pricing For pricing information, see Amazon EC2 Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Naming conventions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-types.pdf", "content": "No main content found."}, {"title": "What is Nitro Enclaves? - AWS", "url": "https://docs.aws.amazon.com/enclaves/latest/user/index.html", "content": "What is Nitro Enclaves? PDF RSS AWS Nitro Enclaves is an Amazon EC2 feature that allows you to create isolated execution\n\t\tenvironments, called enclaves , from Amazon EC2 instances. Enclaves are\n\t\tseparate, hardened, and highly-constrained virtual machines. They provide only secure local\n\t\tsocket connectivity with their parent instance. They have no persistent storage, interactive\n\t\taccess, or external networking. Users cannot SSH into an enclave, and the data and\n\t\tapplications inside the enclave cannot be accessed by the processes, applications, or users\n\t\t(root or admin) of the parent instance. Using Nitro Enclaves, you can secure your most sensitive \n\t\tdata, such as personally identifiable information (PII), and your data processing applications. Note Nitro Enclaves is processor agnostic and it is supported on most Intel, AMD, and AWS \n\t\t\tGraviton-based Amazon EC2 instance types built on the AWS Nitro System. Nitro Enclaves also supports an attestation feature, which allows you to verify an enclave's identity\n\t\tand ensure that only authorized code is running inside it. Nitro Enclaves is integrated with the\n\t\tAWS Key Management Service, which provides built-in support for attestation and enables you to prepare and\n\t\tprotect your sensitive data for processing inside enclaves. Nitro Enclaves can also be used with\n\t\tother key management services. Nitro Enclaves use the same Nitro Hypervisor technology that provides CPU and memory isolation\n\t\tfor Amazon EC2 instances in order to isolate the vCPUs and memory for an enclave from a parent\n\t\tinstance. The Nitro Hypervisor ensures that the parent instance has no access to the\n\t\tisolated vCPUs and memory of the enclave. To learn more about creating your first\n\t\tenclave using a sample enclave application, see Getting started with the Hello Enclaves sample application . Topics Learn more Requirements Considerations Pricing Related services Learn more To learn about the concepts used in Nitro Enclaves, see Nitro Enclaves concepts . To get started with your first enclave using a sample enclave application, see Getting started with the Hello Enclaves sample application . To learn about using the AWS Nitro Enclaves CLI to manage the lifecycle of\n\t\t\t\t\tenclaves, see Nitro Enclaves Command Line Interface . To learn about developing custom enclave applications and the\n\t\t\t\t\tAWS Nitro Enclaves SDK, see Nitro Enclaves application development . To learn about multiple enclaves, see Working with multiple enclaves . Requirements Nitro Enclaves has the following requirements: Parent instance requirements: Virtualized Nitro-based instances Intel or AMD-based instances with at least 4 vCPUs, excluding C7a , C7i , G4ad , M7a , M7i , M7i-Flex , R7a , R7i , R7iz , T3 , T3a , Trn1 , Trn1n , U-* , VT1 AWS Graviton-based instances with at least 2 vCPUs, excluding A1 , C7gd , C7gn , G5g , Hpc7g , Im4gn , Is4gen , M7g , M7gd , R7g , R7gd , T4g Linux or Windows (2016 or later) operating system Enclave requirements: Linux operating system only Considerations Keep the following in mind when using Nitro Enclaves: Nitro Enclaves is supported in the following Regions: US East (Ohio), US East (N. Virginia),\n\t\t\t\t\tUS West (N. California), US West (Oregon), Africa (Cape Town), Asia Pacific (Hong Kong),\n\t\t\t\t\tAsia Pacific (Hyderabad), Asia Pacific (Jakarta), Asia Pacific (Mumbai),\n\t\t\t\t\tAsia Pacific (Osaka), Asia Pacific (Seoul), Asia Pacific (Singapore),\n\t\t\t\t\tAsia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central),\n\t\t\t\t\tEurope (Frankfurt), Europe (Ireland), Europe (London),\n\t\t\t\t\tEurope (Milan), Europe (Paris), Europe (Stockholm),\n\t\t\t\t\tMiddle East (Bahrain), South America (S\u00e3o Paulo), AWS GovCloud (US-East), and\n\t\t\t\t\tAWS GovCloud (US-West). You can create up to four individual enclaves per parent instance. Enclaves can communicate only with the parent instance. Enclaves \n\t\t\t\t\trunning on the same or different parent instances cannot communicate \n\t\t\t\t\twith each other. Enclaves are active only while their parent instance is in the running state. If the parent instance is stopped or \n\t\t\t\t\tterminated, its enclaves are terminated. You cannot enable hibernation and enclaves on the same instance. Nitro Enclaves is not supported on Outposts. Nitro Enclaves is not supported in Local Zones or Wavelength Zones. Pricing There are no additional charges for using Nitro Enclaves. You are billed the standard\n\t\t\tcharges for the Amazon EC2 instance and for the other AWS services that you use. Related services Nitro Enclaves is integrated with the following AWS services: AWS Key Management Service AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic\n\t\t\t\t\t\tkeys and control their use across a wide range of AWS services and in your\n\t\t\t\t\t\tapplications. Nitro Enclaves integrates with AWS KMS and it allows you to perform\n\t\t\t\t\t\tselected KMS operations from the enclave using the AWS Nitro Enclaves SDK . These operations can be tied to the cryptographic attestation process of\n\t\t\t\t\t\tNitro Enclaves by setting a AWS KMS key policy to ensure that the operation works\n\t\t\t\t\t\tonly when the measurements of the enclave match the KMS key policy. For more\n\t\t\t\t\t\tinformation, see AWS KMS condition keys for Nitro Enclaves in the AWS Key Management Service Developer Guide . AWS Certificate Manager AWS Certificate Manager (ACM) is a service that lets you easily provision, manage, and\n\t\t\t\t\t\tdeploy public and private Secure Sockets Layer/Transport Layer Security\n\t\t\t\t\t\t(SSL/TLS) certificates for use with AWS services and your internal connected\n\t\t\t\t\t\tresources. SSL/TLS certificates are used to secure network communications\n\t\t\t\t\t\tand to establish the identity of websites over the internet, as well as\n\t\t\t\t\t\tresources on private networks. ACM removes the time-consuming manual process\n\t\t\t\t\t\tof purchasing, uploading, and renewing SSL/TLS certificates. For more\n\t\t\t\t\t\tinformation, see AWS Certificate Manager for Nitro Enclaves . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Nitro Enclaves concepts"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/enclaves/latest/user/enclaves-user.pdf", "content": "No main content found."}, {"title": "AWS\u00a0Windows AMI reference - AWS Windows AMIs", "url": "https://docs.aws.amazon.com/ec2/latest/windows-ami-reference/index.html", "content": "AWS\u00a0Windows AMI reference PDF RSS AWS provides a set of publicly available Amazon Machine Images (AMIs) that contain\n\t\tsoftware configurations specific to the Windows platform. You can quickly start building and deploying your applications with Amazon EC2 by using these\n\t\tAMIs. First choose the AMI that meets your specific requirements, and then launch an\n\t\tinstance using that AMI. You retrieve the password for the administrator account and then\n\t\tlog in to the instance using Remote Desktop Connection, just as you would with any other\n\t\tWindows Server. In general, the AWS\u00a0Windows AMIs are configured with the default settings used\n\t\tby the Microsoft installation media. However, Amazon does apply some customizations.\n\t\tFor example, the AWS\u00a0Windows AMIs come with the following software and\n\t\tdrivers: EC2Launch v2 (Windows Server 2022) EC2Launch v1 (Windows Server 2016 and 2019) EC2Config (through Windows Server 2012 R2) AWS Systems Manager AWS CloudFormation AWS Tools for Windows PowerShell Network drivers (SRIOV, ENA, Citrix PV) Storage drivers (NVMe, AWS PV, Citrix PV) Graphics drivers (NVidia GPU, Elastic GPU) Spot instance hibernation With the Windows fast launch feature, you can configure pre-provisioned snapshots\n\t\tto launch instances up to 65% faster. For more information, see Configure\n\t\t\tWindows fast launch for your Windows Server AMI in the Amazon EC2 User Guide . To view changes to each release of the AWS\u00a0Windows AMIs, including SQL Server\n\t\tupdates, see the AWS\u00a0Windows AMI version\n\t\t\thistory . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Specialized AWS\u00a0Windows AMIs"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/ec2/latest/windows-ami-reference/windows-ami-reference.pdf", "content": "No main content found."}, {"title": "Programmatic access to Amazon EC2 - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/ec2/latest/devguide/index.html", "content": "Programmatic access to Amazon EC2 PDF You can create and manage your Amazon EC2 resources using the AWS Management Console or a programmatic \n\t    interface. For information about using the Amazon EC2 console, see the Amazon EC2 User Guide . How it works Amazon EC2 endpoints Eventual consistency Idempotency Request throttling Programmatic interfaces AWS Command Line Interface (AWS CLI) AWS CloudFormation AWS SDKs Low-level API Getting started Code examples Console-to-Code Monitoring AWS CloudTrail Monitor requests Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Service endpoints"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/ec2/latest/devguide/ec2-dg.pdf", "content": "No main content found."}, {"title": "Welcome - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/index.html", "content": "Welcome PDF This is the Amazon EC2 API Reference . It provides descriptions, API request\n        parameters, and the XML response for each of the Amazon EC2 Query API actions. Note that the\n        Amazon EC2 API includes actions for Amazon EC2 plus additional services, such as Amazon EBS and Amazon VPC. Alternatively, use one of the following methods to access the Amazon EC2 API, instead of using\n        the Query API directly: AWS CLI Command Reference \u2013 ec2 commands AWS CloudFormation \u2013 Amazon EC2 resource type reference AWS Tools for PowerShell Cmdlet Reference \u2013 Amazon EC2 cmdlets AWS SDKs How Do I? Relevant Documentation Learn about using the Query API Making requests to the Amazon EC2 API Learn about the permissions required to call an Amazon EC2 API action Actions, resources, and condition keys for Amazon EC2 Get the list of API actions by service and resource Actions by service Get the alphabetical list of API actions Actions Get descriptions of the API error codes Error codes for the Amazon EC2 API Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions by service"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api.pdf", "content": "No main content found."}, {"title": "Welcome - Amazon EC2 Instance Connect", "url": "https://docs.aws.amazon.com/ec2-instance-connect/latest/APIReference/index.html", "content": "Welcome PDF This is the Amazon EC2 Instance Connect API Reference . It\n            provides descriptions, syntax, and usage examples for each of the actions for Amazon EC2\n            Instance Connect. Amazon EC2 Instance Connect enables system administrators to publish\n            one-time use SSH public keys to EC2, providing users a simple and secure way to connect\n            to their instances. To view the Amazon EC2 Instance Connect content in the Amazon EC2 User\n                Guide , see Connect to\n                your Linux instance using EC2 Instance Connect . For Amazon EC2 APIs, see the Amazon EC2 API\n                Reference . This document was last published on October 8, 2024. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/ec2-instance-connect/latest/APIReference/ec2-instance-connect-api.pdf", "content": "No main content found."}, {"title": "ec2 \u2014 AWS CLI 1.35.0 Command Reference", "url": "https://docs.aws.amazon.com/cli/latest/reference/ec2/", "content": "No main content found."}, {"title": "Amazon EC2 resource type reference - AWS CloudFormation", "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_EC2.html", "content": "Amazon EC2 resource type reference RSS Resource types AWS::EC2::CapacityReservation AWS::EC2::CapacityReservationFleet AWS::EC2::CarrierGateway AWS::EC2::ClientVpnAuthorizationRule AWS::EC2::ClientVpnEndpoint AWS::EC2::ClientVpnRoute AWS::EC2::ClientVpnTargetNetworkAssociation AWS::EC2::CustomerGateway AWS::EC2::DHCPOptions AWS::EC2::EC2Fleet AWS::EC2::EgressOnlyInternetGateway AWS::EC2::EIP AWS::EC2::EIPAssociation AWS::EC2::EnclaveCertificateIamRoleAssociation AWS::EC2::FlowLog AWS::EC2::GatewayRouteTableAssociation AWS::EC2::Host AWS::EC2::Instance AWS::EC2::InstanceConnectEndpoint AWS::EC2::InternetGateway AWS::EC2::IPAM AWS::EC2::IPAMAllocation AWS::EC2::IPAMPool AWS::EC2::IPAMPoolCidr AWS::EC2::IPAMResourceDiscovery AWS::EC2::IPAMResourceDiscoveryAssociation AWS::EC2::IPAMScope AWS::EC2::KeyPair AWS::EC2::LaunchTemplate AWS::EC2::LocalGatewayRoute AWS::EC2::LocalGatewayRouteTable AWS::EC2::LocalGatewayRouteTableVirtualInterfaceGroupAssociation AWS::EC2::LocalGatewayRouteTableVPCAssociation AWS::EC2::NatGateway AWS::EC2::NetworkAcl AWS::EC2::NetworkAclEntry AWS::EC2::NetworkInsightsAccessScope AWS::EC2::NetworkInsightsAccessScopeAnalysis AWS::EC2::NetworkInsightsAnalysis AWS::EC2::NetworkInsightsPath AWS::EC2::NetworkInterface AWS::EC2::NetworkInterfaceAttachment AWS::EC2::NetworkInterfacePermission AWS::EC2::NetworkPerformanceMetricSubscription AWS::EC2::PlacementGroup AWS::EC2::PrefixList AWS::EC2::Route AWS::EC2::RouteTable AWS::EC2::SecurityGroup AWS::EC2::SecurityGroupEgress AWS::EC2::SecurityGroupIngress AWS::EC2::SnapshotBlockPublicAccess AWS::EC2::SpotFleet AWS::EC2::Subnet AWS::EC2::SubnetCidrBlock AWS::EC2::SubnetNetworkAclAssociation AWS::EC2::SubnetRouteTableAssociation AWS::EC2::TrafficMirrorFilter AWS::EC2::TrafficMirrorFilterRule AWS::EC2::TrafficMirrorSession AWS::EC2::TrafficMirrorTarget AWS::EC2::TransitGateway AWS::EC2::TransitGatewayAttachment AWS::EC2::TransitGatewayConnect AWS::EC2::TransitGatewayMulticastDomain AWS::EC2::TransitGatewayMulticastDomainAssociation AWS::EC2::TransitGatewayMulticastGroupMember AWS::EC2::TransitGatewayMulticastGroupSource AWS::EC2::TransitGatewayPeeringAttachment AWS::EC2::TransitGatewayRoute AWS::EC2::TransitGatewayRouteTable AWS::EC2::TransitGatewayRouteTableAssociation AWS::EC2::TransitGatewayRouteTablePropagation AWS::EC2::TransitGatewayVpcAttachment AWS::EC2::VerifiedAccessEndpoint AWS::EC2::VerifiedAccessGroup AWS::EC2::VerifiedAccessInstance AWS::EC2::VerifiedAccessTrustProvider AWS::EC2::Volume AWS::EC2::VolumeAttachment AWS::EC2::VPC AWS::EC2::VPCCidrBlock AWS::EC2::VPCDHCPOptionsAssociation AWS::EC2::VPCEndpoint AWS::EC2::VPCEndpointConnectionNotification AWS::EC2::VPCEndpointService AWS::EC2::VPCEndpointServicePermissions AWS::EC2::VPCGatewayAttachment AWS::EC2::VPCPeeringConnection AWS::EC2::VPNConnection AWS::EC2::VPNConnectionRoute AWS::EC2::VPNGateway AWS::EC2::VPNGatewayRoutePropagation Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Previous AWS::EC2::CapacityReservation"}, {"title": "What is VM Import/Export? - VM Import/Export", "url": "https://docs.aws.amazon.com/vm-import/latest/userguide/index.html", "content": "What is VM Import/Export? PDF RSS VM Import/Export enables you to import virtual machine (VM) images from your existing\n        virtualization environment to Amazon EC2, and then export them back. This enables you to migrate\n        applications and workloads to Amazon EC2, copy your VM image catalog to Amazon EC2, or create a\n        repository of VM images for backup and disaster recovery. For more information, see VM Import/Export . For more information about how to use VM Import/Export, see How to get started with VM Import/Export . Topics Benefits of VM Import/Export Features of VM Import/Export Pricing for VM Import/Export Related services Benefits of VM Import/Export You can use VM Import/Export to migrate applications and workloads, copy your VM image\n            catalog, or create a disaster recovery repository for VM images. Migrate existing applications and workloads to Amazon EC2 When you migrate your VM-based applications and workloads to Amazon EC2, you\n                        preserve their software and configuration settings. When you create an AMI\n                        from your VM, you can run multiple instances based on the same imported VM.\n                        You can also use the AMI to replicate your applications and workloads around\n                        the world using AMI copy. For more information, see Copying an AMI in the Amazon EC2 User Guide . Import your VM image catalog to Amazon EC2 If you maintain a catalog of approved VM images, you can copy your image\n                        catalog to Amazon EC2 and create AMIs from the imported images. You can import\n                        your existing software, including products that you have installed such as\n                        anti-virus software, intrusion detection systems, and so on, along with your\n                        VM images. You can use the AMIs you create as your Amazon EC2 image\n                        catalog. Create a disaster recovery repository for VM images You can import your local VM images into Amazon EC2 for backup and disaster\n                        recovery purposes. You can import your VMs and store them as AMIs. The AMIs\n                        you create will be ready to launch in Amazon EC2 when you need them. If your\n                        local environment suffers an event, you can quickly launch your instances to\n                        preserve business continuity while simultaneously exporting them to rebuild\n                        your local infrastructure. Features of VM Import/Export VM Import provides the following features: The ability to import a VM from your virtualization environment to Amazon EC2 as an\n                    Amazon Machine Image (AMI). You can launch EC2 instances from your AMI any\n                    time. The ability to import a VM from your virtualization environment to Amazon EC2 as an\n                    EC2 instance. The instance is initially in a stopped state. You can\n                    create an AMI from the instance. The ability to export a VM that was previously imported from your\n                    virtualization environment. The ability to import disks as Amazon EBS snapshots. VM import supports ENA drivers for Linux. ENA support will be enabled only if\n                    the original VM has ENA and/or NVMe drivers installed. We recommend installing\n                    the latest drivers. Pricing for VM Import/Export With Amazon Web Services, you pay only for what you use. There is no additional fee to use\n            VM Import/Export. You pay the standard fees for the Amazon Simple Storage Service (Amazon S3) bucket and EBS volumes\n            used during the import and export processes, and for the EC2 instances that you\n            run. Related services Consider the following services as you plan your migration to AWS: AWS Application Discovery Service \u2013 You can use the Application Discovery Service\n                    to gather information about your data center, such as server utilization data\n                    and dependency mappings, so that you can view information about your workloads.\n                    For more information, see the Application Discovery Service User Guide . AWS Application Migration Service \u2013 If you use VMware vSphere,\n                    Microsoft Hyper-V, or Microsoft Azure, you can use Application Migration Service to automate the\n                    migration of your virtual machines to AWS. For more information, see the Application Migration Service User\n                        Guide . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How to get started with VM Import/Export"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vm-import/latest/userguide/vm-import-ug.pdf", "content": "No main content found."}, {"title": "What is Microsoft SQL Server on Amazon EC2? - Microsoft SQL Server on Amazon EC2", "url": "https://docs.aws.amazon.com/sql-server-ec2/latest/userguide/index.html", "content": "What is Microsoft SQL Server on Amazon EC2? PDF RSS You can run Microsoft SQL Server on Amazon Elastic Compute Cloud (Amazon EC2). Microsoft SQL Server is a relational database management system (RDBMS)\n    whose primary purpose is to store and retrieve data. SQL Server includes additional services, such as\n    Analysis Services (SSAS), Reporting Services (SSRS), Integration Services (SSIS), and Machine\n    Learning (ML). AWS provides a comprehensive set of services and tools to deploy Microsoft SQL Server on\n    the reliable and secure AWS Cloud infrastructure. The benefits of running SQL Server on AWS\n    include cost savings, scalability, high availability and disaster recovery, improved\n    performance, and ease of management. For more information, see Learn why AWS is the best cloud to run Microsoft Windows Server and SQL Server workloads on the AWS Compute blog. Amazon Elastic Compute Cloud (Amazon EC2) supports a self-managed SQL Server. That is, it gives you full control over the\n    setup of the infrastructure and the database environment. Running SQL Server on Amazon EC2 is very similar\n    to running SQL Server on your own server. You have full control of the database and operating\n    system-level access, so you can use your choice of tools to manage the operating system,\n    database software, patches, data replication, backup, and restoration. You are responsible for\n    data replication and recovery across your instances in the same or different AWS Regions. For\n    more information, refer to the AWS Shared Responsibility\n    Model . Overview topics Options to run SQL Server Concepts and terminology Features Pricing Microsoft SQL Server on Amazon EC2 features SQL Server on Amazon EC2 provides the following features: Flexible licensing options \u2014 When you use\n          Amazon EC2 instances with the license included, you are using instances with fully-compliant\n          Windows Server and SQL Server that are licensed through AWS. Flexible BYOL options include\n          default tenant EC2 for products that are eligible for Microsoft License Mobility through Software\n            Assurance , as well as Amazon EC2 Dedicated Hosts and Amazon EC2 Dedicated Instances . You can use AWS License Manager to track the usage of software licenses and reduce the risk of\n          non-compliance. For more information, see Licensing in the Amazon Web Services and\n            Microsoft Frequently Asked Questions . High performance block storage \u2014 Amazon Elastic Block Store provides multiple options for\n          high-performance block storage for Microsoft SQL Server. EC2 Instances using io2 Block Express give you the\n          highest block storage performance with a single storage volume. Other SSD-backed Amazon EBS\n          options include io2 volumes for business-critical applications and gp3 volumes for general\n          purpose applications. Amazon EBS also offers crash-consistent snapshots, and enables\n          application-consistent snapshots through Windows VSS (Volume Shadow Copy Services) to help\n          protect your SQL Server deployments. Fully-managed shared storage \u2014 Amazon FSx for Windows File Server and Amazon FSx for NetApp ONTAP offer\n          fully-managed shared storage for high-availability SQL Server failover cluster instances (FCI)\n          workloads. Windows-based services \u2014 AWS Directory Service offers managed Microsoft Active Directory with\n          identity and access management. Scalable processors \u2014 Intel Xeon\n      Scalable Processors on AWS provide you with better data protection, faster\n     processing of more data volumes, and increased service flexibility for Amazon EC2. Migration programs \u2014 AWS offers programs for\n     migration for customers looking to migrate SQL Server workloads to AWS. AWS Migration Acceleration Program (MAP) for\n      Windows provides services, best practices, and tools to help you save costs and\n     accelerate your migration on AWS. Windows workload optimization \u2014 After you move\n          your SQL Server workloads to AWS, you can continue to optimize costs, usage, and licenses to\n          suit your business requirements. With Cost Explorer Service , you can\n          visualize, understand, and manage your AWS costs and usage over time. AWS Compute Optimizer recommends optimal AWS\n          compute resources for your workloads so that you can reduce costs up to 25% by analyzing\n          historical utilization data. AWS Trusted Advisor can\n          check that your EC2 instances have the required amount of SQL Server licenses and that the EC2\n          instance vCPU count doesn\u2019t exceed what is permitted for the SQL Server edition. AWS Managed Services can help operate your cloud\n          environment post-migration by analyzing alerts and responding to incidents, reducing\n          operational overhead and risk. You can use AWS Systems Manager to automate operational tasks across your AWS resources and better\n          manage your infrastructure at scale. AWS can help you to modernize you Windows-based applications with AWS open source\n          services if you want to reduce the high cost of commercial licensing. Options include\n          running SQL Server database applications on Linux, moving workloads to Amazon Aurora , containerizing your Windows applications with Amazon EKS , going serverless with AWS Lambda , or taking advantage of micro-services based\n          architecture. For more features specific to Amazon EC2, see Features of Amazon\n   EC2 . Microsoft SQL Server on Amazon EC2 pricing For information about pricing for Amazon EC2, see the Amazon EC2 pricing page. For information about creating a price estimate for Microsoft Windows Server and Microsoft\n      SQL Server, see Tutorial: Using\n        Windows Server and SQL Server on Amazon EC2 calculator in the AWS Pricing Calculator User\n        Guide . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Options to run SQL Server"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/sql-server-ec2/latest/userguide/sql-server-ec2.pdf", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs#amazon-ec2", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs#develop-for-amazon-ec2", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs#migrate-to-amazon-ec2", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs#workloads-on-amazon-ec2", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/?icmpid=docs_homepage_featuredsvcs#best-practices", "content": "No main content found."}, {"title": "AWS Glossary - AWS Glossary", "url": "http://docs.aws.amazon.com/general/latest/gr/glos-chap.html", "content": "AWS Glossary PDF Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Numbers and symbols 100-continue A method that gives a client the ability to see whether a server can accept a\n               request before actually sending it. For large PUT requests, this method can save both\n               time and bandwidth charges. A Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z AAD See additional authenticated\n            data . access control list (ACL) A document that defines who can access a particular bucket or object. Each bucket and object in Amazon S3 has an ACL. This document defines\n               what each type of user can do, such as write and read permissions. access identifiers See credentials . access key The combination of an access key ID (for example, AKIAIOSFODNN7EXAMPLE ) and a secret access key (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY ). You use access keys to\n               sign API requests that you make to AWS. access key ID A unique identifier that's associated with a secret access key ; the access key ID and secret access key are used\n               together to sign programmatic AWS requests cryptographically. access key rotation A method to increase security by changing the AWS access key ID. You can use this\n               method to retire an old key at your discretion. access policy language A language for writing documents (specifically, policies ) that specify who can access a particular \n               AWS resource and under what\n               conditions. account A formal relationship with AWS that's associated with all of the\n               following: The owner email address and password The control of resources created\n                     under its umbrella Payment for the AWS activity related to those resources The AWS account has permission to do anything and everything with all the\n               AWS account resources. This is in contrast to a user , which is an entity contained within the account. account activity A webpage showing your month-to-date AWS usage and costs. The account activity\n               page is located at https://aws.amazon.com/account-activity/ . AWS Account Management AWS Account Management is a tool that you can use to update the contact information for each of your AWS accounts. See also https://aws.amazon.com/organizations . ACL See access control list (ACL) . ACM AWS Certificate Manager is a web service for provisioning, managing, and deploying Secure \n                  Sockets Layer/ Transport Layer Security (SSL/TLS) certificates for use with AWS services. See also https://aws.amazon.com/certificate-manager/ . action An API function. Also called operation or call . The activity the principal has permission to perform. The action is B in the statement\n               \"A has permission to do B to C where D applies.\" For example, Jane sends a request to Amazon SQS with Action=ReceiveMessage . CloudWatch : The response initiated by the\n               change in an alarm's state (for example, from OK to ALARM ).\n               The state change might be caused by a metric reaching the alarm threshold, or by a SetAlarmState request. Each alarm can have one or more actions\n               assigned to each state. Actions are performed once each time the alarm changes to a\n               state that has an action assigned. Example actions include an Amazon SNS notification, running an Amazon EC2 Auto Scaling policy , and an Amazon EC2 instance stop/terminate action. active trusted key groups A list that shows each of the trusted key groups , and the IDs of the public keys in each key\n               group, that are active for a distribution in Amazon CloudFront. CloudFront can use the public keys\n               in these key groups to verify the signatures of CloudFront signed URLs\n                  and signed cookies . active trusted signers See active trusted key groups . active-active A class of high availability strategies in which a workload exists simultaneously in multiple Regions, uses multiple primary resources, and serves traffic from all of the Regions to which it's deployed. Sometimes referred to as active/active. See also read local/write global , read local/write local , global consistency . active-passive A class of disaster recovery strategies that involve a primary Region and a standby Region in a back up and restore , hot standby , pilot light , or warm standby configuration. Sometimes referred to as active/passive. additional authenticated\n            data Information that's checked for integrity but not encrypted, such as headers or\n               other contextual metadata. administrative suspension Amazon EC2 Auto Scaling might suspend processes\n               for Auto Scaling group that\n               repeatedly fail to launch instances. Auto Scaling groups that most commonly experience\n               administrative suspension have zero running instances, have been trying to launch\n               instances for more than 24 hours, and have not succeeded in that time. alarm An item that watches a single metric over a specified time period and starts an Amazon SNS topic or an Amazon EC2 Auto Scaling policy . These actions are started if the\n               value of the metric crosses a threshold value over a predetermined number of time\n               periods. allow One of two possible outcomes (the other is deny ) when an IAM access policy is evaluated. When a user\n               makes a request to AWS, AWS evaluates the request based on all permissions that apply\n               to the user and then returns either allow or deny. Amazon Machine Image (AMI) An Amazon Machine Image (AMI) is an encrypted machine image stored in Amazon EBS or Amazon S3 . AMIs function similarly to a\n               template of a computer's root drive. They contain the operating system and can also\n               include software and layers of your application, such as database servers,\n               middleware, and web servers. Amazon Web Services (AWS) An infrastructure web services platform in the cloud for companies of all\n               sizes. See also https://aws.amazon.com/what-is-cloud-computing/ . AMI See Amazon Machine Image\n            (AMI) . Amplify AWS Amplify is a complete solution that frontend web and mobile developers can\n               use to build and deploy secure, scalable full-stack applications powered by AWS.\n               Amplify provides two services: Amplify Hosting and Amplify Studio . See also https://aws.amazon.com/amplify/ . Amplify Android Amplify Android is a collection of open-source client libraries that provides\n               interfaces for specific use cases across many AWS services. Amplify Android is\n               the recommended way to build native Android applications powered by AWS. See also https://aws.amazon.com/amplify/ . Amplify Hosting AWS Amplify Hosting is a fully managed continuous integration and continuous\n               delivery (CI/CD) and hosting service for fast, secure, and reliable static and\n               server-side rendered apps. Amplify Hosting provides a Git-based workflow for\n               hosting full-stack serverless web apps with continuous deployment. See also https://aws.amazon.com/amplify/hosting/ . Amplify iOS Amplify iOS is a collection of open-source client libraries that provides\n               interfaces for specific use cases across many AWS services. Amplify iOS is the\n               recommended way to build native iOS applications powered by AWS. See also https://aws.amazon.com/amplify/ . Amplify Studio AWS Amplify Studio is a visual development environment that web and mobile\n               developers can use to build the frontend UI components and the backend environment\n               for a full-stack application. See also https://aws.amazon.com/amplify/studio/ . analysis rules AWS Clean Rooms : The query restrictions that authorize a specific type of query. analysis scheme CloudSearch : Language-specific text\n               analysis options that are applied to a text field to control stemming and configure\n               stopwords and synonyms. API Gateway Amazon API Gateway is a fully managed service that developers can use to create, publish, maintain,\n               monitor, and secure APIs at any scale. See also https://aws.amazon.com/api-gateway . AWS App2Container AWS App2Container is a transformation tool that modernizes .NET and Java applications by migrating them into containerized applications. See also https://aws.amazon.com/app2container . AWS AppConfig AWS AppConfig is a service used to update software at runtime without deploying new code. With AWS AppConfig, you can configure, validate, and deploy feature flags and application configurations. See also https://aws.amazon.com/systems-manager/features/appconfig . Amazon AppFlow Amazon AppFlow is a fully managed integration service that you can use to transfer data securely between software as a service (SaaS) applications and AWS services. See also https://aws.amazon.com/appflow . application Elastic Beanstalk : A logical collection of\n               components, including environments, versions, and environment configurations. An\n               application is conceptually similar to a folder. CodeDeploy : A name that\n               uniquely identifies the application to be deployed. AWS CodeDeploy uses this name to\n               ensure the correct combination of revision, deployment configuration, and deployment\n               group are referenced during a deployment. Application Auto Scaling AWS Application Auto Scaling is a web service that you can use to configure automatic scaling for AWS resources\n               beyond Amazon EC2, such as Amazon ECS services, Amazon EMR clusters, and DynamoDB tables. See also https://aws.amazon.com/autoscaling/ . Application Billing The location where your customers manage the Amazon DevPay products they've purchased. The\n               web address is http://www.amazon.com/dp-applications . Application Composer AWS Application Composer is a visual designer that you can use to build\n               serverless applications from multiple AWS services. As you design an application,\n               Application Composer automatically generates a YAML template with CloudFormation and AWS SAM template resources. See also https://aws.amazon.com/application-composer/ . Application Cost Profiler AWS Application Cost Profiler is a solution to track the consumption of shared AWS resources used by software applications and report granular cost breakdown across tenant base. See also https://aws.amazon.com/aws-cost-management/aws-application-cost-profiler/ . Application Discovery Service AWS Application Discovery Service is a web service that helps you plan to migrate to AWS by identifying IT assets in\n               a data center\u2014including servers, virtual machines, applications, application\n               dependencies, and network infrastructure. See also https://aws.amazon.com/application-discovery/ . application revision CodeDeploy : An archive file\n               containing source content\u2014such as source code, webpages, executable files, and\n               deployment scripts\u2014along with an application specification\n            file . Revisions are stored in Amazon S3 buckets or GitHub repositories. For Amazon S3, a revision is uniquely identified by\n               its Amazon S3 object key and its ETag, version, or both. For GitHub, a revision is\n               uniquely identified by its commit ID. application specification\n            file CodeDeploy : A YAML-formatted\n               file used to map the source files in an application revision to destinations on the\n               instance. The file is also used to specify custom permissions for deployed files and\n               specify scripts to be run on each instance at various stages of the deployment\n               process. application version Elastic Beanstalk : A specific, labeled\n               iteration of an application that represents a functionally consistent set of\n               deployable application code. A version points to an Amazon S3 object (a JAVA WAR file) that contains the application code. AppSpec file See application specification\n            file . AppStream 2.0 Amazon AppStream 2.0 is a fully managed, secure service for streaming desktop applications to users\n               without rewriting those applications. See also https://aws.amazon.com/appstream/ . AWS AppSync AWS AppSync is an enterprise-level, fully managed GraphQL service with real-time data\n               synchronization and offline programming features. See also https://aws.amazon.com/appsync/ . ARN See Amazon Resource Name (ARN) . artifact CodePipeline : A copy of the\n               files or changes that are worked on by the pipeline. asymmetric encryption Encryption that uses both a\n               public key and a private key. asynchronous bounce A type of bounce that occurs when a receiver initially accepts an email message\n               for delivery and then subsequently fails to deliver it. Athena Amazon Athena is an interactive query service that you can use to analyze data in Amazon S3 using ANSI\n               SQL. Athena is serverless, so there's no infrastructure to manage. Athena scales\n               automatically and is simple to use, so you can start analyzing your datasets within\n               seconds. See also https://aws.amazon.com/athena/ . atomic counter DynamoDB: A method of incrementing or decrementing the value of an existing attribute\n               without interfering with other write requests. attribute A fundamental data element, something that doesn't need to be broken down any\n               further. In DynamoDB, attributes are similar in many ways to fields or columns in other\n               database systems. Amazon Machine Learning: A unique, named property within an observation in a dataset. In tabular\n               data, such as spreadsheets or comma-separated values (.csv) files, the column\n               headings represent the attributes, and the rows contain values for each\n               attribute. AUC Area Under a Curve. An industry-standard metric to evaluate the quality of a\n               binary classification machine learning model. AUC measures the ability of the model\n               to predict a higher score for positive examples, those that are \u201ccorrect,\u201d than for\n               negative examples, those that are \u201cincorrect.\u201d The AUC metric returns a decimal value\n               from 0 to 1. AUC values near 1 indicate an ML model that's highly accurate. Aurora Amazon Aurora is a fully managed MySQL-compatible relational database engine that combines the\n                  speed and availability of commercial databases with the simplicity and\n                  cost-effectiveness of open-source databases. See also https://aws.amazon.com/rds/aurora/ . authenticated encryption Encryption that provides\n               confidentiality, data integrity, and authenticity assurances of the encrypted\n               data. authentication The process of proving your identity to a system. AWS Auto Scaling AWS Auto Scaling is a fully managed service that you can use to quickly discover the scalable AWS\n               resources that are part of your application and to configure dynamic scaling. See also https://aws.amazon.com/autoscaling/ . Auto Scaling group A representation of multiple EC2 instances that \n               share similar characteristics, and that are treated as a logical grouping for the \n               purposes of instance scaling and management. Availability Zone A distinct location within a Region that's insulated from failures in other Availability Zones, and provides inexpensive,\n               low-latency network connectivity to other Availability Zones in the same\n               Region. AWS See Amazon Web Services\n            (AWS) . B Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z back up and restore A disaster recovery strategy in which backups of data in the primary Region are copied to a standby Region and can be restored from the standby Region. You must provision the infrastructure and other resources, such as compute, as part of a failover process. See also active-passive , hot standby , pilot light , warm standby . Backint Agent AWS Backint Agent for SAP HANA is an SAP-certified backup and restore solution for SAP HANA workloads running on Amazon EC2 instances in the cloud. See also https://aws.amazon.com/backint-agent . AWS Backup AWS Backup is a managed backup service that you can use to centralize and automate the backup of\n               data across AWS services in the cloud and on premises. See also https://aws.amazon.com/backup/ . basic monitoring Monitoring of AWS provided metrics derived at a 5-minute frequency. batch See document batch . batch prediction Amazon Machine Learning: An operation that processes multiple input data observations at one time\n               (asynchronously). Unlike real-time predictions, batch predictions aren't available\n               until all predictions have been processed. See also real-time predictions . BGP ASN Border Gateway Protocol Autonomous System Number is a unique identifier for a\n               network, for use in BGP routing. Amazon EC2 supports all 2-byte ASN numbers in the range of 1 \u2013 65335, with the exception\n               of 7224, which is reserved. billing See Billing and Cost Management . Billing and Cost Management AWS Billing and Cost Management is the AWS Cloud computing model where you pay for services on demand and use as\n               much or as little as you need. While resources are active under your account, you pay for the cost of\n               allocating those resources. You also pay for any incidental usage associated with\n               those resources, such as data transfer or allocated storage. See also https://aws.amazon.com/billing/new-user-faqs/ . binary attribute Amazon Machine Learning: An attribute for which one of two possible values is possible. Valid\n               positive values are 1, y, yes, t, and true answers. Valid negative values are 0, n,\n               no, f, and false. Amazon Machine Learning outputs 1 for positive values and 0 for negative\n               values. See also attribute . binary classification model Amazon Machine Learning: A machine learning model that predicts the answer to questions where the\n               answer can be expressed as a binary variable. For example, questions with answers of\n               \u201c1\u201d or \u201c0\u201d, \u201cyes\u201d or \u201cno\u201d, \u201cwill click\u201d or \u201cwill not click\u201d are questions that have\n               binary answers. The result for a binary classification model is always either a \u201c1\u201d\n               (for a \u201ctrue\u201d or affirmative answers) or a \u201c0\u201d (for a \u201cfalse\u201d or negative\n               answers). block A dataset. Amazon EMR breaks large amounts of data into\n               subsets. Each subset is called a data block. Amazon EMR assigns an ID to each block and\n               uses a hash table to keep track of block processing. block device A storage device that supports reading and (optionally) writing data in fixed-size\n               blocks, sectors, or clusters. block device mapping A mapping structure for every AMI and instance that specifies the block devices attached to the\n               instance. AWS Blockchain Templates See Managed Blockchain . blue/green deployment CodeDeploy: A deployment method where the instances in a deployment group (the original\n               environment) are replaced by a different set of instances (the replacement\n               environment). bootstrap action A user-specified default or custom action that runs a script or an application on\n               all nodes of a job flow before Hadoop starts. Border Gateway Protocol Autonomous System Number See BGP ASN . bounce A failed email delivery attempt. Braket Amazon Braket is a fully managed quantum computing service that helps you run quantum algorithms to accelerate your research and discovery. See also https://aws.amazon.com/braket . breach Amazon EC2 Auto Scaling : The condition where a\n               user-set threshold (upper or lower boundary) is passed. If the duration of the breach\n               is significant, as set by a breach duration parameter, it can possibly start a scaling activity . bucket Amazon S3 : A container for stored objects. Every\n               object is contained in a bucket. For example, if the object named photos/puppy.jpg is stored in the amzn-s3-demo-bucket bucket, then authorized users can access the object with the URL https://amzn-s3-demo-bucket.s3.region-code.amazonaws.com/photos/puppy.jpg . bucket owner The person or organization that owns a bucket in Amazon S3 . In the same way that Amazon is the only\n               owner of the domain name Amazon.com, only one person or organization can own a\n               bucket. bundling A commonly used term for creating an Amazon Machine Image\n            (AMI) . It specifically refers to creating instance store-backed AMIs . C Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z cache cluster A logical cache distributed over multiple cache nodes . A cache cluster can be set up with a specific number of\n               cache nodes. cache cluster identifier Customer-supplied identifier for the cache cluster that must be unique for that\n               customer in an AWS Region . cache engine version The version of the Memcached service that's running on the cache node. cache node A fixed-size chunk of secure, network-attached RAM. Each cache node runs an\n               instance of the Memcached service, and has its own DNS name and port. Multiple types\n               of cache nodes are supported, each with varying amounts of associated memory. cache node type An EC2 instance type used to run the\n               cache node. cache parameter group A container for cache engine parameter values that can be applied to one or more\n               cache clusters. cache security group A group maintained by ElastiCache that combines inbound authorizations to cache nodes\n               for hosts belonging to Amazon EC2 security groups that are specified through the\n               console or the API or command line tools. campaign Amazon Personalize : A deployed solution version (trained model) with provisioned dedicated transaction capacity for creating real-time recommendations for your application users. After you create a campaign, you use the getRecommendations or getPersonalizedRanking personalization operations to get recommendations. See also recommendations . See also solution version . canned access policy A standard access control policy that you can apply to a bucket or object. Options include: private,\n               public-read, public-read-write, and authenticated-read. canonicalization The process of converting data into a standard format that a service such as Amazon S3 can recognize. capacity The amount of available compute size at a given time. Each Auto Scaling group is defined with a\n               minimum and maximum compute size. A scaling activity increases or decreases the capacity within the defined\n               minimum and maximum values. Cartesian product A mathematical operation that returns a product from multiple sets. Cartesian product processor A processor that calculates a Cartesian product. Also known as a Cartesian data processor . AWS CDK AWS Cloud Development Kit (AWS CDK) is an open-source software development framework for defining your cloud\n               infrastructure in code and provisioning it through AWS CloudFormation. See also https://aws.amazon.com/cdk/ . CDN See content delivery network (CDN) . certificate A credential that some AWS products use to authenticate AWS accounts and users. Also known as an X.509 certificate . The certificate is paired with a private\n               key. chargeable resources Features or services whose use incurs fees. Although some AWS products are free,\n               others include charges. For example, in an CloudFormation stack , AWS resources that have been created incur charges. The amount charged\n               depends on the usage load. Use the Amazon Web Services Simple Monthly Calculator\n               \n               to estimate your cost prior to creating instances, stacks, or other resources. AWS Chatbot AWS Chatbot is an interactive agent that makes it easier to monitor, troubleshoot, and operate AWS resources in your Slack channels and Amazon Chime chat rooms. See also https://aws.amazon.com/chatbot . Amazon Chime Amazon Chime is a secure, real-time, unified communications service that transforms meetings by\n               making them more efficient and easier to conduct. See also https://aws.amazon.com/chime/ . CIDR block Classless Inter-Domain Routing. An internet protocol address allocation and route\n               aggregation methodology. See also Classless\n               Inter-Domain Routing on Wikipedia. ciphertext Information that has been encrypted , as opposed to plaintext , which is information that has not. classification In machine learning, a type of problem that seeks to place (classify) a data\n               sample into a single category or \u201cclass.\u201d Often, classification problems are modeled\n               to choose one category (class) out of two. These are binary classification problems.\n               Problems with more than two available categories (classes) are called \"multiclass\n               classification\" problems. See also binary classification model . See also multiclass classification\n            model . AWS Clean Rooms AWS Clean Rooms is an AWS service that helps multiple parties to join their data together in a secure collaboration workspace. See also https://aws.amazon.com/clean-rooms/ . Client VPN AWS Client VPN is a client-based, managed VPN service that remote clients can use to securely access your AWS resources using an Open VPN-based software client. See also https://aws.amazon.com/vpn/client-vpn . AWS Cloud Control API AWS Cloud Control API is a set of standardized application programming interfaces (APIs) that developers can use to create, read, update, delete, and list supported cloud infrastructure. See also https://aws.amazon.com/cloudcontrolapi . Cloud Directory Amazon Cloud Directory is a service that provides a highly scalable directory store for your application's\n               multihierarchical data. See also https://aws.amazon.com/cloud-directory/ . AWS Cloud Map AWS Cloud Map is a service that you use to create and maintain a map of the backend services and\n               resources that your applications depend on. With AWS Cloud Map, you can name and\n               discover your AWS Cloud resources. See also https://aws.amazon.com/cloud-map . cloud service provider (CSP) A cloud service provider is a company that provides subscribers with access to internet-hosted computing,\n               storage, and software services. AWS Cloud WAN AWS Cloud WAN is a managed wide-area networking service used to build, manage, and monitor a unified global network. See also https://aws.amazon.com/cloud-wan . AWS Cloud9 AWS Cloud9 is a cloud-based integrated development environment (IDE) that you use to write, run,\n               and debug code. See also https://aws.amazon.com/cloud9/ . CloudFormation AWS CloudFormation is a service for writing or changing templates that create and delete related AWS resources together as a unit. See also https://aws.amazon.com/cloudformation . CloudFront Amazon CloudFront is an AWS content delivery service that helps you improve the performance,\n               reliability, and availability of your websites and applications. See also https://aws.amazon.com/cloudfront . CloudHSM AWS CloudHSM is a web service that helps you meet corporate, contractual, and regulatory\n               compliance requirements for data security by using dedicated hardware security module\n               (HSM) appliances within the AWS Cloud. See also https://aws.amazon.com/cloudhsm/ . CloudSearch Amazon CloudSearch is a fully managed service in the AWS Cloud that you can use to set up, manage, and scale a search solution for your website or application. See also https://aws.amazon.com/cloudsearch/ . CloudTrail AWS CloudTrail is a web service that records AWS API calls for your account and delivers log files\n               to you. The recorded information includes the identity of the API caller, the time of\n               the API call, the source IP address of the API caller, the request parameters, and\n               the response elements that the AWS service returns. See also https://aws.amazon.com/cloudtrail/ . CloudWatch Amazon CloudWatch is a web service that you can use to monitor and manage various metrics, and\n               configure alarm actions based on data from those metrics. See also https://aws.amazon.com/cloudwatch . CloudWatch Events Amazon CloudWatch Events is a web service that you can use to deliver a timely stream of system events that\n               describe changes in AWS resources to Lambda functions, streams in Kinesis Data Streams , Amazon SNS topics, or built-in targets. See also https://aws.amazon.com/cloudwatch . CloudWatch Logs Amazon CloudWatch Logs is a web service for monitoring and troubleshooting your systems and applications\n               from your existing system, application, and custom log files. You can send your\n               existing log files to CloudWatch Logs and monitor these logs in near-real time. See also https://aws.amazon.com/cloudwatch . cluster A logical grouping of container instances that you can place tasks on. OpenSearch Service : A logical grouping of one or more data\n               nodes, optional dedicated master nodes, and storage required to run Amazon OpenSearch Service (OpenSearch Service)\n               and operate your OpenSearch Service domain. See also data node . See also dedicated master node . See also node . cluster compute instance A type of instance that provides a\n               great amount of CPU power coupled with increased networking performance, making it\n               well suited for High Performance Compute (HPC) applications and other demanding\n               network-bound applications. cluster placement group A logical cluster compute instance grouping\n               to provide lower latency and high-bandwidth connectivity between the instances . cluster status OpenSearch Service : An indicator of the health of a cluster. A\n               status can be green, yellow, or red. At the shard level, green means that all shards\n               are allocated to nodes in a cluster, yellow means that the primary shard is allocated\n               but the replica shards aren't, and red means that the primary and replica shards of\n               at least one index aren't allocated. The shard status determines the index status,\n               and the index status determines the cluster status. CNAME Canonical Name Record. A type of resource record in the Domain Name System (DNS) that specifies that the\n               domain name is an alias of another, canonical domain name. Specifically, it's an\n               entry in a DNS table that you can use to alias one fully qualified domain name to\n               another. Code Signing for AWS IoT A service for signing code that you create for any IoT device that's supported by\n               Amazon Web Services (AWS). CodeBuild AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs\n               tests, and produces software packages that are ready to deploy. See also https://aws.amazon.com/codebuild . CodeCommit AWS CodeCommit is a fully managed source control service that companies can use to host secure and\n               highly scalable private Git repositories. See also https://aws.amazon.com/codecommit . CodeDeploy AWS CodeDeploy is a service that automates code deployments to any instance, including EC2 instances and instances running on-premises. See also https://aws.amazon.com/codedeploy . AWS CodeDeploy agent AWS CodeDeploy agent is a software package that, when installed and configured on an instance, enables\n               that instance to be used in CodeDeploy deployments. CodeGuru Amazon CodeGuru is a collection of developer tools that automate code reviews and provide intelligent recommendations to optimize application performance. See also https://aws.amazon.com/codeguru . CodePipeline AWS CodePipeline is a continuous delivery service for fast and reliable application updates. See also https://aws.amazon.com/codepipeline . Amazon Cognito Amazon Cognito is a web service that you can use to save mobile user data in the AWS Cloud without\n               writing any backend code or managing any infrastructure. Examples of mobile user data\n               that you can save include app preferences and game states. Amazon Cognito offers mobile\n               identity management and data synchronization across devices. See also https://aws.amazon.com/cognito/ . collaboration AWS Clean Rooms : A secure logical boundary in AWS Clean Rooms \n               in which members can perform SQL queries on configured tables. AWS CLI AWS Command Line Interface is a unified downloadable and configurable tool for managing AWS services. Control\n               multiple AWS services from the command line and automate them through\n               scripts. See also https://aws.amazon.com/cli/ . complaint The event where a recipient who\n               doesn't want to receive an email message chooses \"Mark as Spam\" within the email\n               client, and the internet service provider\n               (ISP) sends a notification to Amazon SES . compound query CloudSearch : A search request that\n               specifies multiple search criteria using the Amazon CloudSearch structured search syntax. Amazon Comprehend Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find insights and relationships in text. See also https://aws.amazon.com/comprehend/ . Amazon Comprehend Medical Amazon Comprehend Medical is a HIPAA-eligible natural language processing (NLP) service that uses machine learning (ML), and has been pre-trained to understand and extract health data from medical text, such as prescriptions, procedures, or diagnoses. See also https://aws.amazon.com/comprehend/medical . condition IAM : Any restriction or detail\n               about a permission. The condition is D in the statement \"A has\n               permission to do B to C where D applies.\" AWS WAF : A set of attributes that AWS WAF\n               searches for in web requests to AWS resources such as Amazon CloudFront distributions. Conditions can include values such as the IP addresses that web\n               requests originate from or values in request headers. Based on the specified\n               conditions, you can configure AWS WAF to allow or block web requests to AWS\n               resources. conditional parameter See mapping . AWS Config AWS Config is a fully managed service that provides an AWS resource inventory, configuration history, and configuration change\n               notifications for better security and governance. You can create rules that\n               automatically check the configuration of AWS resources that AWS Config records. See also https://aws.amazon.com/config/ . configuration API CloudSearch : The API call that you\n               use to create, configure, and manage search domains. configuration template A series of key\u2013value pairs that define parameters for various AWS products\n               so that Elastic Beanstalk can provision them for\n               an environment. Amazon Connect Amazon Connect is a service solution that offers self-service configuration and provides dynamic,\n               personal, and natural customer engagement at any scale. See also https://aws.amazon.com/connect/ . consistency model The method a service uses to achieve high availability. For example, it could\n               involve replicating data across multiple servers in a data center. See also eventual consistency . console See AWS Management Console . Console Mobile Application AWS Console Mobile Application lets AWS customers monitor and manage a select set of resources to stay informed and connected with their AWS resources while on the go. See also https://aws.amazon.com/console/mobile . consolidated billing A feature of the AWS Organizations service for consolidating payment for multiple AWS accounts. You create an organization that contains your AWS accounts, and you use the\n               management account of your organization to pay for all member accounts. You can see a\n               combined view of AWS costs that are incurred by all accounts in your organization,\n               and you can get detailed cost reports for individual accounts. container A container is a standard unit of software that contains application code and all\n               relevant dependencies. container definition A container definition specifies the details that are associated with running a container on Amazon ECS. More specifically,\n               a container definition specifies details such as the container image to use and how\n               much CPU and memory the container is allocated. The container definition is included\n               as part of an Amazon ECS task definition . container instance A container instance is a self-managed EC2 instance or an on-premises server or virtual machine (VM) that's\n               running the Amazon Elastic Container Service (Amazon ECS) container agent and has been registered into a cluster . A container instance serves as the\n               infrastructure that your Amazon ECS workloads are run on. container registry A container registry is a collection of repositories that store container images.\n               One example is Amazon Elastic Container Registry (Amazon ECR). content delivery network (CDN) A web service that speeds up distribution of your static and dynamic web\n               content\u2014such as .html, .css, .js, media files, and image files\u2014to your users by using\n               a worldwide network of data centers. When a user requests your content, the request\n               is routed to the data center that provides the lowest latency (time delay). If the\n               content is already in the location with the lowest latency, the CDN delivers it\n               immediately. If not, the CDN retrieves it from an origin that you specify (for\n               example, a web server or an Amazon S3 bucket). With some CDNs, you can help secure\n               your content by configuring an HTTPS connection between users and data centers, and\n               between data centers and your origin. Amazon CloudFront is an example of a\n               CDN. contextual metadata Amazon Personalize :\n               Interactions data that you collect about a user's browsing context (such as device\n               used or location) when an event (such as a click) occurs. Contextual metadata can\n               improve recommendation relevance for new and existing users. See also Interactions dataset . See also event . continuous delivery A software development practice where code changes are automatically built,\n               tested, and prepared for a release to production. See also https://aws.amazon.com/devops/continuous-delivery/ . continuous integration A software development practice where developers regularly merge code changes into\n               a central repository, after which automated builds and tests are run. See also https://aws.amazon.com/devops/continuous-integration/ . AWS Control Tower AWS Control Tower is a service used to set up and govern a secure, multi-account AWS environment. See also https://aws.amazon.com/controltower . cooldown period Amount of time that Amazon EC2 Auto Scaling doesn't allow the desired size of the Auto Scaling group to be changed by any other notification from an CloudWatch alarm . core node An EC2 instance that runs Hadoop map and reduce tasks and stores data\n               using the Hadoop Distributed File System (HDFS). Core nodes are managed by the master node , which assigns Hadoop tasks to\n               nodes and monitors their status. The EC2 instances you assign as core nodes are\n               capacity that must be allotted for the entire job flow run. Because core nodes store\n               data, you can't remove them from a job flow. However, you can add more core nodes to\n               a running job flow. Core nodes run both the DataNodes and TaskTracker Hadoop daemons. corpus CloudSearch : A collection of data\n               that you want to search. Corretto Amazon Corretto is a no-cost, multiplatform, production-ready distribution of the Open Java\n               Development Kit (OpenJDK). See also https://aws.amazon.com/corretto/ . coverage Amazon Personalize : An\n               evaluation metric that tells you the proportion of unique items that Amazon Personalize might\n               recommend using your model out of the total number of unique items in Interactions\n               and Items datasets. To make sure Amazon Personalize recommends more of your items, use a model\n               with a higher coverage score. Recipes that feature item exploration, such as\n               user-personalization, have higher coverage than those that don\u2019t, such as\n               popularity-count. See also metrics . See also Items dataset . See also Interactions dataset . See also item exploration . See also user-personalization recipe . See also popularity-count recipe . credential helper CodeCommit : A program that\n               stores credentials for repositories and supplies them to Git when making connections\n               to those repositories. The AWS CLI includes a credential helper that you can use with Git when connecting to CodeCommit\n               repositories. credentials Also called access credentials or security\n                  credentials . In authentication and authorization, a system uses\n               credentials to identify who is making a call and whether to allow the requested\n               access. In AWS, these credentials are typically the access key ID and the secret access key . cross-account access The process of permitting limited, controlled use of resources in one AWS account by a user in another AWS account. For\n               example, in CodeCommit and CodeDeploy you can configure\n               cross-account access so that a user in AWS account A can access an CodeCommit repository\n               created by account B. Or a pipeline in CodePipeline created by account A can use CodeDeploy resources created\n               by account B. In IAM you use a role to delegate temporary access to a user in one account to resources in\n               another. cross-Region replication A solution for replicating data across different AWS Regions , in near-real time. Cryptographic Computing for Clean Rooms (C3R) AWS Clean Rooms : A capability in AWS Clean Rooms that organizations can use\n               to bring sensitive data together to derive new insights from data analytics while cryptographically limiting \n               what any party in the process can learn. customer gateway A router or software application on your side of a VPN tunnel that's managed by Amazon VPC . The internal interfaces of the\n               customer gateway are attached to one or more devices in your home network. The\n               external interface is attached to the virtual private gateway (VGW) across the VPN tunnel. customer managed policy An IAM managed policy that you create and\n               manage in your AWS account . customer master key (CMK) We no longer use customer master key or CMK. These terms \n               are replaced by AWS KMS key (first mention) and KMS key \n               (subsequent mention). For more information, see KMS key . D Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z dashboard See service health dashboard . data consistency A concept that describes when data is written or updated successfully and all\n               copies of the data are updated in all AWS Regions . However, it takes time for the data to propagate to all\n               storage locations. To support varied application requirements, DynamoDB supports both eventually consistent\n               and strongly consistent reads. See also eventual consistency . See also eventually consistent read . See also strongly consistent read . AWS Data Exchange AWS Data Exchange is a service that helps you find, subscribe to, and use third-party data in the cloud. See also https://aws.amazon.com/data-exchange . Amazon Data Lifecycle Manager Amazon Data Lifecycle Manager is an Amazon service that automates and manages the lifecycle of Amazon EBS\n               snapshots and Amazon EBS-backed AMIs . data node OpenSearch Service : An OpenSearch instance that holds data and\n               responds to data upload requests. See also dedicated master node . See also node . Data Pipeline AWS Data Pipeline is a web service for processing and moving data between different AWS compute and\n               storage services, as well as on-premises data sources, at specified intervals. See also https://aws.amazon.com/datapipeline . data schema See schema . data source The database, file, or repository that provides information required by an\n               application or database. For example, in OpsWorks , valid data sources include an instance for a stack's MySQL layer or a stack's Amazon RDS service layer. In Amazon Redshift , valid data sources include text\n               files in an Amazon S3 bucket , in an Amazon EMR cluster, or on a remote host that a cluster can access through an\n               SSH connection. See also datasource . database engine The database software and version running on the DB instance . database name The name of a database hosted in a DB instance . A DB instance can host multiple databases, but databases\n               hosted by the same DB instance must each have a unique name within that instance. dataset Amazon Personalize : A container for the data used by Amazon Personalize. There are three types of Amazon Personalize datasets: Users, Items, and Interactions. See also Interactions dataset . See also Users dataset . See also Items dataset . dataset group Amazon Personalize : A container for Amazon Personalize components, including datasets, event trackers, solutions, filters, campaigns, and batch inference jobs. A dataset group organizes your resources into independent collections, so resources from one dataset group can\u2019t influence resources in any other dataset group. See also dataset . See also event tracker . See also solution . See also campaign . datasource Amazon ML : An object\n               that contains metadata about the input data. Amazon ML reads the input data, computes\n               descriptive statistics on its attributes, and stores the statistics\u2014along with\n               a schema and other information\u2014as part of the datasource object. Amazon ML uses\n               datasources to train and evaluate a machine learning model and generate batch\n               predictions. See also data source . DataSync AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between storage systems and services. See also https://aws.amazon.com/datasync . DB compute class The size of the database compute platform used to run the instance. DB instance An isolated database environment running in the cloud. A DB instance can contain\n               multiple user-created databases. DB instance identifier User-supplied identifier for the DB instance. The identifier must be unique for\n               that user in an AWS Region . DB parameter group A container for database engine parameter values that apply to one or more DB instances . DB security group A method that controls access to the DB instance . By default, network access is turned off to DB instances.\n               After inbound traffic is configured for a security group , the same rules apply to all DB instances associated\n               with that group. DB snapshot A user-initiated point backup of a DB instance . Dedicated Host A physical server with EC2 instance capacity fully dedicated to a user. Dedicated Instance An instance that's physically isolated\n               at the host hardware level and launched within a Amazon VPC . dedicated master node OpenSearch Service : An OpenSearch instance that performs\n               cluster management tasks, but doesn't hold data or respond to data upload requests.\n               Amazon OpenSearch Service (OpenSearch Service) uses dedicated master nodes to increase cluster stability. See also data node . See also node . Dedicated Reserved Instance An option that you purchase to guarantee that sufficient capacity will be\n               available to launch Dedicated Instances into a Amazon VPC . AWS DeepComposer AWS DeepComposer is a web service designed specifically to educate developers through tutorials, sample code, and training data. See also https://aws.amazon.com/deepcomposer . AWS DeepLens AWS DeepLens is a tool that provides AWS customers with a centralized place to search, discover, and connect with trusted APN Technology and Consulting Partners, based on customers' business needs. See also https://aws.amazon.com/deeplens . AWS DeepRacer AWS DeepRacer is a cloud-based 3D racing simulator, global racing league, and fully autonomous 1/18th-scale race car driven by reinforcement learning. See also https://aws.amazon.com/deepracer . delegation Within a single AWS account : Giving\n               AWS users access to resources your AWS account. Between two AWS accounts: Setting up a trust between the account that owns the\n               resource (the trusting account), and the account that contains the users that need to\n               access the resource (the trusted account). See also trust policy . delete marker An object with a key and version ID, but without content. Amazon S3 inserts delete markers automatically into versioned buckets when an object is deleted. deliverability The likelihood that an email message arrives at its intended destination. deliveries The number of email messages, sent through Amazon SES , that were accepted by an internet service provider\n               (ISP) for\n               delivery to recipients over a period of\n               time. deny The result of a policy statement that\n               includes deny as the effect, so that a specific action or actions are expressly\n               forbidden for a user, group, or role. Explicit deny take precedence over explicit allow . deployment configuration CodeDeploy : A set of deployment\n               rules and success and failure conditions used by the service during a\n               deployment. deployment group CodeDeploy : A set of\n               individually tagged instances or EC2 instances in Auto Scaling groups , or both. Description property A property added to parameters, resources , resource properties, mappings, and outputs to help you to\n               document CloudFormation template\n               elements. detailed monitoring Monitoring of AWS provided metrics derived at a 1-minute frequency. Detective Amazon Detective is a service that collects log data from your AWS resources to analyze and identify\n               the root cause of security findings or suspicious activities. The Detective behavior\n               graph provides visualizations to help you to determine the nature and extent of\n               possible security issues and conduct an efficient investigation. See also https://aws.amazon.com/detective/ . Device Farm AWS Device Farm is an app testing service that you can use to test Android, iOS, and web\n               apps on real, physical phones and tablets that are hosted by AWS. See also https://aws.amazon.com/device-farm/ . Amazon DevOps\u00a0Guru Amazon DevOps\u00a0Guru is a fully managed operations service powered by machine learning (ML), designed to improve an application's operational performance and availability. See also https://aws.amazon.com/devops-guru/ . dimension A name\u2013value pair (for example, InstanceType=m1.small, or\n               EngineName=mysql), that contains additional information to identify a metric. Direct Connect AWS Direct Connect is a web service that simplifies establishing a dedicated network connection from\n               your premises to AWS. Using AWS Direct Connect, you can establish private connectivity\n               between AWS and your data center, office, or colocation environment. See also https://aws.amazon.com/directconnect . Directory Service AWS Directory Service is a managed service for connecting your AWS resources to an existing on-premises Microsoft Active Directory or to\n               set up and operate a new, standalone directory in the AWS Cloud. See also https://aws.amazon.com/directoryservice . discussion forums A place where AWS users can post technical questions and feedback to help\n               accelerate their development efforts and to engage with the AWS community. For more\n               information, see the Amazon Web Services Discussion Forums . distribution A link between an origin server (such as an Amazon S3 bucket ) and a domain name, which CloudFront automatically assigns.\n               Through this link, CloudFront identifies the object you have stored in your origin server . DKIM DomainKeys Identified Mail is a standard that email senders use to sign their\n               messages. ISPs use those signatures to verify that messages are legitimate. For more\n               information, see https://tools.ietf.org/html/rfc6376 . AWS DMS AWS Database Migration Service is a web service that can help you migrate data to and from many widely used\n               commercial and open-source databases. See also https://aws.amazon.com/dms . DNS See Domain Name System . Docker image A layered file system template that's the basis of a Docker container . Docker images can comprise\n               specific operating systems or applications. document CloudSearch : An item that can be\n               returned as a search result. Each document has a collection of fields that contain\n               the data that can be searched or returned. The value of a field can be either a\n               string or a number. Each document must have a unique ID and at least one field. document batch CloudSearch : A collection of add and\n               delete document operations. You use the document service API to submit batches to\n               update the data in your search domain. document service API CloudSearch : The API call that you\n               use to submit document batches to update the data in a search domain. document service endpoint CloudSearch : The URL that you\n               connect to when sending document updates to an Amazon CloudSearch domain. Each search domain has a\n               unique document service endpoint that remains the same for the life of the\n               domain. Amazon DocumentDB Amazon DocumentDB (with MongoDB compatibility) is a managed database service that you can use to set up, operate, and scale\n               MongoDB-compatible databases in the cloud. See also https://aws.amazon.com/documentdb/ . domain OpenSearch Service : The hardware, software, and data exposed\n               by Amazon OpenSearch Service (OpenSearch Service) endpoints. An OpenSearch Service domain is a service wrapper around an OpenSearch\n               cluster. An OpenSearch Service domain encapsulates the engine instances that process OpenSearch Service requests,\n               the indexed data that you want to search, snapshots of the domain, access policies,\n               and metadata. See also cluster . See also Elasticsearch . Domain Name System Domain Name System is a service that routes internet traffic to websites by translating human-readable\n               domain names (for example, www.example.com ) into the numeric IP\n               addresses, such as 192.0.2.1, which computers use to connect to each other. Donation button An HTML-coded button to provide a simple and secure way for US-based,\n               IRS-certified 501(c)(3) nonprofit organizations to solicit donations. DynamoDB Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable\n               performance with seamless scalability. See also https://aws.amazon.com/dynamodb/ . Amazon DynamoDB Encryption Client Amazon DynamoDB Encryption Client is a software library that helps you protect your table data before you send it to DynamoDB . Amazon DynamoDB Storage Backend for\n            Titan Amazon DynamoDB Storage Backend for Titan is a graph database implemented on top of Amazon DynamoDB.\n               Titan is a scalable graph database optimized for storing and querying graphs. See also https://aws.amazon.com/dynamodb/ . DynamoDB Streams Amazon DynamoDB Streams is an AWS service that captures a time-ordered sequence of item-level modifications\n               in any Amazon DynamoDB table. This service also stores this information in a log for up to\n               24 hours. Applications can access this log and view the data items as they appeared\n               before and after they were modified, in near-real time. See also https://aws.amazon.com/dynamodb/ . E Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Amazon EBS Amazon Elastic Block Store is a service that provides block level storage volumes or use with EC2 instances . See also https://aws.amazon.com/ebs . Amazon EBS-backed AMI An Amazon EBS-backed AMI is a type of Amazon Machine Image\n            (AMI) whose instances use an Amazon EBS volume as their root device. Compare this\n               with instances launched from instance store-backed\n                  AMIs , which use the instance store as the root device. Amazon EC2 Amazon Elastic Compute Cloud is a web service for launching and managing Linux/UNIX and Windows Server instances in Amazon data\n               centers. See also https://aws.amazon.com/ec2 . Amazon EC2 Auto Scaling Amazon EC2 Auto Scaling is a web service that launches or terminates instances automatically based on user-defined policies , schedules, \n               and health checks . See also https://aws.amazon.com/ec2/autoscaling . EC2 instance A compute instance in the Amazon EC2 service. Other AWS services use the\n               term EC2 instance to distinguish these instances from other\n               types of instances they support. Amazon ECR Amazon Elastic Container Registry (Amazon ECR) is a fully managed Docker container registry that you can use to store, manage, and\n               deploy Docker container images. Amazon ECR is integrated with Amazon ECS and IAM . See also https://aws.amazon.com/ecr . Amazon ECS Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that you can use to run, stop, and manage Docker containers on a cluster of EC2 instances. See also https://aws.amazon.com/ecs . edge location edge location is a data center that an AWS service uses to perform service-specific operations.\n               For example, CloudFront uses edge\n               locations to cache copies of your content, so the content is closer to your users and\n               can be delivered faster regardless of their location. Route\u00a053 uses edge locations to speed up the response to\n               public DNS queries. Amazon EFS Amazon Elastic File System is a file storage service for EC2 instances . Amazon EFS provides an interface\n               that you can use to create and configure file systems. Amazon EFS storage capacity grows\n               and shrinks automatically as you add and remove files. See also https://aws.amazon.com/efs/ . Amazon EKS Amazon Elastic Kubernetes Service is a managed service that you can use to run Kubernetes on AWS without needing to\n               stand up or maintain your own Kubernetes control plane. See also https://aws.amazon.com/eks/ . Elastic A company that provides open-source solutions\u2014including OpenSearch, Logstash,\n               Kibana, and Beats\u2014that take data from any source and search, analyze, and visualize\n               it in real time. Amazon OpenSearch Service (OpenSearch Service) is an AWS managed service for deploying, operating, and scaling\n               OpenSearch in the AWS Cloud. See also OpenSearch Service . See also Elasticsearch . Elastic Beanstalk AWS Elastic Beanstalk is a web service for deploying and managing applications in the AWS Cloud without\n               worrying about the infrastructure that runs those applications. See also https://aws.amazon.com/elasticbeanstalk . Elastic Block Store See Amazon EBS . Elastic Inference Amazon Elastic Inference is a resource that customers can use to attach low-cost GPU-powered acceleration to Amazon EC2 and SageMaker instances, or Amazon ECS tasks, to reduce the cost of running deep learning inference by up to 75%. See also https://aws.amazon.com/machine-learning/elastic-inference . Elastic IP address A fixed (static) IP address that you have allocated in Amazon EC2 or Amazon VPC and then attached to an instance . Elastic IP addresses are associated\n               with your account, not a specific instance. They are elastic because you can easily allocate, attach, detach, and free them as your needs change.\n               Unlike traditional static IP addresses, Elastic IP addresses allow you to mask\n               instance or Availability Zone failures by rapidly remapping\n               your public IP addresses to another instance. ELB Elastic Load Balancing is a web service that improves an application's availability by distributing incoming\n               traffic between two or more EC2 instances . See also https://aws.amazon.com/elasticloadbalancing . elastic network interface An additional network interface that can be attached to an instance . Elastic network interfaces include\n               a primary private IP address, one or more secondary private IP addresses, an Elastic\n               IP Address (optional), a MAC address, membership in specified security groups , a description, and a\n               source/destination check flag. You can create an elastic network interface, attach it\n               to an instance, detach it from an instance, and attach it to another instance. Elastic Transcoder Amazon Elastic Transcoder is a cloud-based media transcoding service. Elastic Transcoder is a highly scalable tool for\n               converting (or transcoding ) media files from their source format\n               into versions that play on devices such as smartphones, tablets, and PCs. See also https://aws.amazon.com/elastictranscoder/ . ElastiCache Amazon ElastiCache is a web service that simplifies deploying, operating, and scaling an in-memory cache\n               in the cloud. The service improves the performance of web applications by providing\n               information retrieval from fast, managed, in-memory caches, instead of relying\n               entirely on slower disk-based databases. See also https://aws.amazon.com/elasticache/ . Elasticsearch An open-source, real-time distributed search and analytics engine used for\n               full-text search, structured search, and analytics. OpenSearch was developed by the\n               Elastic company. Amazon OpenSearch Service (OpenSearch Service) is an AWS managed service for deploying, operating, and scaling\n               OpenSearch in the AWS Cloud. See also OpenSearch Service . See also Elastic . AWS Elemental MediaConnect AWS Elemental MediaConnect is a fully-managed live video distribution service that reliably and securely ingests video into the AWS Cloud and transports it to multiple destinations within the AWS network and the internet. See also https://aws.amazon.com/mediaconnect . AWS Elemental MediaConvert AWS Elemental MediaConvert is a file-based media conversion service that transforms content into formats for traditional broadcast and internet streaming. See also https://aws.amazon.com/mediaconvert . AWS Elemental MediaLive AWS Elemental MediaLive is a cloud-based live video encoding service that creates high-quality streams for delivery to broadcasts and internet-connected devices. See also https://aws.amazon.com/medialive . AWS Elemental MediaPackage AWS Elemental MediaPackage is a highly-scalable video origination and packaging service that delivers video securely and reliably. See also https://aws.amazon.com/mediapackage . AWS Elemental MediaStore AWS Elemental MediaStore is a storage service optimized for media that provides the performance, consistency,\n               and low latency required to deliver live and on-demand video content at scale. See also https://aws.amazon.com/mediastore . AWS Elemental MediaTailor AWS Elemental MediaTailor is a channel assembly and personalized ad-insertion service for over-the-top (OTT) video and audio applications. See also https://aws.amazon.com/mediatailor . EMP The AWS End-of-Support Migration Program for Windows Server provides the technology and guidance to migrate your applications running on Windows Server 2003, Windows Server 2008, and Windows Server 2008 R2 to the latest, supported versions of Windows Server running on Amazon Web Services (AWS). Amazon EMR Amazon Elastic Map Reduce is a web service that you can use to process large amounts of data efficiently. Amazon EMR\n               uses Hadoop processing combined with several\n               AWS products to do such tasks as web indexing, data mining, log file analysis,\n               machine learning, scientific simulation, and data warehousing. See also https://aws.amazon.com/elasticmapreduce . encrypt To use a mathematical algorithm to make data unintelligible to unauthorized users . Encryption also gives authorized\n               users a method (such as a key or password) to convert the altered data back to its\n               original state. encryption context A set of key\u2013value pairs that contains additional information associated\n               with AWS KMS \u2013encrypted\n               information. AWS Encryption SDK AWS Encryption SDK is a client-side encryption library that you can use to encrypt and decrypt data\n               using industry standards and best practices. See also https://aws.amazon.com/blogs/security/tag/aws-encryption-sdk/ . endpoint A URL that identifies a host and port as the entry point for a web service. Every\n               web service request contains an endpoint. Most AWS products provide endpoints for a\n               Region to enable faster connectivity. ElastiCache : The DNS name of a cache node . Amazon RDS : The DNS name of a DB instance . CloudFormation : The DNS name or\n               IP address of the server that receives an HTTP request. endpoint port ElastiCache : The port number used by\n               a cache node . Amazon RDS : The port number used by a DB instance . envelope encryption The use of a master key and a data key to algorithmically protect data. The master\n               key is used to encrypt and decrypt the data key and the data key is used to encrypt\n               and decrypt the data itself. environment Elastic Beanstalk : A specific running instance\n               of an application . The application has\n               a CNAME and includes an application version and a customizable configuration (which\n               is inherited from the default container type). CodeDeploy : Instances in a\n               deployment group in a blue/green deployment. At the start of a blue/green deployment,\n               the deployment group is made up of instances in the original environment. At the end\n               of the deployment, the deployment group is made up of instances in the replacement\n               environment. environment configuration A collection of parameters and settings that define how an environment and its\n               associated resources behave. ephemeral store See instance store . epoch The date from which time is measured. For most Unix environments, the epoch is\n               January 1, 1970. ETL See extract, transform, and load (ETL) . evaluation Amazon Machine Learning: The process of measuring the predictive performance of a machine\n               learning (ML) model. Also a machine learning object that stores the details and result of an ML model\n               evaluation. evaluation datasource The data that Amazon Machine Learning uses to evaluate the predictive accuracy of a machine\n               learning model. event Amazon Personalize : A user activity\u2014such as a click, a purchase, or a video viewing\u2014that you record and upload to an Amazon Personalize Interactions dataset. You record events individually in real time or record and upload events in bulk. See also dataset . See also Interactions dataset . event tracker Amazon Personalize : Specifies a destination dataset group for event data that you record in real time. When you record events in real time, you provide the ID of the event tracker so that Amazon Personalize knows where to add the data. See also dataset group . See also event . EventBridge Amazon EventBridge is a serverless event bus service that you can use to connect your applications with\n               data from a variety of sources and routes that data to targets such as AWS Lambda. You\n               can set up routing rules to determine where to send your data to build application\n               architectures that react in real time to all of your data sources. See also https://aws.amazon.com/eventbridge/ . eventual consistency The method that AWS services use to achieve high availability. This involves\n               replicating data across multiple servers in Amazon data centers. When data is written\n               or updated and Success is returned, all copies of the data are updated.\n               However, it takes time for the data to propagate to all storage locations. The data\n               will eventually be consistent, but an immediate read might not show the change.\n               Consistency is usually reached within seconds. See also data consistency . See also eventually consistent read . See also strongly consistent read . eventually consistent read A read process that returns data from only one Region and might not show the most\n               recent write information. However, if you repeat your read request after a short\n               time, the response should eventually return the latest data. See also data consistency . See also eventual consistency . See also strongly consistent read . eviction The deletion by CloudFront of\n               an object from an edge location before its\n               expiration time. If an object in an edge location isn't frequently requested, CloudFront\n               might evict the object (remove the object before its expiration date) to make room\n               for objects that are more popular. exbibyte (EiB) A contraction of exa binary byte. An exbibyte (EiB) is 2^60 or\n               1,152,921,504,606,846,976 bytes. An exabyte (EB) is 10^18 or\n               1,000,000,000,000,000,000 bytes. 1,024 EiB is a zebibyte (ZiB) . expiration For CloudFront caching, the\n               time when CloudFront stops responding to user requests with an object. If you don't use\n               headers or CloudFront distribution settings to specify how long you want objects to stay in an edge location , the objects expire after 24 hours.\n               The next time a user requests an object that has expired, CloudFront forwards the request\n               to the origin . explicit impressions Amazon Personalize : A list of items that you manually add to an Amazon Personalize Interactions dataset to influence future recommendations. Unlike implicit impressions , where Amazon Personalize automatically derives the impressions data, you choose what to include in explicit impressions. See also recommendations . See also Interactions dataset . See also impressions data . See also implicit impressions . explicit launch permission An Amazon Machine Image\n            (AMI) launch\n               permission granted to a specific AWS account . exponential backoff A strategy that incrementally increases the wait between retry attempts in order\n               to reduce the load on the system and increase the likelihood that repeated requests\n               will succeed. For example, client applications might wait up to 400 milliseconds\n               before attempting the first retry, up to 1600 milliseconds before the second, and up to\n               6400 milliseconds (6.4 seconds) before the third. expression CloudSearch : A numeric expression\n               that you can use to control how search hits are sorted. You can construct Amazon CloudSearch\n               expressions using numeric fields, other rank expressions, a document's default\n               relevance score, and standard numeric operators and functions. When you use the sort option to specify an expression in a search request, the\n               expression is evaluated for each search hit and the hits are listed according to\n               their expression values. extract, transform, and load (ETL) A process that's used to integrate data from multiple sources. Data is collected\n               from sources (extract), converted to an appropriate format (transform), and written\n               to a target data store (load) for purposes of analysis and querying. ETL tools combine these three functions to consolidate and move data from one\n               environment to another. AWS Glue is a fully\n               managed ETL service for discovering and organizing data, transforming it, and making\n               it available for search and analytics. F Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z facet CloudSearch : An index field that\n               represents a category that you want to use to refine and filter search\n               results. facet enabled CloudSearch : An index field option\n               that enables facet information to be calculated for the field. AWS Fargate AWS Fargate is a serverless, pay-as-you-go compute engine that you can use to\n               build applications on AWS. You can use Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS) to maintain\n               container applications using AWS Fargate. See also https://aws.amazon.com/fargate/ . Fault Injection Simulator (AWS FIS) AWS Fault Injection Service is a managed service that you can use to perform fault injection experiments on your AWS workloads. See also https://aws.amazon.com/fis . FBL See feedback loop (FBL) . feature transformation Amazon Machine Learning: The machine learning process of constructing more predictive input\n               representations or \u201cfeatures\u201d from the raw input variables to optimize a machine\n               learning model\u2019s ability to learn and generalize. Also known as data\n                  transformation or feature engineering . federated identity management (FIM) Allows individuals to sign in to different networks or services, using the same\n               group or personal credentials to access data across all networks. With identity\n               federation in AWS, external identities (federated users) are granted secure access\n               to resources in an AWS account without having to create IAM users . These external identities can come\n               from a corporate identity store (such as LDAP or Windows Active Directory) or from a\n               third party (such as Login with Amazon, Facebook, or Google). AWS federation also\n               supports SAML 2.0. federated user See federated identity management\n            (FIM) . federation See federated identity management\n            (FIM) . feedback loop (FBL) The mechanism by which a mailbox provider (for example, an internet service provider\n               (ISP) )\n               forwards a recipient 's complaint back to the sender . field weight The relative importance of a text field in a search index. Field weights control\n               how much matches in particular text fields affect a document's relevance\n               score. filter A criterion that you specify to limit the results when you list or describe your Amazon EC2 resources . filter query A way to filter search results without affecting how the results are scored and\n               sorted. Specified with the CloudSearch fq parameter. FIM See federated identity management\n            (FIM) . FinSpace Amazon FinSpace is a data management and analytics service purpose-built for the financial services industry (FSI). See also https://aws.amazon.com/finspace . Firehose See Firehose . Firewall Manager AWS Firewall Manager is a service that you use with AWS WAF to simplify your AWS WAF administration\n               and maintenance tasks across multiple accounts and resources. With AWS Firewall Manager, you set\n               up your firewall rules only once. The service automatically applies your rules across\n               your accounts and resources, even as you add new resources. See also https://aws.amazon.com/firewall-manager . Forecast Amazon Forecast is a fully managed service that uses statistical and machine learning algorithms to produce highly accurate time-series forecasts. See also https://aws.amazon.com/forecast/ . format version See template format version . forums See discussion forums . function See intrinsic function . fuzzy search A simple search query that uses approximate string matching (fuzzy matching) to\n               correct for typographical errors and misspellings. G Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z GameKit AWS GameKit is an open-source SDK and game engine plugin that empowers game\n               developers to build and deploy cloud-based features with AWS from their game\n               engine. See also https://aws.amazon.com/gamekit/ . Amazon GameLift Amazon GameLift is a managed service for deploying, operating, and scaling session-based multiplayer\n               games. See also https://aws.amazon.com/gamelift/ . GameSparks Amazon GameSparks is a fully managed AWS service that provides a multi-service backend\n               for game developers. See also https://aws.amazon.com/gamesparks/ . geospatial search A search query that uses locations specified as a latitude and longitude to\n               determine matches and sort the results. gibibyte (GiB) A contraction of giga binary byte, a gibibyte is 2^30 or 1,073,741,824 bytes. A\n               gigabyte (GB) is 10^9 or 1,000,000,000 bytes. 1,024 GiB is a tebibyte (TiB) . GitHub A web-based repository that uses Git for version control. Global Accelerator AWS Global Accelerator is a network layer service that you use to create accelerators that direct traffic to\n               optimal endpoints over the AWS global network. This improves the availability and\n               performance of your internet applications that are used by a global audience. See also https://aws.amazon.com/global-accelerator . global consistency An active-active strategy in which all reads and writes for a workload are handled in the Region where the request originates and are replicated synchronously to all other Regions in the architecture. See also read local/write global , read local/write local . global secondary index An index with a partition key and a sort key that can be different from those on\n               the table. A global secondary index is considered global because queries on the index\n               can span all of the data in a table, across all partitions. See also local secondary index . AWS Glue AWS Glue is a fully managed extract, transform, and load (ETL) service that you can use to catalog data and load\n               it for analytics. With AWS Glue, you can discover your data, develop scripts to\n               transform sources into targets, and schedule and run ETL jobs in a serverless\n               environment. See also https://aws.amazon.com/glue . AWS GovCloud (US) AWS GovCloud (US) is an isolated AWS Region that hosts sensitive workloads in the cloud, ensuring\n               that this work meets the US government's regulatory and compliance requirements. The\n               AWS GovCloud (US) Region adheres to United States International Traffic in Arms\n               Regulations (ITAR), Federal Risk and Authorization Management Program (FedRAMP)\n               requirements, Department of Defense (DOD) Cloud Security Requirements Guide (SRG)\n               Levels 2 and 4, and Criminal Justice Information Services (CJIS) Security Policy\n               requirements. See also https://aws.amazon.com/govcloud-us/ . grant AWS KMS : A mechanism for giving AWS principals long-term permissions to\n               use KMS keys. grant token A type of identifier that allows the permissions in a grant to take effect\n               immediately. ground truth The observations used in the machine learning (ML) model training process that\n               include the correct value for the target attribute. To train an ML model to predict\n               house sales prices, the input observations would typically include prices of previous\n               house sales in the area. The sale prices of these houses constitute the ground\n               truth. group A collection of IAM users . You can use IAM groups to\n               simplify specifying and managing permissions for multiple users. GuardDuty Amazon GuardDuty is a continuous security monitoring service. Amazon GuardDuty can help to identify\n               unexpected and potentially unauthorized or malicious activity in your AWS\n               environment. See also https://aws.amazon.com/guardduty/ . H Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Hadoop Software that enables distributed processing for big data by using clusters and\n               simple programming models. For more information, see http://hadoop.apache.org . hard bounce A persistent email delivery failure such as \"mailbox does not exist.\" hardware VPN A hardware-based IPsec VPN connection over the internet. AWS Health AWS Health is a service that provides ongoing visibility into AWS customers' accounts and the availability of their AWS services and resources. See also https://aws.amazon.com/premiumsupport/technology/aws-health-dashboard . health check A system call to check on the health status of each instance in an Amazon EC2 Auto Scaling group. HealthLake AWS HealthLake is a HIPAA-eligible service that helps customers store, query, and generate artificial intelligence (AI) and machine learning (ML) insights from healthcare data and enables healthcare data interoperability. See also https://aws.amazon.com/healthlake . highlight enabled CloudSearch : An index field option\n               that enables matches within the field to be highlighted. highlights CloudSearch : Excerpts returned with\n               search results that show where the search terms appear within the text of the\n               matching documents. high-quality email Email that recipients find valuable and want to receive. Value means different\n               things to different recipients and can come in such forms as offers, order\n               confirmations, receipts, or newsletters. hit A document that matches the criteria specified in a search request. Also referred\n               to as a search result . HMAC Hash-based Message Authentication Code is a specific construction for calculating a\n               message authentication code (MAC) involving a cryptographic hash function in\n               combination with a secret key. You can use it to verify both the data integrity and\n               the authenticity of a message at the same time. AWS calculates the HMAC using a\n               standard, cryptographic hash algorithm, such as SHA-256. hosted zone A collection of resource record sets that Route\u00a053 hosts. Similar to a\n               traditional DNS zone file, a hosted zone represents a collection of records that are\n               managed together under a single domain name. hot standby An active-passive disaster recovery strategy in which a workload is fully scaled up in both the primary and standby Regions, but serves traffic from only the primary Region. See also back up and restore , pilot light , warm standby . HRNN Amazon Personalize : A hierarchical recurrent neural network machine learning algorithm that models changes in user behavior and predicts the items that a user might interact with in personal recommendation applications. HTTP-Query See Query . HVM virtualization Hardware Virtual Machine virtualization. Allows the guest VM to run as though it's\n               on a native hardware platform, except that it still uses paravirtual (PV) network and\n               storage drivers for improved performance. See also PV virtualization . I Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z IAM AWS Identity and Access Management is a web service that Amazon Web Services\n            (AWS) customers can use to manage users and user\n               permissions within AWS. See also https://aws.amazon.com/iam . IAM Access Analyzer Access Management Access Analyzer is a feature of IAM that you can use to\n               identify the resources in your organization and accounts that are shared with an\n               external entity. Example resources include Amazon S3 buckets or IAM roles. See also https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-aws-identity-and-access-management-access-analyzer/ . IAM group See group . IAM Identity Center AWS IAM Identity Center is a cloud-based service that brings together administration of users and their access to AWS accounts and\n               cloud applications. You can control single sign-on access and user permissions across all\n               your AWS accounts in AWS Organizations. See also https://aws.amazon.com/single-sign-on/ . IAM policy simulator See policy simulator . IAM role See role . IAM user See user . Identity and Access Management See IAM . identity provider (IdP) An IAM entity that holds metadata\n               about external identity providers. IdP See identity provider (IdP) . image See Amazon Machine Image\n            (AMI) . Image Builder EC2 Image Builder is a service that facilitates building, maintaining, and distributing customized server images that launch EC2 instances, or that run in Docker containers. See also https://aws.amazon.com/image-builder . implicit impressions Amazon Personalize : The recommendations that your application shows a user. Unlike explicit impressions , where you manually record each impression, Amazon Personalize automatically derives implicit impressions from your recommendation data. See also recommendations . See also impressions data . See also explicit impressions . import log A report that contains details about how Import/Export processed your data. Import/Export AWS Import/Export is a service for transferring large amounts of data between AWS and portable storage\n               devices. See also https://aws.amazon.com/importexport . import/export station A machine that uploads or downloads your data to or from Amazon S3 . impressions data Amazon Personalize : The list of\n               items that you presented to a user when they interacted with a particular item such as by\n               clicking it, watching it, or purchasing it. Amazon Personalize uses impressions data to calculate\n               the relevance of new items for a user based on how frequently users have selected or\n               ignored the same item. See also explicit impressions . See also implicit impressions . index See search index . index field A name\u2013value pair that's included in an CloudSearch domain's index. An index field can contain text or numeric\n               data, dates, or a location. indexing options Configuration settings that define an CloudSearch domain's index fields, how document data is mapped to\n               those index fields, and how the index fields can be used. inline policy An IAM policy that's embedded in a single IAM user , group , or role . in-place deployment CodeDeploy: A deployment method where the application on each instance in the\n               deployment group is stopped, the latest application revision is installed, and the\n               new version of the application is started and validated. You can choose to use a load\n               balancer so each instance is deregistered during its deployment and then restored to\n               service after the deployment is complete. input data Amazon Machine Learning: The observations that you provide to Amazon Machine Learning to train and evaluate a\n               machine learning model and generate predictions. Amazon Inspector Amazon Inspector is an automated security assessment service that helps improve the security and\n               compliance of applications deployed on AWS. Amazon Inspector automatically assesses\n               applications for vulnerabilities or deviations from best practices. After performing\n               an assessment, Amazon Inspector produces a detailed report with prioritized steps for\n               remediation. See also https://aws.amazon.com/inspector . instance A copy of an Amazon Machine Image\n            (AMI) running as a virtual server in the AWS Cloud. instance family A general instance type grouping\n               using either storage or CPU capacity. instance group A Hadoop cluster contains one master\n               instance group that contains one master node , a core instance group that contains one or more core node and an optional task node instance group, which can contain\n               any number of task nodes. instance profile A container that passes IAM role information to an EC2 instance at launch. instance store Disk storage that's physically attached to the host computer for an EC2 instance , and therefore has the same\n               lifespan as the instance. When the instance is terminated, you lose any data in the\n               instance store. instance store-backed AMI A type of Amazon Machine Image\n            (AMI) whose instances use an instance store volume as the root device. Compare this with\n               instances launched from Amazon EBS-backed AMIs , which use\n               an Amazon EBS volume as the root device. instance type A specification that defines the memory, CPU, storage capacity, and usage cost for\n               an instance . Some instance types are for\n               standard applications, whereas others are for CPU-intensive, memory-intensive\n               applications. Interactions dataset Amazon Personalize : A container for historical and real-time data collected from interactions between users and items (called events). Interactions data can include impressions data and contextual metadata. See also dataset . See also event . See also impressions data . See also contextual metadata . internet gateway Connects a network to the internet. You can route traffic for IP addresses outside\n               your Amazon VPC to the internet gateway. internet service provider (ISP) A company that provides subscribers with access to the internet. Many ISPs are\n               also mailbox providers . Mailbox\n               providers are sometimes referred to as ISPs, even if they only provide mailbox\n               services. intrinsic function A special action in a CloudFormation template that assigns values to properties not\n               available until runtime. These functions follow the format Fn::Attribute , such as Fn::GetAtt . Arguments for\n               intrinsic functions can be parameters, pseudo parameters, or the output of other\n               intrinsic functions. AWS IoT 1-Click AWS IoT 1-Click is a service that simple devices can use to launch AWS Lambda functions. See also https://aws.amazon.com/iot-1-click . AWS IoT Analytics AWS IoT Analytics is a fully managed service used to run sophisticated analytics on massive volumes of\n               IoT data. See also https://aws.amazon.com/iot-analytics . AWS IoT Core AWS IoT Core is a managed cloud platform that lets connected devices easily and securely interact\n               with cloud applications and other devices. See also https://aws.amazon.com/iot . AWS IoT Device Defender AWS IoT Device Defender is an AWS IoT security service that you can use to audit the configuration of your\n               devices, monitor your connected devices to detect abnormal behavior, and to mitigate\n               security risks. See also https://aws.amazon.com/iot-device-defender . AWS IoT Device Management AWS IoT Device Management is a service used to securely onboard, organize, monitor, and remotely manage IoT\n               devices at scale. See also https://aws.amazon.com/iot-device-management . AWS IoT Events AWS IoT Events is a fully managed AWS IoT service that you can use to detect and respond to events\n               from IoT sensors and applications. See also https://aws.amazon.com/iot-events . AWS IoT FleetWise AWS IoT FleetWise is a service that you can use to collect, transform, and transfer vehicle data to the cloud at scale. See also https://aws.amazon.com/iot-fleetwise . AWS IoT Greengrass AWS IoT Greengrass is a software that you can use to run local compute, messaging, data caching, sync, and\n               ML inference capabilities for connected devices in a secure way. See also https://aws.amazon.com/greengrass . AWS IoT RoboRunner AWS IoT RoboRunner is a solution that provides infrastructure for integrating robots with work management systems and building robotics fleet management applications. See also https://aws.amazon.com/roborunner . AWS IoT SiteWise AWS IoT SiteWise is a managed service that you can use to collect, organize, and analyze data from\n               industrial equipment at scale. See also https://aws.amazon.com/iot-sitewise . AWS IoT Things Graph AWS IoT Things Graph is a service that you can use to visually connect different devices and web services\n               to build IoT applications. See also https://aws.amazon.com/iot-things-graph . IP address A numerical address (for example, 192.0.2.44) that networked devices use to\n               communicate with one another using the Internet Protocol (IP). Each EC2 instance is assigned two IP addresses\n               at launch, which are directly mapped to each other through network address\n               translation ( NAT ): a private IP address\n               (following RFC 1918) and a public IP address. Instances launched in a VPC are assigned only a private IP address. Instances launched in your\n               default VPC are assigned both a private IP address and a public IP address. IP match condition AWS WAF : An attribute that specifies the\n               IP addresses or IP address ranges that web requests originate from. Based on the\n               specified IP addresses, you can configure AWS WAF to allow or block web requests to AWS resources such as Amazon CloudFront distributions. AWS IQ AWS IQ is a cloud service that AWS customers can use to find, engage, and pay AWS Certified third-party experts for on-demand project work. See also https://iq.aws.amazon.com . ISP See internet service provider\n               (ISP) . issuer The person who writes a policy to grant\n               permissions to a resource . The issuer (by\n               definition) is always the resource owner. AWS doesn't permit Amazon SQS users to create policies for resources they don't own. If John is\n               the resource owner, AWS authenticates John's identity when he submits the policy\n               he's written to grant permissions for that resource. item A group of attributes that's uniquely identifiable among all of the other items.\n               Items in DynamoDB are similar in many ways\n               to rows, records, or tuples in other database systems. item exploration Amazon Personalize : The process\n               that Amazon Personalize uses to test different item recommendations, including recommendations of\n               new items with no or little interaction data, and learn how users respond. You\n               configure item exploration at the campaign level for solution versions created with\n               the user-personalization recipe. See also recommendations . See also campaign . See also solution version . See also user-personalization recipe . Items dataset Amazon Personalize : A container\n               for metadata about items, such as price, genre, or availability. See also dataset . item-to-item similarities (SIMS) recipe Amazon Personalize : A RELATED_ITEMS recipe that uses the data from an Interactions dataset to make recommendations for items that are similar to a specified item. The SIMS recipe calculates similarity based on the way users interact with items instead of matching item metadata, such as price or age. See also recipe . See also RELATED_ITEMS recipes . See also Interactions dataset . J Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z job flow Amazon EMR : One or more steps that\n               specify all of the functions to be performed on the data. job ID A five-character, alphanumeric string that uniquely identifies an Import/Export storage device in your\n               shipment. AWS issues the job ID in response to a CREATE JOB email\n               command. job prefix An optional string that you can add to the beginning of an Import/Export log file name to\n               prevent collisions with objects of the same name. See also key prefix . JSON JavaScript Object Notation. A lightweight data interchange format. For information\n               about JSON, see http://www.json.org/ . junk folder The location where email messages that various filters determine to be of lesser\n               value are collected so that they don't arrive in the recipient 's inbox but are still accessible to the recipient. This is\n               also referred to as a spam or bulk\n               folder. K Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Amazon Kendra Amazon Kendra is a search service powered by machine learning (ML) that developers can use to add search capabilities to their applications so their end users can discover information stored within the vast amount of content spread across their company. See also https://aws.amazon.com/kendra/ . key A credential that identifies an AWS account or user to AWS\n               (such as the AWS secret access key ). Amazon S3 , Amazon EMR : The\n               unique identifier for an object in a bucket .\n               Every object in a bucket has exactly one key. Because a bucket and key together\n               uniquely identify each object, you can think of Amazon S3 as a basic data map between the bucket + key , and the object itself. You can uniquely address\n               every object in Amazon S3 through the combination of the web service endpoint, bucket\n               name, and key, as in this example: http://doc.s3.amazonaws.com/2006-03-01/AmazonS3.wsdl , where doc is the name of the bucket, and 2006-03-01/AmazonS3.wsdl is the key. Import/Export : The name of an\n               object in Amazon S3. It's a sequence of Unicode characters whose UTF-8 encoding can't\n               exceed 1024 bytes. If a key (for example, logPrefix + import-log-JOBID) is longer\n               than 1024 bytes, Elastic Beanstalk returns an InvalidManifestField error. IAM : In a policy , a specific characteristic that's the\n               basis for restricting access (such as the current time or the IP address of the\n               requester). Tagging resources: A general tag label that\n               acts like a category for more specific tag values. For example, you might have EC2 instance with the tag key of Owner and the tag value of Jan . You can\n               tag an AWS resource with up to 10\n               key\u2013value pairs. Not all AWS resources can be tagged. key pair A set of security credentials that you use to prove your identity electronically.\n               A key pair consists of a private key and a public key. key prefix A string of characters that is a subset of an object key name, starting with \n               the first character. The prefix can be any length, up to the maximum length of \n               the object key name (1,024 bytes). Amazon Keyspaces Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra-compatible database service. See also https://aws.amazon.com/keyspaces/ . kibibyte (KiB) A contraction of kilo binary byte, a kibibyte is 2^10 or 1,024 bytes. A kilobyte\n               (KB) is 10^3 or 1,000 bytes. 1,024 KiB is a mebibyte (MiB) . Kinesis Amazon Kinesis is a platform for streaming data on AWS. Kinesis offers services that simplify the\n               loading and analysis of streaming data. See also https://aws.amazon.com/kinesis/ . Firehose Amazon Data Firehose is a fully managed service for loading streaming data into AWS. Firehose can capture and\n               automatically load streaming data into Amazon S3 and Amazon Redshift , enabling\n               near real-time analytics with existing business intelligence tools and dashboards.\n               Firehose automatically scales to match the throughput of your data and requires no\n               ongoing administration. It can also batch, compress, and encrypt the data before\n               loading it. See also https://aws.amazon.com/kinesis/firehose/ . Kinesis Data Streams Amazon Kinesis Data Streams is a web service for building custom applications that process or analyze streaming\n               data for specialized needs. Amazon Kinesis Data Streams can continuously capture and store terabytes of\n               data per hour from hundreds of thousands of sources. See also https://aws.amazon.com/kinesis/streams/ . AWS KMS AWS Key Management Service is a managed service that simplifies the creation and control of encryption keys that are used to encrypt data. See also https://aws.amazon.com/kms . KMS key The primary resource in AWS Key Management Service. In general, \n               KMS keys are created, used, and deleted entirely within KMS. KMS supports symmetric and asymmetric KMS keys for encryption and signing. KMS keys can be either customer managed, AWS managed, or AWS owned. For more information, see AWS KMS keys in the AWS Key Management Service Developer Guide . L Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z labeled data In machine learning, data for which you already know the target or \u201ccorrect\u201d\n               answer. Lake Formation AWS Lake Formation is a managed service that makes it easy to set up, secure, and manage your data lakes. Lake Formation helps you discover your data sources and then catalog, cleanse, and transform the data. See also https://aws.amazon.com/lake-formation . Lambda AWS Lambda is a web service that you can use to run code without provisioning or managing\n               servers. You can run code for virtually any type of application or backend service\n               with zero administration. You can set up your code to automatically start from other\n               AWS services or call it directly from any web or mobile app. See also https://aws.amazon.com/lambda/ . launch configuration A set of descriptive parameters used to create new EC2 instances in an Amazon EC2 Auto Scaling activity. A template that an Auto Scaling group uses to launch new EC2 instances. The launch\n               configuration contains information such as the Amazon Machine Image\n            (AMI) ID, the instance\n               type, key pairs, security groups , and\n               block device mappings, among other configuration settings. launch permission An Amazon Machine Image\n            (AMI) attribute that allows users to launch an AMI. Launch Wizard AWS Launch Wizard is a cloud solution that offers a guided way of sizing, configuring, and deploying AWS resources for third-party applications, such as Microsoft SQL Server Always On and HANA based SAP systems, without the need to manually identify and provision individual AWS resources. See also https://aws.amazon.com/launchwizard . Amazon Lex Amazon Lex is a fully managed artificial intelligence (AI) service with advanced natural language models to design, build, test, and deploy conversational interfaces in applications. See also https://aws.amazon.com/lex/ . lifecycle The lifecycle state of the EC2 instance contained in an Auto Scaling group . EC2 instances progress through several states over their lifespan; these include Pending , InService , Terminating and Terminated . lifecycle action An action that can be paused by Auto Scaling, such as launching or terminating an EC2\n               instance. lifecycle hook A feature for pausing Auto Scaling after it launches or terminates an EC2 instance so that\n               you can perform a custom action while the instance isn't in service. Lightsail Amazon Lightsail is a service used to launch and manage a virtual private server with AWS.\n               Lightsail offers bundled plans that include everything you need to deploy a virtual\n               private server, for a low monthly rate. See also https://aws.amazon.com/lightsail/ . load balancer A DNS name combined with a set of ports, which together provide a destination for\n               all requests intended for your application. A load balancer can distribute traffic to\n               multiple application instances across every Availability Zone within a Region . Load balancers can span\n               multiple Availability Zones within an AWS Region into which an Amazon EC2 instance was launched. But load\n               balancers can't span multiple Regions. local secondary index An index that has the same partition key as the table, but a different sort key. A\n               local secondary index is local in the sense that every partition of a local secondary\n               index is scoped to a table partition that has the same partition key value. See also local secondary index . Amazon Location Amazon Location Service is a fully managed service that makes it easy for a developer to add location functionality, such as maps, points of interest, geocoding, routing, tracking, and geofencing, to their applications, without sacrificing data security, user privacy, data quality, or cost. See also https://aws.amazon.com/location/ . logical name A case-sensitive unique string within an CloudFormation template that identifies a resource , mapping , parameter, or output. In an AWS CloudFormation template, each parameter, resource , property, mapping, and output\n               must be declared with a unique logical name. You use the logical name when\n               dereferencing these items using the Ref function. Lookout for Equipment Amazon Lookout for Equipment is a machine learning service that uses data from sensors mounted on factory\n               equipment to detect abnormal behavior so you can take action before machine failures\n               occur. See also https://aws.amazon.com/lookout-for-equipment/ . Lookout for Metrics Amazon Lookout for Metrics is a machine learning (ML) service that automatically detects and diagnoses anomalies in business and operational data, such as a sudden dip in sales revenue or customer acquisition rates. See also https://aws.amazon.com/lookout-for-metrics . Lookout for Vision Amazon Lookout for Vision is a machine learning service that uses computer vision (CV) to find defects in industrial products. Amazon Lookout for Vision can identify missing components in an industrial product, damage to vehicles or structures, irregularities in production lines, and even minuscule defects in silicon wafers\u2014or any other physical item where quality is important. See also https://aws.amazon.com/lookout-for-vision/ . Lumberyard See O3DE . M Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Macie Amazon Macie is a security service that uses machine learning to automatically discover, classify,\n               and protect sensitive data in AWS. See also http://aws.amazon.com/macie/ . Mail Transfer Agent (MTA) Software that transports email messages from one computer to another by using a\n               client-server architecture. mailbox provider An organization that provides email mailbox hosting services. Mailbox providers\n               are sometimes referred to as internet service providers (ISPs) , even if they only provide mailbox\n               services. mailbox simulator A set of email addresses that you can use to test an Amazon SES -based email-sending application without sending\n               messages to actual recipients. Each email address represents a specific scenario\n               (such as a bounce or complaint) and generates a typical response that's specific to\n               the scenario. main route table The default route table that any new Amazon VPC subnet uses for routing. You can associate a\n               subnet with a different route table of your choice. You can also change which route\n               table is the main route table. AWS Mainframe Modernization AWS Mainframe Modernization service is a cloud native platform for migration, modernization, execution, and operation of mainframe applications. See also https://aws.amazon.com/mainframe-modernization . Managed Blockchain Amazon Managed Blockchain is a fully managed service for creating and managing scalable blockchain networks\n               using popular open source frameworks. See also http://aws.amazon.com/managed-blockchain/ . Amazon Managed Grafana Amazon Managed Grafana is a fully managed and secure data visualization service that you can use to instantly query, correlate, and visualize operational metrics, logs, and traces from multiple data sources. See also https://aws.amazon.com/grafana/ . AWS managed key One type of KMS key in AWS KMS . managed policy A standalone IAM policy that you can attach to multiple users , groups , and roles s in your IAM account . Managed policies can either be\n               AWS managed policies (which are created and managed by AWS) or customer managed\n               policies (which you create and manage in your AWS account). AWS managed policy An IAM managed policy that's created and\n               managed by AWS. Amazon Managed Service for Prometheus Amazon Managed Service for Prometheus is a service that provides highly available, secure, and managed monitoring for your\n               containers. See also https://aws.amazon.com/prometheus/ . AWS Management Console AWS Management Console is a graphical interface to manage compute, storage, and other cloud resources . See also https://aws.amazon.com/console . management portal AWS Management Portal for vCenter is a web service for managing your AWS resources using VMware vCenter. You install the portal as a vCenter\n               plugin within your existing vCenter environment. After it's installed, you can migrate\n               VMware VMs to Amazon EC2 and manage AWS\n               resources from within vCenter. See also https://aws.amazon.com/ec2/vcenter-portal/ . manifest When sending a create job request for an import or export\n               operation, you describe your job in a text file called a manifest. The manifest file\n               is a YAML-formatted file that specifies how to transfer data between your storage\n               device and the AWS Cloud. manifest file Amazon Machine Learning: The file used for describing batch predictions. The manifest file\n               relates each input data file with its associated batch prediction results. It's\n               stored in the Amazon S3 output location. mapping A way to add conditional parameter values to an CloudFormation template. You specify\n               mappings in the template's optional Mappings section and retrieve the desired value\n               using the FN::FindInMap function. marker See pagination token . AWS Marketplace AWS Marketplace is a web portal where qualified partners market and sell their software to AWS\n               customers. AWS Marketplace is an online software store that helps customers find,\n               buy, and immediately start using the software and services that run on AWS. See also https://aws.amazon.com/partners/aws-marketplace/ . master node A process running on an Amazon Machine Image\n            (AMI) that keeps track of the work its core and task\n               nodes complete. maximum price The maximum price you pay to launch one or more Spot Instances . If your maximum price\n               exceeds the current Spot price and your\n               restrictions are met, Amazon EC2 launches\n               instances on your behalf. maximum send rate The maximum number of email messages that you can send per second using Amazon SES . mean reciprocal rank at 25 Amazon Personalize : An evaluation metric that assesses the relevance of a model\u2019s highest ranked recommendation. Amazon Personalize calculates this metric using the average accuracy of the model when ranking the most relevant recommendation out of the top 25 recommendations over all requests for recommendations. See also metrics . See also recommendations . mebibyte (MiB) A contraction of mega binary byte. A mebibyte (MiB) is 2^20 or 1,048,576 bytes. A\n               megabyte (MB) is 10^6 or 1,000,000 bytes. 1,024 MiB is a gibibyte (GiB) . member resources See resource . MemoryDB Amazon MemoryDB is a Redis-compatible, durable, in-memory database service that's purpose-built for modern applications with microservices architectures. See also https://aws.amazon.com/memorydb . message ID Amazon SES : A unique identifier that's assigned to\n               every email message that's sent. Amazon SQS : The identifier returned when you send a message to a queue. metadata Information about other data or objects. In Amazon S3 and Amazon EMR metadata takes the form of name\u2013value pairs that describe the object. These\n               include default metadata such as the date last modified and standard HTTP metadata\n               (for example, Content-Type). Users can also specify custom metadata at the time they\n               store an object. In Amazon EC2 metadata includes data\n               about an EC2 instance that the instance\n               can retrieve to determine things about itself, such as the instance type or the IP\n               address. metric An element of time-series data defined by a unique combination of exactly one namespace , exactly one metric name,\n               and between zero and ten dimensions. Metrics and the statistics derived from them are\n               the basis of CloudWatch . metric name The primary identifier of a metric, used with a namespace and optional dimensions. metrics Amazon Personalize : Evaluation data that Amazon Personalize generates when you train a model. You use metrics to evaluate the performance of the model, view the effects of modifying a solution\u2019s configuration, and compare results between solutions that use the same training data but were created with different recipes. See also solution . See also recipe . MFA See multi-factor authentication\n               (MFA) . micro instance A type of EC2 instance that's more\n               economical to use if you have occasional bursts of high CPU activity. AWS Microservice Extractor for .NET AWS Microservice Extractor for .NET is an assistive modernization tool that helps to reduce the time and effort required to break down large, monolithic applications running on the AWS Cloud or on premises into smaller, independent services. These services can be operated and managed independently. Migration Hub AWS Migration Hub is a service that provides a single location to track migration tasks across multiple\n               AWS tools and partner solutions. See also https://aws.amazon.com/migration-hub/ . MIME See Multipurpose Internet Mail\n            Extensions (MIME) . Amazon ML Amazon Machine Learning is a cloud-based service that creates machine learning (ML) models by finding\n               patterns in your data, and uses these models to process new data and generate\n               predictions. See also http://aws.amazon.com/machine-learning/ . ML model In machine learning (ML), a mathematical model that generates predictions by\n               finding patterns in data. Amazon Machine Learning supports three types of ML models: binary\n               classification, multiclass classification, and regression. Also known as a predictive model . See also binary classification model . See also multiclass classification\n            model . See also regression model . Mobile Analytics Amazon Mobile Analytics is a service for collecting, visualizing, understanding, and extracting mobile app\n               usage data at scale. See also https://aws.amazon.com/mobileanalytics . Mobile Hub See Amplify . AWS Mobile SDK See Amplify . Mobile SDK for Android See Amplify Android . Mobile SDK for iOS See Amplify iOS . Mobile SDK for Unity The AWS Mobile SDK for Unity is included in the AWS SDK for .NET . Mobile SDK for Xamarin The AWS Mobile SDK for Xamarin is included in the AWS SDK for .NET . Amazon Monitron Amazon Monitron is an end-to-end system that uses machine learning (ML) to detect abnormal behavior in industrial machinery. Use Amazon Monitron to implement predictive maintenance and reduce unplanned downtime. See also https://aws.amazon.com/monitron/ . Amazon MQ Amazon MQ is a managed message broker service for Apache ActiveMQ that you can use to set up\n               and operate message brokers in the cloud. See also https://aws.amazon.com/amazon-mq/ . MTA See Mail Transfer Agent (MTA) . Multi-AZ deployment A primary DB instance that has a\n               synchronous standby replica in a different Availability Zone .\n               The primary DB instance is synchronously replicated across Availability Zones to the\n               standby replica. multiclass classification\n            model A machine learning model that predicts values that belong to a limited,\n               pre-defined set of permissible values. For example, \"Is this product a book, movie,\n               or clothing?\" multi-factor authentication (MFA) An optional AWS account security feature. After\n               you enable AWS MFA, you must provide a six-digit, single-use code in addition to\n               your sign-in credentials whenever you access secure AWS webpages or the AWS Management Console . You get\n               this single-use code from an authentication device that you keep in your physical possession. See also https://aws.amazon.com/mfa/ . multipart upload A feature that you can use to upload a single object as a set of parts. Multipurpose Internet Mail\n            Extensions (MIME) An internet standard that extends the email protocol to include non-ASCII text and\n               nontext elements, such as attachments. Multitool A cascading application that provides a simple command-line interface for managing\n               large datasets. multi-valued attribute An attribute with more than one value. Amazon MWAA Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow to assist in setting up and operating end-to-end data pipelines in the cloud at scale. See also https://aws.amazon.com/managed-workflows-for-apache-airflow . N Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z namespace An abstract container that provides context for the items (names, or technical\n               terms, or words) it holds, and allows disambiguation of homonym items residing in\n               different namespaces. NAT Network address translation. A strategy of mapping one or more IP addresses to\n               another while data packets are in transit across a traffic routing device. This is\n               commonly used to restrict internet communication to private instances while allowing\n               outgoing traffic. See also Network Address Translation and Protocol\n            Translation . See also NAT gateway . See also NAT instance . NAT gateway A NAT device, managed by AWS, that performs\n               network address translation in a private subnet , to secure inbound internet traffic. A NAT gateway uses both\n               NAT and port address translation. See also NAT instance . NAT instance A NAT device, configured by a user, that\n               performs network address translation in a Amazon VPC public subnet to secure inbound internet\n               traffic. See also NAT gateway . Neptune Amazon Neptune is a managed graph database service that you can use to build and run applications\n               that work with highly connected datasets. Neptune supports the popular graph query\n               languages Apache TinkerPop Gremlin and W3C's SPARQL, enabling you to build queries\n               that efficiently navigate highly connected datasets. See also https://aws.amazon.com/neptune/ . network ACL An optional layer of security that acts as a firewall for controlling traffic in\n               and out of a subnet . You can associate\n               multiple subnets with a single network ACL , but a subnet can be associated with only one network ACL at a\n               time. Network Address Translation and Protocol\n            Translation ( NAT -PT) An internet protocol standard\n               defined in RFC 2766. See also NAT instance . See also NAT gateway . Network Firewall AWS Network Firewall is a managed service that deploys essential network protections for all\n               Amazon Virtual Private Clouds (Amazon VPCs). See also https://aws.amazon.com/network-firewall . n-gram processor A processor that performs n-gram transformations. See also n-gram transformation . n-gram transformation Amazon Machine Learning: A transformation that aids in text string analysis. An n-gram\n               transformation takes a text variable as input and outputs strings by sliding a window\n               of size n words, where n is specified by\n               the user, over the text, and outputting every string of words of size n and all smaller sizes. For example, specifying the n-gram\n               transformation with window size =2 returns all the two-word combinations and all of\n               the single words. NICE Desktop Cloud Visualization A remote visualization technology for securely connecting users to\n               graphic-intensive 3D applications hosted on a remote, high-performance server. Nimble Studio Amazon Nimble Studio is a managed AWS cloud service for creative studios to produce visual effects, animation, and interactive content\u2014from storyboard to final deliverable. See also https://aws.amazon.com/nimble-studio/ . node OpenSearch Service : An OpenSearch instance. A node can be\n               either a data instance or a dedicated master instance. See also dedicated master node . NoEcho A property of CloudFormation parameters that prevent the otherwise default reporting of names and values of a\n               template parameter. Declaring the NoEcho property causes the parameter\n               value to be masked with asterisks in the report by the cfn-describe-stacks command. normalized discounted cumulative gain (NCDG) at K (5/10/25) Amazon Personalize : An evaluation metric that tells you about the relevance of your model\u2019s highly ranked recommendations, where K is a sample size of 5, 10, or 25 recommendations. Amazon Personalize calculates this by assigning weight to recommendations based on their position in a ranked list, where each recommendation is discounted (given a lower weight) by a factor dependent on its position. The normalized discounted cumulative gain at K assumes that recommendations that are lower on a list are less relevant than recommendations higher on the list. See also metrics . See also recommendations . NoSQL Nonrelational database systems that are highly available, scalable, and optimized\n               for high performance. Instead of the relational model, NoSQL databases (for example, DynamoDB ) use alternate models for data\n               management, such as key\u2013value pairs or document storage. null object A null object is one whose version ID is null. Amazon S3 adds a null object to a bucket when versioning for that bucket is\n               suspended. It's possible to have only one null object for each key in a\n               bucket. number of passes The number of times that you allow Amazon Machine Learning to use the same data records to train\n               a machine learning model. O Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z O3DE Open 3D Engine (successor to Amazon Lumberyard) is an open-source 3D development engine\n               for creating games and simulations. O3DE is licensed under Apache 2.0 and maintained\n               by a community of contributors, including Amazon. See also https://www.o3de.org/ . See also https://aws.amazon.com/lumberyard/ . See also https://docs.aws.amazon.com/lumberyard/ . object Amazon S3 : The fundamental entity type stored in\n               Amazon S3. Objects consist of object data and metadata. The data portion is opaque to\n               Amazon S3. CloudFront : Any entity that can be served\n               either over HTTP or a version of RTMP. observation Amazon Machine Learning: A single instance of data that Amazon Machine Learning (Amazon ML) uses to either train a\n               machine learning model how to predict or to generate a prediction. Each row in an\n               Amazon ML input data file is an observation. On-Demand Instance An Amazon EC2 pricing option that\n               charges you for compute capacity by the hour or second (minimum of 60 seconds) with no long-term commitment. Open 3D Engine See O3DE . OpenSearch Service Amazon OpenSearch Service is an AWS managed service for deploying, operating, and scaling OpenSearch, an\n               open-source search and analytics engine, in the AWS Cloud. Amazon OpenSearch Service (OpenSearch Service) also\n               offers security options, high availability, data durability, and direct access to the\n               OpenSearch API. See also https://aws.amazon.com/elasticsearch-service . operation An API function. Also called an action . OpsWorks AWS OpsWorks is a configuration management service that helps you use Chef to configure and\n               operate groups of instances and applications. You can define the application's\n               architecture and the specification of each component including package installation,\n               software configuration, and resources such\n               as storage. You can automate tasks based on time, load, or lifecycle events. See also https://aws.amazon.com/opsworks/ . optimistic locking A strategy to ensure that an item that you want to update has not been modified by\n               others before you perform the update. For DynamoDB , optimistic locking support is provided by the AWS\n               SDKs. opt-in Region An AWS Region that is disabled by default. To use an opt-in Region, you must\n               enable it. Regions introduced after March 20, 2019 are opt-in Regions. For a list of\n               opt-in Regions, see Considerations before enabling and disabling Regions in the AWS Account Management Guide . See also Region that is enabled by default . organization Organizations : An entity\n               that you create to consolidate and manage your AWS accounts. An organization has\n               one management account along with zero or more member accounts. organizational unit Organizations : A container\n               for accounts within a root of an organization.\n               An organizational unit (OU) can contain other OUs. Organizations AWS Organizations is an account management service that you can use to consolidate multiple\n               AWS accounts into an organization that you create and centrally manage. See also https://aws.amazon.com/organizations/ . origin access identity Also called OAI. When using Amazon CloudFront to serve content with an Amazon S3 bucket as the origin, a virtual identity\n               that you use to require users to access your content through CloudFront URLs instead of\n               Amazon S3 URLs. Usually used with CloudFront private content . origin server The Amazon S3 bucket or custom origin containing the\n               definitive original version of the content you deliver through CloudFront . original environment The instances in a deployment group at the start of an CodeDeploy blue/green\n               deployment. OSB transformation Orthogonal sparse bigram transformation. In machine learning, a transformation\n               that aids in text string analysis and that's an alternative to the n-gram\n               transformation. OSB transformations are generated by sliding the window of size n words over the text, and outputting every pair of words\n               that includes the first word in the window. See also n-gram transformation . OU See organizational unit . Outposts AWS Outposts is a fully managed service by AWS that extends AWS infrastructure, services, APIs, and tools to on-premises data centers and edge locations. Use AWS Outposts for workloads and devices requiring low latency access to on-premises systems, local data processing, data residency, and application migration with local system interdependencies. See also https://aws.amazon.com/outposts . output location Amazon Machine Learning: An Amazon S3 location where the results of a batch prediction are\n               stored. P Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z pagination The process of responding to an API request by returning a large list of records\n               in small separate parts. Pagination can occur in the following situations: The client sets the maximum number of returned records to a value below the\n                     total number of records. The service has a default maximum number of returned records that's lower\n                     than the total number of records. When an API response is paginated, the service sends a subset of the large list of\n               records and a pagination token that indicates that more records are available. The\n               client includes this pagination token in a subsequent API request, and the service\n               responds with the next subset of records. This continues until the service responds\n               with a subset of records and no pagination token, indicating that all records have\n               been sent. pagination token A marker that indicates that an API response contains a subset of a larger list of\n               records. The client can return this marker in a subsequent API request to retrieve\n               the next subset of records until the service responds with a subset of records and no\n               pagination token, indicating that all records have been sent. See also pagination . paid AMI An Amazon Machine Image\n            (AMI) that\n               you sell to other Amazon EC2 users on AWS Marketplace . AWS Panorama AWS Panorama is a machine learning (ML) Appliance and Software Development Kit (SDK) that organizations can use to bring computer vision (CV) to on-premises cameras to make predictions locally. See also https://aws.amazon.com/panorama . AWS ParallelCluster AWS ParallelCluster is an AWS supported open source cluster management tool that helps you to deploy and\n               manage high performance computing (HPC) clusters in the AWS Cloud. paravirtual virtualization See PV virtualization . part A contiguous portion of the object's data in a multipart upload request. partition A group of AWS Regions .\n               Each Region is in only one partition, and each partition contains one or more\n               Regions. Partitions have independent instances of the AWS Identity and Access Management (IAM)\n               infrastructure. In other words, a partition is comprised of Regions that share the\n               same authentication, account, and resource stack. Each AWS account is scoped to one\n               partition. You can't use IAM credentials from one partition to interact with\n               resources in a different partition. Some AWS services are designed to provide cross-Region functionality. Such\n               cross-Region functionality is supported only between Regions in the same partition.\n               AWS commercial Regions are in the AWS partition, China Regions are\n               in the AWS-cn partition, and AWS GovCloud (US) Regions are in the AWS-us-gov partition. partition key A simple primary key, composed of one attribute (also known as a hash\n                  attribute ). See also primary key . See also sort key . PAT Port address translation. pebibyte (PiB) A contraction of peta binary byte, a pebibyte is 2^50 or 1,125,899,906,842,624\n               bytes. A petabyte (PB) is 10^15 or 1,000,000,000,000,000 bytes. 1,024 PiB is an exbibyte (EiB) . period See sampling period . permission A statement within a policy that allows\n               or denies access to a particular resource .\n               You can state any permission in the following way: \"A has permission to do B to\n               C.\" For example, Jane (A) has permission to read messages (B) from John's Amazon SQS queue (C). Whenever Jane sends a request to Amazon SQS to use John's\n               queue, the service checks to see if she has permission. It further checks to see if\n               the request satisfies the conditions John set forth in the permission. persistent storage A data storage solution where the data remains intact until it's deleted. Options\n               within AWS include: Amazon S3 , Amazon RDS , DynamoDB , and other\n               services. Amazon Personalize Amazon Personalize is an artificial intelligence service for creating individualized product and content recommendations. See also https://aws.amazon.com/personalize/ . PERSONALIZED_RANKING recipes Amazon Personalize : Recipes that provide item recommendations in ranked order based on the predicted interest for a user. See also recipe . See also recommendations . See also personalized-ranking recipe . See also popularity-count recipe . personalized-ranking recipe Amazon Personalize : A PERSONALIZED_RANKING recipe that ranks a collection of items that you provide based on the predicted interest level for a specific user. Use the personalized-ranking recipe to create curated lists of items or ordered search results that are personalized for a specific user. See also recipe . See also PERSONALIZED_RANKING recipes . physical name A unique label that CloudFormation assigns to each resource when creating\n               a stack . Some AWS CloudFormation commands accept the\n               physical name as a value with the --physical-name parameter. pilot light An active-passive disaster recovery strategy in which you replicate data from the primary Region as standby, then provision a replica that contains only the core workload infrastructure. To make this infrastructure functional and serve requests, you must provision the remaining resources, such as compute. See also back up and restore , hot standby , warm standby . Amazon Pinpoint Amazon Pinpoint is a multichannel communications service that helps organizations send timely, targeted content through SMS, email, mobile push notifications, voice messages, and in-application channels. See also https://aws.amazon.com/pinpoint . pipeline CodePipeline : A workflow\n               construct that defines the way software changes go through a release process. plaintext Information that has not been encrypted , as opposed to ciphertext . policy IAM : A document defining\n               permissions that apply to a user, group, or role; the permissions in turn determine\n               what users can do in AWS. A policy typically allows access to specific actions, and can optionally grant that the\n               actions are allowed for specific resources , such as EC2 instances or Amazon S3 buckets . Policies can also explicitly deny access. Amazon EC2 Auto Scaling : An object that stores\n               the information that's needed to launch or terminate instances for an Auto Scaling group.\n               Running the policy causes instances to be launched or terminated. You can configure\n               an alarm to invoke an Auto Scaling policy. policy generator A tool in the IAM AWS Management Console that\n               helps you build a policy by selecting\n               elements from lists of available options. policy simulator A tool in the IAM AWS Management Console that\n               helps you test and troubleshoot policies so you can see their effects in real-world scenarios. policy validator A tool in the IAM AWS Management Console that\n               examines your existing IAM access control policies to ensure that they comply with the IAM policy\n               grammar. Amazon Polly Amazon Polly is a text-to-speech (TTS) service that turns text into natural-sounding human speech.\n               Amazon Polly provides dozens of lifelike voices across a broad set of languages so that\n               you can build speech-enabled applications that work in many different\n               countries. See also https://aws.amazon.com/polly/ . popularity-count recipe Amazon Personalize : A USER_PERSONALIZATION recipe that recommends the items that have had the most interactions with unique users. See also recipe . See also USER_PERSONALIZATION recipes . Porting Assistant for .NET Porting Assistant for .NET is a compatibility analyzer that reduces the manual effort required to port Microsoft .NET Framework applications to open source .NET Core. precision at K (5/10/25) Amazon Personalize : An evaluation metric that tells you how relevant your model\u2019s recommendations are based on a sample size of K (5, 10, or 25) recommendations. Amazon Personalize calculates this metric based on the number of relevant recommendations out of the top K recommendations, divided by K, where K is 5, 10, or 25. See also metrics . See also recommendations . prefix See job prefix . Premium Support A one-on-one, fast-response support channel that AWS customers can subscribe to\n               for support for AWS infrastructure services. See also https://aws.amazon.com/premiumsupport/ . presigned URL A web address that uses query string authentication . primary key One or two attributes that uniquely identify each item in a DynamoDB table, so that no two items can have\n               the same key. See also partition key . See also sort key . primary shard See shard . principal The user , service, or account that receives permissions that are\n               defined in a policy . The principal is A in\n               the statement \"A has permission to do B to C.\" AWS Private CA AWS Private Certificate Authority is a hosted private certificate authority service for issuing and revoking private\n               digital certificates . See also https://aws.amazon.com/certificate-manager/private-certificate-authority/ . private content When using Amazon CloudFront to\n               serve content with an Amazon S3 bucket as the origin, a method of\n               controlling access to your content by requiring users to use signed URLs. Signed URLs\n               can restrict user access based on the current date and time, the IP addresses that\n               the requests originate from, or both. private IP address A private numerical address (for example, 192.0.2.44) that networked devices use\n               to communicate with one another using the Internet Protocol (IP). Each EC2 instance is assigned two IP addresses\n               at launch, which are directly mapped to each other through network address\n               translation ( NAT ): a private address (following\n               RFC 1918) and a public address. Exception: Instances launched in Amazon VPC are assigned only a private IP\n               address. private subnet A Amazon VPC subnet whose instances can't be reached from\n               the internet. product code An identifier provided by AWS when you submit a product to AWS Marketplace . properties See resource property . property rule A JSON -compliant markup standard for\n               declaring properties, mappings, and output values in an CloudFormation template. Provisioned IOPS A storage option that delivers fast, predictable, and consistent I/O performance.\n               When you specify an IOPS rate while creating a DB instance, Amazon RDS provisions that IOPS rate\n               for the lifetime of the DB instance. pseudo parameter A predefined setting (for example, AWS:StackName ) that can be used in CloudFormation templates without\n               having to declare them. You can use pseudo parameters anywhere you can use a regular\n               parameter. public AMI An Amazon Machine Image\n            (AMI) that\n               all AWS accounts have permission to\n               launch. public dataset A large collection of public information that can be seamlessly integrated into\n               applications that are based in the AWS Cloud. Amazon stores public datasets at no\n               charge to the community and, similar to other AWS services, users pay only for the\n               compute and storage they use for their own applications. These datasets currently\n               include data from the Human Genome Project, the US Census, Wikipedia, and other\n               sources. See also https://aws.amazon.com/publicdatasets . public IP address A public numerical address (for example, 192.0.2.44) that networked devices use to\n               communicate with one another using the Internet Protocol (IP). Each EC2 instance is assigned two IP addresses\n               at launch, which are directly mapped to each other through Network Address\n               Translation ( NAT ): a private address (following\n               RFC 1918) and a public address. Exception: Instances launched in Amazon VPC are assigned only a private IP\n               address. public subnet A subnet whose instances can be reached\n               from the internet. PV virtualization Paravirtual virtualization allows guest VMs to run on host systems that don't\n               have special support extensions for full hardware and CPU virtualization. Because PV\n               guests run a modified operating system that doesn't use hardware emulation, they\n               can't provide hardware-related features, such as enhanced networking or GPU\n               support. See also HVM virtualization . Q Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Amazon QLDB Amazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority. See also https://aws.amazon.com/qldb . quartile binning\n            transformation Amazon Machine Learning: A process that takes two inputs, a numerical variable and a parameter\n               called a bin number, and outputs a categorical variable. Quartile binning\n               transformations discover non-linearity in a variable's distribution by enabling the\n               machine learning model to learn separate importance values for parts of the numeric\n               variable\u2019s distribution. Query A type of web service that generally uses only the GET or POST HTTP method and a\n               query string with parameters in the URL. See also REST . query string authentication An AWS feature that you can use to place the authentication information in the\n               HTTP request query string instead of in the Authorization header, which\n               provides URL-based access to objects in a bucket . queue A sequence of messages or jobs that are held in temporary storage awaiting\n               transmission or processing. queue URL A web address that uniquely identifies a queue. QuickSight Amazon QuickSight is a fast, cloud-powered business analytics service that you can use to build\n               visualizations, perform analysis, and quickly get business insights from your data. See also https://aws.amazon.com/quicksight/ . quota The maximum value for your resources, actions, and items in your AWS account R Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z AWS RAM AWS Resource Access Manager is a web service that AWS customers can use to securely share AWS resources with any AWS account or within your organization. See also https://aws.amazon.com/ram . range GET A request that specifies a byte range of data to get for a download. If an object\n               is large, you can break up a download into smaller units by sending multiple range\n               GET requests that each specify a different byte range to GET. raw email A type of sendmail request with which you can specify the\n               email headers and MIME types. Amazon RDS Amazon Relational Database Service is a web service that makes it easier to set up, operate, and scale a relational\n               database in the cloud. It provides cost-efficient, resizable capacity for an\n               industry-standard relational database and manages common database administration\n               tasks. See also https://aws.amazon.com/rds . read local/write global An active-active strategy in which all writes for a workload are sent to one\n               primary Region and all read traffic is served from the Region where the request\n               originates. Typically architected with an asynchronous data store. Sometimes referred\n               to as read local-write global . See also read local/write local , global consistency . read local/write local An active-active strategy in which all writes for a workload are sent to one primary Region and all read traffic is served from the Region where the request originates. Typically architected with an asynchronous data store. Sometimes referred to as read local-write global. See also read local/write global , global consistency . read replica Amazon RDS : An active copy of another\n               DB instance. Any updates to the data on the source DB instance are replicated to the\n               read replica DB instance using the built-in replication feature of MySQL 5.1. real-time predictions Amazon Machine Learning: Synchronously generated predictions for individual data\n               observations. See also batch prediction . receipt handle Amazon SQS : An identifier that you get when you receive a message from the\n               queue. This identifier is required to delete a message from the queue or when\n               changing a message's visibility timeout. receiver The entity that consists of the network systems, software, and policies that\n               manage email delivery for a recipient . recipe Amazon Personalize : An Amazon Personalize\n               algorithm that's preconfigured to predict the items that a user interacts with (for\n               USER_PERSONALIZATION recipes), or calculate items that are similar to specific items\n               that a user has shown interest in (for RELATED_ITEMS recipes), or rank a collection\n               of items that you provide based on the predicted interest for a specific user (for\n               PERSONALIZED_RANKING recipes). See also USER_PERSONALIZATION recipes . See also RELATED_ITEMS recipes . See also PERSONALIZED_RANKING recipes . recipient Amazon SES : The person or entity receiving an email\n               message. For example, a person named in the \"To\" field of a message. recommendations Amazon Personalize : A list of\n               items that Amazon Personalize predicts that a user interacts with. Depending on the Amazon Personalize recipe\n               used, recommendations can be either a list of items (with USER_PERSONALIZATION\n               recipes and RELATED_ITEMS recipes), or a ranking of a collection of items you\n               provided (with PERSONALIZED_RANKING recipes). See also recipe . See also campaign . See also solution version . See also USER_PERSONALIZATION recipes . See also RELATED_ITEMS recipes . See also PERSONALIZED_RANKING recipes . Redis A fast, open-source, in-memory key-value data structure store. Redis comes with a\n               set of versatile in-memory data structures with which you can easily create a variety\n               of custom applications. Amazon Redshift Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With Amazon Redshift,\n               you can analyze your data using your existing business intelligence tools. See also https://aws.amazon.com/redshift/ . reference A means of inserting a property from one AWS resource into another. For example, you could insert an Amazon EC2 security group property into an Amazon RDS resource. Region A named set of AWS resources that's in the same geographical area. A Region is comprised of\n               at least three Availability Zones .\n               AWS Regions are divided into partitions . AWS commercial Regions are in the AWS partition, China Regions are in the AWS-cn partition, and\n               AWS GovCloud (US) Regions are in the AWS-us-gov partition. Region that is enabled by default An AWS Region that is enabled by default. Regions that were introduced before\n               March 20, 2019 are enabled by default and can\u2019t be disabled. For a list of Regions\n               that aren\u2019t enabled by default ( opt-in Region ), see Considerations before enabling and disabling Regions in the AWS Account Management Guide . regression model Amazon Machine Learning: Preformatted instructions for common data transformations that fine-tune\n               machine learning model performance. regression model A type of machine learning model that predicts a numeric value, such as the exact\n               purchase price of a house. regularization A machine learning (ML) parameter that you can tune to obtain higher-quality ML\n               models. Regularization helps prevent ML models from memorizing training data examples\n               instead of learning how to generalize the patterns it sees (called overfitting). When\n               training data is overfitted, the ML model performs well on the training data, but\n               doesn't perform well on the evaluation data or on new data. Amazon Rekognition Amazon Rekognition is a machine learning service that identifies objects, people, text, scenes, and activities, including inappropriate content, in either image or video files. With Amazon Rekognition Custom Labels, you can create a customized ML model that detects objects and scenes specific to your business in images. See also https://aws.amazon.com/rekognition/ . RELATED_ITEMS recipes Amazon Personalize Recipes that recommend items that are\n               similar to a specified item, such as the item-to-item (SIMS) recipe. See also recipe . See also item-to-item similarities (SIMS) recipe . replacement environment The instances in a deployment group after the CodeDeploy blue/green deployment. replica shard See shard . reply path The email address that an email reply is sent to. This is different from the return path . representational state transfer See REST . reputation 1. An Amazon SES metric, based on\n               factors that might include bounces , complaints , and other metrics, regarding\n               whether a customer is sending high-quality email. 2. A measure of confidence, as judged by an internet service provider\n               (ISP) or\n               other entity that an IP address that they are receiving email from isn't the source\n               of spam . requester The person (or application) that sends a request to AWS to perform a specific\n               action. When AWS receives a request, it first evaluates the requester's permissions\n               to determine whether the requester is allowed to perform the request action (if\n               applicable, for the requested resource ). Requester Pays An Amazon S3 feature that allows a bucket owner to specify that anyone who\n               requests access to objects in a particular bucket must pay the data transfer and request costs. reservation A collection of EC2 instances started\n               as part of the same launch request. This is not to be confused with a Reserved Instance . Reserved Instance A pricing option for EC2 instances that discounts the on-demand usage charge for instances that meet the specified parameters.\n               Customers pay for the entire term of the instance, regardless of how they use\n               it. Reserved Instance Marketplace An online exchange that matches sellers who have reserved capacity that they no\n               longer need with buyers who are looking to purchase additional capacity. reserved instances that you purchase\n               from third-party sellers have less than a full standard term remaining and can be\n               sold at different upfront prices. The usage or reoccurring fees remain the same as\n               the fees set when the Reserved Instances were originally purchased. Full standard\n               terms for Reserved Instances available from AWS run for one year or three\n               years. Resilience Hub AWS Resilience Hub gives you a central place to define, validate, and track the resiliency of your AWS application. It helps you to protect your applications from disruptions, and reduce recovery costs to optimize business continuity to help meet compliance and regulatory requirements. See also https://aws.amazon.com/resilience-hub . resource An entity that users can work with in AWS, such as an EC2 instance , an DynamoDB table, an Amazon S3 bucket , an IAM user, or an OpsWorks stack . Resource Groups AWS Resource Groups is a web service that AWS customers can use to manage and automate tasks on large numbers of resources at one time. See also AWS Resource Groups . Amazon Resource Name (ARN) Amazon Resource Name is a standardized way to refer to an AWS resource (for example, arn:aws:iam::123456789012:user/division_abc/subdivision_xyz/Bob ). resource property A value required when including an AWS resource in an CloudFormation stack . Each resource can have one or more\n               properties associated with it. For example, an AWS::EC2::Instance resource might have a UserData property. In an AWS CloudFormation template, resources\n               must declare a properties section, even if the resource has no properties. resource record Also called resource record set . The fundamental information\n               elements in the Domain Name System (DNS). See also Domain Name\n                  System on Wikipedia. REST Representational state transfer. A simple stateless architecture that generally\n               runs over HTTPS/TLS. REST emphasizes that resources have unique and hierarchical\n               identifiers (URIs), are represented by common media types (such as HTML, XML, or JSON ), and that operations on the resources\n               are either predefined or discoverable within the media type. In practice, this\n               generally results in a limited number of operations. See also Query . See also WSDL . See also SOAP . RESTful web service Also known as RESTful API. A web service that follows REST architectural constraints. The API operations must use HTTP\n               methods explicitly, expose hierarchical URIs, and transfer either XML, JSON , or both. return enabled CloudSearch : An index field option\n               that enables the field's values to be returned in the search results. return path The email address that bounced email is returned to. The return path is specified\n               in the header of the original email. This is different from the reply path . revision CodePipeline : A change that's\n               made to a source that's configured in a source action, such as a pushed commit to a GitHub repository or an update to a file\n               in a versioned Amazon S3 bucket . AWS RoboMaker AWS RoboMaker is a cloud-based simulation service that robotics developers use to run, scale, and automate simulation without managing any infrastructure. See also https://aws.amazon.com/robomaker . role A tool for giving temporary access to AWS resources in your AWS account . rollback A return to a previous state that follows the failure to create an object, such as CloudFormation stack . All resources that are associated with the failure are deleted during the rollback.\n               For AWS CloudFormation, you can override this behavior using the --disable-rollback option on the command line. root Organizations : A parent\n               container for the accounts in your organization. If you apply a service control policy to the\n               root, it applies to every organizational unit and account in the organization. root credentials Authentication information associated with the AWS account owner. root device volume A volume that contains the image used to\n               boot the instance (also known as a root device ). If you launched the instance from an AMI backed\n               by instance store , this is an instance\n               store volume created from a template stored\n               in Amazon S3 . If you launched the instance\n               from an AMI backed by Amazon EBS , this is\n               an Amazon EBS volume created from an Amazon EBS snapshot. route table A set of routing rules that controls the traffic leaving any subnet that's associated with the route table.\n               You can associate multiple subnets with a single route table, but a subnet can be\n               associated with only one route table at a time. Route\u00a053 Amazon Route\u00a053 is a web service that you can use to create a new DNS service or to migrate your\n               existing DNS service to the cloud. See also https://aws.amazon.com/route53 . row identifier Amazon Machine Learning: An attribute in the input data that you can include in the evaluation or\n               prediction output to make it easier to associate a prediction with an\n               observation. rule AWS WAF : A set of conditions that AWS WAF\n               searches for in web requests to AWS resources such as Amazon CloudFront distributions. You add rules to a web\n                  ACL , and then specify whether you want to allow or block web requests based\n               on each rule. S Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z Amazon S3 Amazon S3 is storage for the internet. You can use it to store and retrieve any amount of data\n               at any time, from anywhere on the web. See also https://aws.amazon.com/s3 . Amazon S3 Glacier Amazon S3 Glacier is a secure, durable, and low-cost storage service for data archiving and long-term\n               backup. You can reliably store large or small amounts of data for significantly less\n               than on-premises solutions. S3 Glacier is optimized for infrequently accessed data, where a\n               retrieval time of several hours is suitable. See also https://aws.amazon.com/glacier/ . Amazon S3-Backed AMI See instance store-backed AMI . SageMaker Amazon SageMaker is a fully managed cloud service that builds, trains, and deploys machine learning (ML) models by using AWS infrastructure, tools, and workflows. See also https://aws.amazon.com/sagemaker . AWS SAM AWS Serverless Application Model is an open-source framework for building and running serverless\n               applications. AWS SAM provides a command line interface tool and a shorthand syntax\n               template specification that you can use to quickly iterate through your serverless\n               application lifecycle. See also https://aws.amazon.com/serverless/sam/ . sampling period A defined duration of time, such as one minute, which CloudWatch computes a statistic over. sandbox A testing location where you can test the functionality of your application\n               without affecting production, incurring charges, or purchasing products. Amazon SES : An environment that\n               developers can use to test and evaluate the service. In the sandbox, you have full\n               access to the Amazon SES API, but you can only send messages to verified email addresses\n               and the mailbox simulator. To get out of the sandbox, you must apply for production\n               access. Accounts in the sandbox also have lower sending limits than production accounts. scale in To remove EC2 instances from an Auto Scaling group . scale out To add EC2 instances to an Auto Scaling group . scaling activity A process that changes the size, configuration, or makeup of an Auto Scaling group by launching or\n               terminating instances. scaling policy A description of how Auto Scaling automatically scales an Auto Scaling group in response to\n               changing demand. See also scale in . See also scale out . scheduler The method used for placing tasks on container instances . schema Amazon Machine Learning: The information that's needed to interpret the input data for a machine\n               learning model, including attribute names and their assigned data types, and the\n               names of special attributes. score cut-off value Amazon Machine Learning: A binary classification model outputs a score that ranges from 0 to 1.\n               To decide whether an observation is classified as 1 or 0, you pick a classification\n               threshold, or cut-off, and Amazon ML compares the score against it. Observations with\n               scores higher than the cut-off are predicted as target equals 1, and scores lower\n               than the cut-off are predicted as target equals 0. SCP See service control policy . AWS SCT AWS Schema Conversion Tool is a desktop application that automates heterogeneous database migrations. You can use AWS SCT to convert database schemas and code objects, SQL code in your applications, and ETL scripts to a format compatible with the target database. Then, you can use AWS SCT data extraction agents to migrate data to your target database. See also https://aws.amazon.com/dms/schema-conversion-tool . AWS SDK for .NET AWS SDK for .NET is a software development kit that provides .NET API operations for AWS services\n               including Amazon S3 , Amazon EC2 , IAM ,\n               and more. You can download the SDK as multiple service-specific packages on\n               NuGet. See also https://aws.amazon.com/sdk-for-net/ . SDK for C++ AWS SDK for C++ is a software development kit that provides C++ APIs for many AWS services\n               including Amazon S3 , Amazon EC2 , DynamoDB ,\n               and more. The single, downloadable package includes the AWS C++ library, code\n               examples, and documentation. See also https://aws.amazon.com/sdk-for-cpp/ . SDK for Go AWS SDK for Go is a software development kit for integrating your Go application with the full suite\n               of AWS services. See also https://aws.amazon.com/sdk-for-go/ . SDK for Java AWS SDK for Java is a software development kit that provides Java API operations for many AWS services including Amazon S3 , Amazon EC2 , DynamoDB ,\n               and more. The single, downloadable package includes the AWS Java library, code\n               examples, and documentation. See also https://aws.amazon.com/sdk-for-java/ . SDK for JavaScript in Node.js AWS SDK for JavaScript in Node.js is a software development kit for accessing AWS services from JavaScript in\n               Node.js. The SDK provides JavaScript objects for AWS services, including Amazon S3 , Amazon EC2 , DynamoDB , and Amazon SWF . The single, downloadable package includes the AWS JavaScript\n               library and documentation. See also https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/ . SDK for JavaScript in the Browser AWS SDK for JavaScript in the Browser is a software development kit for accessing AWS services from JavaScript code\n               running in the browser. Authenticate users through Facebook, Google, or Login with\n               Amazon using web identity federation. Store application data in DynamoDB , and save user files to Amazon S3 . See also https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/ . SDK for PHP AWS SDK for PHP is a software development kit and open-source PHP library for integrating your PHP\n               application with AWS services such as Amazon S3 , Amazon S3 Glacier , and DynamoDB . See also https://aws.amazon.com/sdk-for-php/ . SDK for Python (Boto3) AWS SDK for Python (Boto3) is a software development kit for using Python to access AWS services such as Amazon EC2 , Amazon EMR , Amazon EC2 Auto Scaling , Kinesis , or Lambda . See also http://boto.readthedocs.org/en/latest/ . SDK for Ruby AWS SDK for Ruby is a software development kit for accessing AWS services from Ruby. The SDK\n               provides Ruby classes for many AWS services including Amazon S3 , Amazon EC2 , DynamoDB and more. The single, downloadable\n               package includes the AWS Ruby Library and documentation. See also https://aws.amazon.com/sdk-for-ruby/ . SDK for Rust AWS SDK for Rust is a software development kit that provides APIs and utilities for developers. It enables Rust applications to integrate with AWS services such as Amazon S3 and Amazon EC2. SDK for Swift AWS SDK for Swift is a software development kit that provides support for accessing AWS infrastructure and services using the Swift language. search API CloudSearch : The API that you use to\n               submit search requests to a search domain . search domain CloudSearch : Encapsulates your\n               searchable data and the search instances that handle your search requests. You\n               typically set up a separate Amazon CloudSearch domain for each different collection of data that\n               you want to search. search domain configuration CloudSearch : A domain's indexing\n               options, analysis schemes , expressions , suggesters , access policies, and scaling and\n               availability options. search enabled CloudSearch : An index field option\n               that enables the field data to be searched. search endpoint CloudSearch : The URL that you\n               connect to when sending search requests to a search domain. Each Amazon CloudSearch domain has a\n               unique search endpoint that remains the same for the life of the domain. search index CloudSearch : A representation of\n               your searchable data that facilitates fast and accurate data retrieval. search instance CloudSearch : A compute resource that indexes your data and processes\n               search requests. An Amazon CloudSearch domain has one or more search instances, each with a finite\n               amount of RAM and CPU resources. As your data volume grows, more search instances or\n               larger search instances are deployed to contain your indexed data. When necessary,\n               your index is automatically partitioned across multiple search instances. As your\n               request volume or complexity increases, each search partition is automatically\n               replicated to provide additional processing capacity. search request CloudSearch : A request that's sent\n               to an Amazon CloudSearch domain's search endpoint to retrieve documents from the index that match\n               particular search criteria. search result CloudSearch : A document that matches\n               a search request. Also referred to as a search hit . secret access key A key that's used with the access key ID to cryptographically sign programmatic AWS requests. Signing a request\n               identifies the sender and prevents the request from being altered. You can generate\n               secret access keys for your AWS account , individual\n               IAM users and temporary\n               sessions. Secrets Manager AWS Secrets Manager is a service for securely encrypting, storing, and rotating credentials for databases\n               and other services. See also https://aws.amazon.com/secrets-manager/ . security group A named set of allowed inbound network connections for an instance. (Security\n               groups in Amazon VPC also include support for outbound\n               connections.) Each security group consists of a list of protocols, ports, and IP\n               address ranges. A security group can apply to multiple instances, and multiple groups\n               can regulate a single instance. Security Hub AWS Security Hub is a service that provides a comprehensive view of the security state of your AWS\n               resources. Security Hub collects security data from AWS accounts and services and helps\n               you analyze your security trends to identify and prioritize the security issues\n               across your AWS environment. See also https://aws.amazon.com/security-hub/ . sender The person or entity sending an email message. Sender ID A Microsoft controlled version of SPF . An\n               email authentication and anti-spoofing system. For more information about Sender ID,\n               see Sender ID in\n               Wikipedia. sending limits The sending quota and maximum send rate that are associated\n               with every Amazon SES account. sending quota The maximum number of email messages that you can send using Amazon SES in a 24-hour period. AWS Serverless Application Repository AWS Serverless Application Repository is a managed repository that teams, organizations, and individual\n               developers can use to store and share reusable applications, and assemble and deploy\n               serverless architectures in powerful new ways. See also https://aws.amazon.com/serverless/serverlessrepo/ . server-side encryption (SSE) The encrypting of data at\n               the server level. Amazon S3 supports three modes of\n               server-side encryption: SSE-S3, where Amazon S3 manages the keys; SSE-C, where the\n               customer manages the keys; and SSE-KMS, where AWS KMS manages keys. Service Catalog AWS Service Catalog is a web service that helps organizations create and manage catalogs of IT services\n               that are approved for use on AWS. These IT services can include everything from\n               virtual machine images, servers, software, and databases to complete multitier\n               application architectures. See also https://aws.amazon.com/servicecatalog/ . service control policy Organizations : A\n               policy-based control that specifies the services and actions that users and roles can\n               use in the accounts that the service control policy (SCP) affects. service endpoint See endpoint . service health dashboard A webpage showing up-to-the-minute information about AWS service availability.\n               The dashboard is located at http://status.aws.amazon.com/ . AWS Service Management Connector AWS Service Management Connector enables customers to provision, manage, and operate AWS resources and capabilities in familiar IT Service Management (ITSM) tooling. See also https://aws.amazon.com/service-management-connector . Service Quotas A service for viewing and managing your quotas easily and at scale as your AWS\n               workloads grow. Quotas, also referred to as limits, are the maximum number of\n               resources that you can create in an AWS account. service role An IAM role that grants permissions to an AWS service\n               so it can access AWS resources . The\n               policies that you attach to the service role determine which AWS resources the\n               service can access and what it can do with those resources. Amazon SES Amazon Simple Email Service is a simple and cost-effective email solution for applications. See also https://aws.amazon.com/ses . session The period when the temporary security credentials that are provided by AWS STS allow access to your AWS account. SHA Secure Hash Algorithm. SHA1 is an earlier version of the algorithm, which AWS\n               has replaced with SHA256. shard OpenSearch Service : A partition of data in an index. You can\n               split an index into multiple shards, which can include primary shards (original\n               shards) and replica shards (copies of the primary shards). Replica shards provide\n               failover. This means that, if a cluster node that contains a primary shard fails, a\n               replica shard is promoted to a primary shard. Replica shards also can handle\n               requests. shared AMI An Amazon Machine Image\n            (AMI) that a\n               developer builds and makes available for others to use. Shield AWS Shield is a service that helps to protect your resources\u2014such as Amazon EC2 instances,\n               Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Route\u00a053 hosted\n               zones\u2014against DDoS attacks. AWS Shield is automatically included at no extra\n               cost beyond what you already pay for AWS WAF and your other AWS services. For\n               added protection against DDoS attacks, AWS offers AWS Shield Advanced. See also https://aws.amazon.com/shield . shutdown action Amazon EMR : A predefined bootstrap action that launches a script that runs a\n               series of commands in parallel before terminating the job flow. signature Refers to a digital signature , which is a mathematical way to\n               confirm the authenticity of a digital message. AWS uses signatures to authenticate\n               the requests you send to our web services. For more information, to https://aws.amazon.com/security . SIGNATURE file Import/Export : A file that you\n               copy to the root directory of your storage device. The file contains a job ID,\n               manifest file, and a signature. Signature Version 4 Protocol for authenticating inbound API requests to AWS services in all AWS Regions. Signer AWS Signer is a fully managed code-signing service used to ensure the authenticity and integrity of an AWS customer's code. Silk Amazon Silk is a next-generation web browser that's available only on Fire OS tablets and phones.\n               Built on a split architecture that divides processing between the client and the\n               AWS Cloud, Amazon Silk creates a faster, more responsive mobile browsing\n               experience. Simple Mail Transfer Protocol See SMTP . Simple Object Access Protocol See SOAP . SimSpace Weaver AWS SimSpace Weaver is a managed service that you can use to build and run large-scale\n               spatial simulations in the AWS Cloud. See also https://aws.amazon.com/simspaceweaver/ . SIMS recipe See item-to-item similarities (SIMS) recipe . single sign-on An authentication scheme that allows users to sign in one time to access multiple applications and websites. The service name AWS Single Sign-On is now AWS IAM Identity Center. See also IAM Identity Center . Single-AZ DB instance A standard (non-Multi-AZ) DB instance that's deployed in one Availability Zone , without a standby\n               replica in another Availability Zone. See also Multi-AZ deployment . Site-to-Site VPN AWS Site-to-Site VPN is a fully managed service that you can use to establish Internet Protocol security (IPsec) VPN connections between your AWS networks and your on-premises networks. See also https://aws.amazon.com/vpn/site-to-site-vpn . sloppy phrase search A search for a phrase that specifies how close the terms must be to one another to\n               be considered a match. AWS SMS AWS Server Migration Service is a service that combines data collection tools with automated server replication to speed the migration of on-premises servers to AWS. See also https://aws.amazon.com/server-migration-service . SMTP Simple Mail Transfer Protocol. The standard that's used to exchange email messages\n               between internet hosts for the purpose of routing and delivery. snapshot Amazon EBS : A backup of your volumes that's stored in Amazon S3 . You can use these snapshots as the starting point for new Amazon EBS\n               volumes or to protect your data for long-term durability. See also DB snapshot . Snowball AWS Snowball is a petabyte-scale data transport solution that uses devices that are secure to\n               transfer large amounts of data into and out of the AWS Cloud. See also https://aws.amazon.com/snowball . Amazon SNS Amazon Simple Notification Service is a web service that applications, users, and devices can use to instantly send and\n               receive notifications from the cloud. See also https://aws.amazon.com/sns . SOAP Simple Object Access Protocol. An XML-based protocol that you can use to exchange\n               information over a particular protocol (for example, HTTP or SMTP) between\n               applications. See also REST . See also WSDL . soft bounce A temporary email delivery failure such as one resulting from a full\n               mailbox. software VPN A software appliance-based VPN connection over the internet. solution Amazon Personalize : The recipe, customized parameters, and trained models (solution versions) that can be used to generate recommendations. See also recipe . See also solution version . See also recommendations . solution version Amazon Personalize : A trained model that you create as part of a solution in Amazon Personalize. You deploy a solution version in a campaign to generate recommendations. See also solution . See also campaign . See also recommendations . sort enabled CloudSearch : An index field option\n               that enables a field to be used to sort the search results. sort key An attribute used to sort the order of partition keys in a composite primary key\n               (also known as a range attribute ). See also partition key . See also primary key . source/destination checking A security measure to verify that an EC2 instance is the origin of all traffic that it sends and the\n               ultimate destination of all traffic that it receives. In other words, this measure\n               verifies that the instance isn't relaying traffic. By default, source/destination\n               checking is turned on. For instances that function as gateways, such as Amazon VPC NAT instances, source/destination checking must\n               be disabled. spam Unsolicited bulk emails. spamtrap An email address that's set up by an anti- spam entity. This email address isn't for correspondence but rather\n               for monitoring unsolicited emails. This is also called a honeypot . SPF Sender Policy Framework. A standard for authenticating email. SPICE A robust in-memory engine that is part of Amazon QuickSight . Engineered for the cloud, SPICE (Super-fast,\n               Parallel, In-memory Calculation Engine) uses a combination of storage and in-memory\n               technologies. It uses these to get faster results from interactive queries and\n               advanced calculations on large datasets. SPICE automatically replicates data for high\n               availability. SPICE makes it possible for Amazon QuickSight to support hundreds of thousands\n               of simultaneous analyses across a variety of data sources. Spot Instance A type of EC2 instance that you can\n               bid on to use unused Amazon EC2 capacity. Spot price The price for a Spot Instance at any\n               given time. If your maximum price exceeds the current price and your restrictions are\n               met, Amazon EC2 launches instances on your\n               behalf. SQL injection match condition AWS WAF : An attribute that specifies the\n               part of web requests (such as a header or a query string) that AWS WAF inspects for\n               malicious SQL code. Based on the specified conditions, you can configure AWS WAF to\n               allow or block web requests to an AWS resource , such as an Amazon CloudFront distribution. Amazon SQS Amazon Simple Queue Service is a reliable and scalable hosted queues for storing messages as they travel between\n               computers. See also https://aws.amazon.com/sqs . Amazon SWF Amazon Simple Workflow Service is a fully managed service that helps developers build, run, and scale background\n               jobs that have parallel or sequential steps. Amazon SWF functions similar to a state\n               tracker and task coordinator in the AWS Cloud. See also https://aws.amazon.com/swf/ . SSE See server-side encryption (SSE) . SSL Secure Sockets Layer See also Transport Layer Security\n               (TLS) . stack CloudFormation : A collection of\n               AWS resources that you create and delete\n               as a single unit. OpsWorks : A set of instances that you\n               manage collectively, typically because they have a common purpose such as serving PHP\n               applications. A stack serves as a container and handles tasks that apply to the group\n               of instances as a whole, such as managing applications and cookbooks. station CodePipeline : A portion of a\n               pipeline workflow where one or more actions are performed. station A place at an AWS facility where your AWS Import/Export data is transferred on to, or off of,\n               your storage device. statistic One of five functions of the values submitted for a given sampling period . These functions are Maximum , Minimum , Sum , Average , and SampleCount . stem The common root or substring shared by a set of related words. stemming The process of mapping related words to a common stem. This enables matching on\n               variants of a word. For example, a search for \"horse\" could return matches for\n               horses, horseback, and horsing, as well as horse. CloudSearch supports both dictionary based and algorithmic\n               stemming. step Amazon EMR : A single function applied to the data in a job flow . The sum of all steps comprises a job\n               flow. Step Functions AWS Step Functions is a web service that coordinates the components of distributed applications as a\n               series of steps in a visual workflow. See also https://aws.amazon.com/step-functions/ . step type Amazon EMR : The type of work done in a step. There are a limited number of step\n               types, such as moving data from Amazon S3 to Amazon EC2 or from Amazon EC2 to Amazon S3. sticky session A feature of the ELB load balancer that\n               binds a user's session to a specific application instance. This is so that all\n               requests that are coming from the user during the session are sent to the same\n               application instance. By contrast, a load balancer defaults to route each request\n               independently to the application instance with the smallest load. stopping The process of filtering stop words from an index or search request. stopword A word that isn't indexed and is automatically filtered out of search requests\n               because it's either insignificant or so common that including it results in too many\n               matches to be useful. Stopwords are language specific. Storage Gateway AWS Storage Gateway is a hybrid cloud storage service that provides on-premises access to\n               virtually unlimited cloud storage. See also AWS Storage Gateway . streaming Amazon EMR : A\n               utility that comes with Hadoop that you can\n               use to develop MapReduce executables in languages other than Java. CloudFront : The ability to use a media\n               file in real time\u2014as it's transmitted in a steady stream from a server. streaming distribution A special kind of distribution that\n               serves streamed media files using a Real Time Messaging Protocol (RTMP)\n               connection. Streams See Kinesis Data Streams . string match condition AWS WAF : An attribute that specifies the\n               strings that AWS WAF searches for in a web request, such as a value in a header or a\n               query string. Based on the specified strings, you can configure AWS WAF to allow or\n               block web requests to an AWS resource , such\n               as a CloudFront distribution. string-to-sign Before you calculate an HMAC signature, you\n               first assemble the required components in a canonical order. The preencrypted string\n               is the string-to-sign. strongly consistent read A read process that returns a response with the most up-to-date data. This data\n               reflects the updates from all previous write operations that were\n               successful\u2014regardless of the Region. See also data consistency . See also eventual consistency . See also eventually consistent read . structured query Search criteria that are specified using the CloudSearch structured query language. You use the structured query\n               language to construct compound queries that use advanced search options and combine\n               multiple search criteria using Boolean operators. AWS STS AWS Security Token Service is a web service for requesting temporary, limited-privilege credentials for IAM users or for users that you authenticate\n               ( federated\n                  users ). See also https://aws.amazon.com/iam/ . subnet A segment of the IP address range of a Amazon VPC that an EC2 instance can be attached to.\n               You can create subnets to group instances according to security and operational\n               needs. Subscription button An HTML-coded button that provides a simple way to charge customers a recurring\n               fee. suggester CloudSearch : Specifies an index\n               field for getting autocomplete suggestions and options that can enable fuzzy matches\n               and control how suggestions are sorted. suggestions Documents that contain a match for the partial search string in the field that's\n               designated by the suggester . CloudSearch suggestions include the\n               document IDs and field values for each matching document. To be a match, the string\n               must match the contents of the field starting from the beginning of the field. Sumerian Amazon Sumerian is a set of tools for creating and running high-quality 3D, augmented reality (AR),\n               and virtual reality (VR) applications on the web. See also https://aws.amazon.com/sumerian/ . supported AMI An Amazon Machine Image\n            (AMI) similar\n               to a paid AMI , except that the owner charges\n               for additional software or a service that customers use with their own AMIs. SWF See Amazon SWF . symmetric encryption Encryption that uses a\n               private key only. See also asymmetric encryption . synchronous bounce A type of bounce that occurs while the\n               email servers of the sender and receiver are actively communicating. synonym A word that's the same or nearly the same as an indexed word and that likely\n               produces the same results when specified in a search request. For example, a search\n               for \"Rocky Four\" or \"Rocky 4\" likely returns the fourth Rocky movie. You can do this by designating that four and 4 are\n               synonyms for IV . Synonyms are language specific. Systems Manager AWS Systems Manager is the operations hub for AWS and hybrid cloud environments that can help achieve secure operations at scale. It provides a unified user interface for users to view operations data from multiple AWS services and automate tasks across their AWS resources. See also https://aws.amazon.com/systems-manager . T Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z table A collection of data. Similar to other database systems, DynamoDB stores data in\n               tables. tag Metadata that you can define and assign to AWS resources , such as an EC2 instance . Not all AWS resources can be tagged. tagging Tagging resources: Applying a tag to an AWS resource . Amazon SES : Also called labeling . A way to format return path email addresses so that you can specify a different return\n               path for each recipient of a message. You can use tagging to support VERP . For example, if Andrew manages a mailing\n               list, he can use the return paths andrew+recipient1@example.net and andrew+recipient2@example.net so that he can determine which email bounced. target attribute Amazon Machine Learning (Amazon ML): The attribute in the input data that contains the \u201ccorrect\u201d\n               answers. Amazon ML uses the target attribute to learn how to make predictions on new data.\n               For example, if you were building a model for predicting the sale price of a house,\n               the target attribute would be \u201ctarget sale price in USD.\u201d target revision CodeDeploy : The most recent\n               version of the application revision that has been uploaded to the repository and will\n               be deployed to the instances in a deployment group. In other words, the application\n               revision currently targeted for deployment. This is also the revision that will be\n               pulled for automatic deployments. task An instantiation of a task definition that's running on a container instance . task definition The blueprint for your task. Specifies the name of the task , revisions, container definitions , and volume information. task node An EC2 instance that runs Hadoop map and reduce tasks, but doesn't store\n               data. Task nodes are managed by the master node , which assigns Hadoop tasks to nodes and monitors their\n               status. While a job flow is running, you can increase and decrease the number of task\n               nodes. Because they don't store data and can be added and removed from a job flow,\n               you can use task nodes to manage the EC2 instance capacity your job flow uses,\n               increasing capacity to handle peak loads and decreasing it later. Task nodes only run a TaskTracker Hadoop daemon. tebibyte (TiB) A contraction of tera binary byte. A tebibyte (TiB) is 2^40 or 1,099,511,627,776\n               bytes. A terabyte (TB) is 10^12 or 1,000,000,000,000 bytes. 1,024 TiB is a pebibyte (PiB) . template format version The version of an CloudFormation template design that determines the available features. If you omit the AWSTemplateFormatVersion section from your template, AWS CloudFormation assumes\n               the most recent format version. template validation The process of confirming the use of JSON code in an CloudFormation template.\n               You can validate any AWS CloudFormation template using the cfn-validate-template command. temporary security credentials Authentication information that's provided by AWS STS when you call an STS API action. Includes an access key ID , a secret access key , a session token, and an expiration time. Amazon Textract Amazon Textract is a service that automatically extracts text and data from scanned documents.\n               Amazon Textract goes beyond simple optical character recognition (OCR) to also identify\n               the contents of fields in forms and information stored in tables. See also https://aws.amazon.com/textract/ . throttling The automatic restricting or slowing down of a process based on one or more\n               limits. For example, Kinesis Data Streams throttles operations if an application (or group\n               of applications operating on the same stream) attempts to get data from a shard at a\n               rate faster than the shard limit. API Gateway uses throttling to limit the steady-state request rates for\n               a single account. Amazon SES uses\n               throttling to reject attempts to send email that exceeds the sending limits . time-series data Data that's provided as part of a metric. The time value is assumed to be when the\n               value occurred. A metric is the fundamental concept for CloudWatch and represents a time-ordered set of\n               data points. You publish metric data points into CloudWatch and later retrieve statistics\n               about those data points as a time-series ordered dataset. timestamp A date/time string in the ISO 8601 format (more specifically, in the YYYY-MM-DD format). Timestream Amazon Timestream is a scalable and serverless time series database service for real-time analytics, DevOps, and IoT applications that you can use to store and analyze trillions of events per day. See also https://aws.amazon.com/timestream . TLS See Transport Layer Security\n               (TLS) . tokenization The process of splitting a stream of text into separate tokens on detectable\n               boundaries such as white space and hyphens. AWS Toolkit for Eclipse AWS Toolkit for Eclipse is an open-source plugin for the Eclipse Java integrated development environment\n               (IDE) that makes it easier to develop, debug, and deploy Java applications using\n               Amazon Web Services. See also https://aws.amazon.com/eclipse/ . AWS Toolkit for JetBrains AWS Toolkit for JetBrains is an open-source plugin for the integrated development environments (IDEs) from\n               JetBrains that makes it easier to develop, debug, and deploy serverless applications\n               using Amazon Web Services. See also https://aws.amazon.com/intellij/ , https://aws.amazon.com/pycharm/ . AWS Toolkit for Microsoft Azure DevOps AWS Toolkit for Microsoft Azure DevOps provides tasks you can use in build and release\u00a0definitions in VSTS to interact\n               with AWS services. See also https://aws.amazon.com/vsts/ . AWS Toolkit for Visual Studio AWS Toolkit for Visual Studio is an extension for Visual Studio that helps in developing, debugging, and deploying\n               .NET applications using Amazon Web Services. See also https://aws.amazon.com/visualstudio/ . AWS Toolkit for Visual Studio Code AWS Toolkit for Visual Studio Code is an open-source plugin for the Visual Studio Code (VS Code) editor that makes it\n               easier to develop, debug, and deploy applications using Amazon Web Services. See also https://aws.amazon.com/visualstudiocode/ . AWS Tools for PowerShell AWS Tools for PowerShell is a set of PowerShell cmdlets to help developers and administrators manage their\n               AWS services from the PowerShell scripting environment. See also https://aws.amazon.com/powershell/ . topic A communication channel to send messages and subscribe to notifications. It\n               provides an access point for publishers and subscribers to communicate with each\n               other. Traffic Mirroring An Amazon VPC feature that you can use to copy network traffic from an elastic network\n               interface of Amazon EC2 instances. You can then send this network traffic to out-of-band\n               security and monitoring appliances for content inspection, threat monitoring, and\n               troubleshooting. See also https://aws.amazon.com/vpc/ . training datasource A datasource that contains the data that Amazon Machine Learning uses to train the machine\n               learning model to make predictions. Amazon Transcribe Amazon Transcribe is a machine learning service that uses automatic speech recognition (ASR) to quickly and accurately convert speech to text. See also https://aws.amazon.com/transcribe/ . Amazon Transcribe Medical Amazon Transcribe Medical is an automatic speech recognition (ASR) service for adding medical speech-to-text capabilities to voice-enabled clinical documentation applications. See also https://aws.amazon.com/transcribe/medical/ . Transfer Family AWS Transfer Family offers fully managed support for transferring files over SFTP, FTPS, and FTP into and out of Amazon S3 or Amazon EFS, as well as support for the Applicability Statement 2 (AS2) protocol for business-to-business (B2B) transfers. See also https://aws.amazon.com/aws-transfer-family . transition CodePipeline : The act of a\n               revision in a pipeline continuing from one stage to the next in a workflow. Amazon Translate Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. See also https://aws.amazon.com/translate/ . Transport Layer Security (TLS) A cryptographic protocol that provides security for communication over the\n               internet. Its predecessor is Secure Sockets Layer (SSL). trust policy An IAM policy that's an inherent part of an IAM role . The trust policy specifies which\n                  principals are allowed to use the role. Trusted Advisor AWS Trusted Advisor is a web service that inspects your AWS environment and makes recommendations for\n               saving money, improving system availability and performance, and helping to close\n               security gaps. See also https://aws.amazon.com/premiumsupport/trustedadvisor/ . trusted key groups Amazon CloudFront key groups whose public keys CloudFront can use to verify the signatures of CloudFront signed URLs\n                  and signed cookies . trusted signers See trusted key groups . tuning Selecting the number and type of AMIs to\n               run a Hadoop job flow most\n               efficiently. tunnel A route for transmission of private network traffic that uses the internet to\n               connect nodes in the private network. The tunnel uses encryption and secure protocols\n               such as PPTP to prevent the traffic from being intercepted as it passes through\n               public routing nodes. U Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z unbounded The number of potential occurrences isn't limited by a set number. This value is\n               often used when defining a data type that's a list (for example, maxOccurs=\"unbounded\" ), in WSDL . unit Standard measurement for the values submitted to CloudWatch as metric data. Units include seconds, percent, bytes, bits,\n               count, bytes/second, bits/second, count/second, and none. usage report An AWS record that details your usage of a particular AWS service. You can\n               generate and download usage reports from https://aws.amazon.com/usage-reports/ . user A person or application under an account that makes API calls to AWS products. Each user has a unique name within the\n               AWS account, and a set of security credentials that aren't shared with other users.\n               These credentials are separate from the security credentials for the AWS account.\n               Each user is associated with one and only one AWS account. USER_PERSONALIZATION recipes Amazon Personalize : Recipes\n               that are used to build a recommendation system that predicts the items that a user\n               interacts with based on data provided in Interactions, Items, and Users\n               datasets. See also recipe . See also user-personalization recipe . See also popularity-count recipe . See also HRNN . user-personalization recipe Amazon Personalize : An\n               HRNN-based USER_PERSONALIZATION recipe that predicts the items that a user interacts\n               with. The user-personalization recipe can use item exploration and impressions data\n               to generate recommendations for new items. See also HRNN . See also recipe . See also USER_PERSONALIZATION recipes . See also item exploration . See also impressions data . See also recommendations . Users dataset Amazon Personalize : A container for metadata about your users, such as age, gender, or loyalty membership. See also dataset . V Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z validation See template validation . value Instances of attributes for an item, such as\n               cells in a spreadsheet. An attribute might have multiple values. Tagging resources: A specific tag label that\n               acts as a descriptor within a tag category (key). For example, you might have EC2 instance with the tag key of Owner and the tag value of Jan . You can\n               tag an AWS resource with up to 10\n               key\u2013value pairs. Not all AWS resources can be tagged. Variable Envelope Return Path See VERP . verification The process of confirming that you own an email address or a domain so that you\n               can send email from or to it. VERP Variable Envelope Return Path. A way that email-sending applications can match bounced email with the undeliverable\n               address that caused the bounce by using a different return path for each recipient. VERP is typically used for mailing\n               lists. With VERP, the recipient's email address is embedded in the address of the\n               return path, which is where bounced email is returned. This makes it possible to\n               automate the processing of bounced email without having to open the bounce messages,\n               which might vary in content. versioning Every object in Amazon S3 has a key and a version ID.\n               Objects with the same key, but different version IDs can be stored in the same bucket . Versioning is enabled at the bucket\n               layer using PUT Bucket versioning. VGW See virtual private gateway (VGW) . virtual private gateway (VGW) The Amazon side of a VPN connection that maintains connectivity. The internal interfaces of the virtual private gateway\n               connect to your Amazon VPC through the VPN attachment.\n               The external interfaces connect to the VPN connection, which leads to the customer gateway . virtualization Allows multiple guest virtual machines (VM) to run on a host operating system.\n               Guest VMs can run on one or more levels above the host hardware, depending on the\n               type of virtualization. See also PV virtualization . See also HVM virtualization . visibility timeout The period of time that a message is invisible to the rest of your application\n               after an application component gets it from the queue. During the visibility timeout,\n               the component that received the message usually processes it, and then deletes it\n               from the queue. This prevents multiple components from processing the same\n               message. VM Import/Export VM Import/Export is a service for importing virtual machine (VM) images from your existing\n               virtualization environment to Amazon EC2 and then exporting them back. See also https://aws.amazon.com/ec2/vm-import . volume A fixed amount of storage on an instance . You can share volume data between more than one container and persist the data on the container instance when the\n               containers are no longer running. Amazon VPC Amazon Virtual Private Cloud is a web service for provisioning a logically isolated section of the AWS Cloud\n               virtual network that you define. You control your virtual networking environment by selecting your own IP address range, creating subnets and configuring route tables and network gateways. See also https://aws.amazon.com/vpc . VPC endpoint A feature that you can use to create a private connection between your Amazon VPC and another AWS service without requiring\n               access over the internet, through a NAT instance, a VPN connection , or Direct Connect . VPG See virtual private gateway (VGW) . AWS VPN AWS Virtual Private Network provides functionality that establishes encrypted connections between\n               your network or device, and AWS. AWS VPN is comprised of two services: AWS Client VPN and AWS Site-to-Site VPN . See also https://aws.amazon.com/vpn . AWS VPN CloudHub AWS VPN CloudHub is a feature that enables secure communication between branch offices using a simple hub-and-spoke model, with or without a VPN. VPN connection Amazon Web Services\n            (AWS) : The IPsec\n               connection that's between a Amazon VPC and some other\n               network, such as a corporate data center, home network, or colocation\n               facility. W Numbers and symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X, Y, Z AWS WAF AWS WAF is a web application firewall service that controls access to content by allowing or\n               blocking web requests based on criteria that you specify. For example, you can filter\n               access based on the header values or the IP addresses that the requests originate\n               from. AWS WAF helps protect web applications from common web exploits that could affect\n               application availability, compromise security, or consume excessive resources. See also https://aws.amazon.com/waf/ . Amazon WAM Amazon WorkSpaces Application Manager (Amazon WAM) is a web service for deploying and managing applications for WorkSpaces. Amazon WAM accelerates\n               software deployment, upgrades, patching, and retirement by packaging Windows desktop\n               applications into virtualized application containers. See also https://aws.amazon.com/workspaces/applicationmanager . warm standby An active-passive disaster recovery strategy in which a workload is scaled down in the passive standby Region, but is otherwise fully functional. This is not an Amazon EC2 Auto Scaling term, but an industry-standard resilience term. See also back up and restore , hot standby , pilot light . AWS Wavelength AWS Wavelength is a service by AWS that embeds AWS compute and storage services within 5G networks to provide mobile edge computing infrastructure. Use AWS Wavelength to develop, deploy, and scale ultra-low-latency applications to mobile devices and end users. See also https://aws.amazon.com/wavelength . web access control list (web ACL) AWS WAF : A set of rules that defines the\n               conditions that AWS WAF searches for in web requests to an AWS resource , such as a Amazon CloudFront distribution. A web\n               access control list (web ACL) specifies if to allow, block, or count the\n               requests. Web Services Description Language See WSDL . WorkDocs Amazon WorkDocs is a managed, secure enterprise document storage and sharing service with\n               administrative controls and feedback capabilities. See also https://aws.amazon.com/workdocs/ . Amazon WorkLink Amazon WorkLink is a cloud-based service that provides secure access to internal websites and web\n               apps from mobile devices. See also https://aws.amazon.com/worklink/ . WorkMail Amazon WorkMail is a managed, secure business email and calendar service with support for existing\n               desktop and mobile email clients. See also https://aws.amazon.com/workmail/ . WorkSpaces Amazon WorkSpaces is a managed, secure desktop computing service for provisioning cloud-based desktops\n               and providing users access to documents, applications, and resources from supported devices. See also https://aws.amazon.com/workspaces/ . WSDL Web Services Description Language. A language that's used to describe the actions\n               that a web service can perform, along with the syntax of action requests and\n               responses. See also REST . See also SOAP . X, Y, Z X.509 certificate A digital document that uses the X.509 public key infrastructure (PKI) standard to\n               verify that a public key belongs to the entity that's described in the certificate . X-Ray AWS X-Ray is a web service that collects data about requests that your application serves.\n               X-Ray provides tools that you can use to view, filter, and gain insights into that\n               data to identify issues and opportunities for optimization. See also https://aws.amazon.com/xray/ . yobibyte (YiB) A contraction of yotta binary byte. A yobibyte (YiB) is 2^80 or\n               1,208,925,819,614,629,174,706,176 bytes. A yottabyte (YB) is 10^24 or\n               1,000,000,000,000,000,000,000,000 bytes. zebibyte (ZiB) A contraction of zetta binary byte. A zebibyte (ZiB) is 2^70 or\n               1,180,591,620,717,411,303,424 bytes. A zettabyte (ZB) is 10^21 or\n               1,000,000,000,000,000,000,000 bytes. 1,024 ZiB is a yobibyte (YiB) . zone awareness OpenSearch Service : A configuration that distributes nodes in\n               a cluster across two Availability Zones in the same Region. Zone awareness helps to prevent data loss and minimizes downtime\n               if a node and data center fails. If you enable zone awareness, you must have an even\n               number of data instances in the instance count, and you also must use the Amazon OpenSearch Service\n               Configuration API to replicate your data for your OpenSearch cluster. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions"}, {"title": "Amazon Simple Storage Service Documentation", "url": "https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=S3&topic_url=https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "What is Amazon S3? - Amazon Simple Storage Service", "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/index.html", "content": "What is Amazon S3? PDF RSS Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability,\n\t\tdata availability, security, and performance. Customers of all sizes and industries can use\n\t\tAmazon S3 to store and protect any amount of data for a range of use cases, such as data lakes,\n\t\twebsites, mobile applications, backup and restore, archive, enterprise applications, IoT\n\t\tdevices, and big data analytics. Amazon S3 provides management features so that you can optimize,\n\t\torganize, and configure access to your data to meet your specific business, organizational,\n\t\tand compliance requirements. Note For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see Directory buckets and S3 Express One Zone and Directory buckets overview . Topics Features of Amazon S3 How Amazon S3 works Amazon S3 data consistency model Related services Accessing Amazon S3 Paying for Amazon S3 PCI DSS compliance Features of Amazon S3 Storage classes Amazon S3 offers a range of storage classes designed for different use cases. For\n\t\t\t\texample, you can store mission-critical production data in S3 Standard or S3 Express One Zone for frequent\n\t\t\t\taccess, save costs by storing infrequently accessed data in S3 Standard-IA or\n\t\t\t\tS3 One Zone-IA, and archive data at the lowest costs in S3 Glacier Instant Retrieval,\n\t\t\t\tS3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive. Amazon S3 Express One Zone is a high-performance, single-zone Amazon S3 storage class that is purpose-built\n\t\t\t\tto deliver consistent, single-digit millisecond data access for your most\n\t\t\t\tlatency-sensitive applications. S3 Express One Zone is the lowest latency cloud object\n\t\t\t\tstorage class available today, with data access\n\t\t\t\tspeeds\n\t\t\t\tup to 10x faster and with request costs\n\t\t\t\t50\n\t\t\t\tpercent lower than S3 Standard. S3 Express One Zone is the first S3 storage class where you can select a single Availability Zone with \nthe option to co-locate your object storage with your compute resources, which provides the highest possible access speed.\n\t\t\t\tAdditionally, to further increase access speed and support hundreds of thousands of\n\t\t\t\trequests per second, data is stored in a new bucket type: an\n\t\t\t\tAmazon S3 directory bucket. For more information, see Directory buckets and S3 Express One Zone and Directory buckets overview . You can store data with changing or unknown access patterns in\n\t\t\t\tS3 Intelligent-Tiering, which optimizes storage costs by automatically moving your\n\t\t\t\tdata between four access tiers when your access patterns change. These four access\n\t\t\t\ttiers include two low-latency access tiers optimized for frequent and infrequent\n\t\t\t\taccess, and two opt-in archive access tiers designed for asynchronous access for\n\t\t\t\trarely accessed data. For more information, see Understanding and managing Amazon S3 storage classes . Storage management Amazon S3 has storage management features that you can use to manage costs, meet\n\t\t\t\tregulatory requirements, reduce latency, and save multiple distinct copies of your\n\t\t\t\tdata for compliance requirements. S3 Lifecycle \u2013 Configure a lifecycle configuration to manage\n\t\t\t\t\t\tyour objects and store them cost effectively throughout their lifecycle. You\n\t\t\t\t\t\tcan transition objects to other S3 storage classes or expire objects that\n\t\t\t\t\t\treach the end of their lifetimes. S3 Object Lock \u2013 Prevent Amazon S3 objects from being\n\t\t\t\t\t\tdeleted or overwritten for a fixed amount of time or indefinitely. You can\n\t\t\t\t\t\tuse Object Lock to help meet regulatory requirements that require write-once-read-many (WORM) storage or to simply add another\n\t\t\t\t\t\tlayer of protection against object changes and deletions. S3 Replication \u2013 Replicate objects and their respective metadata and object tags to\n\t\t\t\t\t\tone or more destination buckets in the same or different AWS Regions for\n\t\t\t\t\t\treduced latency, compliance, security, and other use cases. S3 Batch Operations \u2013 Manage billions of objects at scale\n\t\t\t\t\t\twith a single S3 API request or a few clicks in the Amazon S3 console. You can\n\t\t\t\t\t\tuse Batch Operations to perform operations such as Copy , Invoke AWS Lambda\n\t\t\t\t\t\t\tfunction , and Restore on\n\t\t\t\t\t\tmillions or billions of objects. Access management and security Amazon S3 provides features for auditing and managing access to your buckets and\n\t\t\t\tobjects. By default, S3 buckets and the objects in them are private. You have access\n\t\t\t\tonly to the S3 resources that you create. To grant granular resource permissions\n\t\t\t\tthat support your specific use case or to audit the permissions of your Amazon S3\n\t\t\t\tresources, you can use the following features. S3 Block Public Access \u2013 Block public access to S3\n\t\t\t\t\t\tbuckets and objects. By default, Block Public Access settings are turned on\n\t\t\t\t\t\tat the bucket level. We recommend that you keep all Block Public Access\n\t\t\t\t\t\tsettings enabled unless you know that you need to turn off one or more of\n\t\t\t\t\t\tthem for your specific use case. For more information, see Configuring block public access\n\t\t\t\tsettings for your S3 buckets . AWS Identity and Access Management (IAM) \u2013 IAM is a web service that helps\n\t\t\t\t\t\tyou securely control access to AWS resources, including your Amazon S3\n\t\t\t\t\t\tresources. With IAM, you can centrally manage permissions that control\n\t\t\t\t\t\twhich AWS resources users can access. You use IAM to control who is\n\t\t\t\t\t\tauthenticated (signed in) and authorized (has permissions) to use\n\t\t\t\t\t\tresources. Bucket\n\t\t\t\t\t\t\tpolicies \u2013 Use IAM-based policy language to configure\n\t\t\t\t\t\tresource-based permissions for your S3 buckets and the objects in\n\t\t\t\t\t\tthem. Amazon S3 access points \u2013 Configure named network endpoints with dedicated access policies to\n\t\t\t\t\t\tmanage data access at scale for shared datasets in Amazon S3. Access control\n\t\t\t\t\t\t\tlists (ACLs) \u2013 Grant read and write permissions for\n\t\t\t\t\t\tindividual buckets and objects to authorized users. As a general rule, we\n\t\t\t\t\t\trecommend using S3 resource-based policies (bucket policies and access point\n\t\t\t\t\t\tpolicies) or IAM user policies for access control instead of ACLs.\n\t\t\t\t\t\tPolicies are a simplified and more flexible access control option. With\n\t\t\t\t\t\tbucket policies and access point policies, you can define rules that apply\n\t\t\t\t\t\tbroadly across all requests to your Amazon S3 resources. For more information\n\t\t\t\t\t\tabout the specific cases when you'd use ACLs instead of resource-based\n\t\t\t\t\t\tpolicies or IAM user policies, see Managing access with ACLs . S3 Object Ownership \u2013 Take ownership of every object\n\t\t\t\t\t\tin your bucket, simplifying access management for data stored in Amazon S3.\n\t\t\t\t\t\tS3 Object Ownership is an Amazon S3 bucket-level setting that you can use to\n\t\t\t\t\t\tdisable or enable ACLs. By default, ACLs are disabled. With ACLs disabled,\n\t\t\t\t\t\tthe bucket owner owns all the objects in the bucket and manages access to\n\t\t\t\t\t\tdata exclusively by using access-management policies. IAM Access Analyzer for S3 \u2013 Evaluate and monitor your S3 bucket access policies, ensuring that\n\t\t\t\t\t\tthe policies provide only the intended access to your S3 resources. Data processing To transform data and trigger workflows to automate a variety of other processing\n\t\t\t\tactivities at scale, you can use the following features. S3 Object Lambda \u2013 Add your own code to S3 GET, HEAD, and LIST requests to modify and process data as\n\t\t\t\t\t\tit is returned to an application. Filter rows, dynamically resize images,\n\t\t\t\t\t\tredact confidential data, and much more. Event\n\t\t\t\t\t\t\tnotifications \u2013 Trigger workflows that use Amazon Simple Notification Service\n\t\t\t\t\t\t(Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3\n\t\t\t\t\t\tresources. Storage logging and monitoring Amazon S3 provides logging and monitoring tools that you can use to monitor and control\n\t\t\t\thow your Amazon S3 resources are being used. For more information, see Monitoring\n\t\t\t\t\ttools . Automated monitoring tools Amazon CloudWatch\n\t\t\t\t\t\t\tmetrics for Amazon S3 \u2013 Track the operational health of your\n\t\t\t\t\t\tS3 resources and configure billing alerts when estimated charges reach a\n\t\t\t\t\t\tuser-defined threshold. AWS CloudTrail \u2013 Record actions taken by a user, a role, or an AWS service in\n\t\t\t\t\t\tAmazon S3. CloudTrail logs provide you with detailed API tracking for S3 bucket-level\n\t\t\t\t\t\tand object-level operations. Manual monitoring tools Server access\n\t\t\t\t\t\t\tlogging \u2013 Get detailed records for the requests that are\n\t\t\t\t\t\tmade to a bucket. You can use server access logs for many use cases, such as\n\t\t\t\t\t\tconducting security and access audits, learning about your customer base,\n\t\t\t\t\t\tand understanding your Amazon S3 bill. AWS Trusted\n\t\t\t\t\t\t\tAdvisor \u2013 Evaluate your account by using AWS best\n\t\t\t\t\t\tpractice checks to identify ways to optimize your AWS infrastructure,\n\t\t\t\t\t\timprove security and performance, reduce costs, and monitor service quotas.\n\t\t\t\t\t\tYou can then follow the recommendations to optimize your services and\n\t\t\t\t\t\tresources. Analytics and insights Amazon S3 offers features to help you gain visibility into your storage usage, which\n\t\t\t\tempowers you to better understand, analyze, and optimize your storage at\n\t\t\t\tscale. Amazon S3 Storage Lens \u2013 Understand, analyze, and optimize your storage. S3 Storage Lens provides\n\t\t\t\t\t\t60+ usage and activity metrics and interactive dashboards to aggregate data\n\t\t\t\t\t\tfor your entire organization, specific accounts, AWS Regions, buckets, or\n\t\t\t\t\t\tprefixes. Storage\n\t\t\t\t\t\t\tClass Analysis \u2013 Analyze storage access patterns to\n\t\t\t\t\t\tdecide when it's time to move data to a more cost-effective storage class. S3 Inventory with\n\t\t\t\t\t\t\tInventory reports \u2013 Audit and report on objects and their\n\t\t\t\t\t\tcorresponding metadata and configure other Amazon S3 features to take action in\n\t\t\t\t\t\tInventory reports. For example, you can report on the replication and\n\t\t\t\t\t\tencryption status of your objects. For a list of all the metadata available\n\t\t\t\t\t\tfor each object in Inventory reports, see Amazon S3 Inventory list . Strong consistency Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of\n\t\t\t\tobjects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both\n\t\t\t\twrites of new objects as well as PUT requests that overwrite existing objects and\n\t\t\t\tDELETE requests. In addition, read operations on Amazon S3 Select, Amazon S3 access control\n\t\t\t\tlists (ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object)\n\t\t\t\tare strongly consistent. For more information, see Amazon S3 data consistency model . How Amazon S3 works Amazon S3 is an object storage service that stores data as objects within buckets. An object is a file and any metadata that describes\n\t\t\tthe file. A bucket is a container for objects. To store your data in Amazon S3, you first create a bucket and specify a bucket name and\n\t\t\tAWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object\n\t\t\thas a key (or key\n\t\t\t\tname ), which is the unique identifier for the object within the\n\t\t\tbucket. S3 provides features that you can configure to support your specific use case. For\n\t\t\texample, you can use S3 Versioning to keep multiple versions of an object in the same\n\t\t\tbucket, which allows you to restore objects that are accidentally deleted or\n\t\t\toverwritten. Buckets and the objects in them are private and can be accessed only if you explicitly\n\t\t\tgrant access permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies,\n\t\t\taccess control lists (ACLs), and S3 Access Points to manage access. Topics Buckets Objects Keys S3 Versioning Version ID Bucket policy S3 Access Points Access control lists (ACLs) Regions Buckets A bucket is a container for objects stored in Amazon S3. You can store any number of\n\t\t\t\tobjects in a bucket and can have up to 100 buckets in your account. To see your bucket utilization, bucket quota, or request an\n\t\t\t\tincrease, visit the Service Quotas\n\t\t\t\tconsole . Every object is contained in a bucket. For example, if the object named photos/puppy.jpg is stored in the amzn-s3-demo-bucket bucket in the US West (Oregon)\n\t\t\t\tRegion, then it is addressable by using the URL https://amzn-s3-demo-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg .\n\t\t\t\tFor more information, see Accessing a\n\t\t\t\t\tBucket . When you create a bucket, you enter a bucket name and choose the AWS Region\n\t\t\t\twhere the bucket will reside. After you create a bucket, you cannot change the name\n\t\t\t\tof the bucket or its Region. Bucket names must follow the bucket naming rules . You can also configure a bucket to use S3 Versioning or other storage management features. Buckets also: Organize the Amazon S3 namespace at the highest level. Identify the account responsible for storage and data transfer\n\t\t\t\t\t\tcharges. Provide access control options, such as bucket policies, access control\n\t\t\t\t\t\tlists (ACLs), and S3 Access Points, that you can use to manage access to\n\t\t\t\t\t\tyour Amazon S3 resources. Serve as the unit of aggregation for usage reporting. For more information about buckets, see Buckets overview . Objects Objects are the fundamental entities stored in Amazon S3. Objects consist of object\n\t\t\t\tdata and metadata. The metadata is a set of name-value pairs that describe the\n\t\t\t\tobject. These pairs include some default metadata, such as the date last modified,\n\t\t\t\tand standard HTTP metadata, such as Content-Type . You can also specify\n\t\t\t\tcustom metadata at the time that the object is stored. An object is uniquely identified within a bucket by a key (name) and a version ID (if\n\t\t\t\tS3 Versioning is enabled on the bucket). For more information about objects, see Amazon S3 objects overview . Keys An object key (or key\n\t\t\t\t\tname ) is the unique identifier for an object within a bucket. Every\n\t\t\t\tobject in a bucket has exactly one key. The combination of a bucket, object key, and\n\t\t\t\toptionally, version ID (if S3 Versioning is enabled for the bucket) uniquely identify\n\t\t\t\teach object. So you can think of Amazon S3 as a basic data map between \"bucket + key +\n\t\t\t\tversion\" and the object itself. Every object in Amazon S3 can be uniquely addressed through the combination of the web service\n\t\t\t\tendpoint, bucket name, key, and optionally, a version. For example, in the URL https:// amzn-s3-demo-bucket .s3.us-west-2.amazonaws.com/photos/puppy.jpg , amzn-s3-demo-bucket is the name of the bucket\n\t\t\t\tand photos/puppy.jpg is the key. For more information about object keys, see Naming Amazon S3 objects . S3 Versioning You can use S3 Versioning to keep multiple variants of an object in the same\n\t\t\t\tbucket. With S3 Versioning, you can preserve, retrieve, and restore every version of\n\t\t\t\tevery object stored in your buckets. You can easily recover from both unintended\n\t\t\t\tuser actions and application failures. For more information, see Retaining multiple versions of objects with S3 Versioning . Version ID When you enable S3 Versioning in a bucket, Amazon S3 generates a unique version ID for\n\t\t\t\teach object added to the bucket. Objects that already existed in the bucket at the\n\t\t\t\ttime that you enable versioning have a version ID of null . If you\n\t\t\t\tmodify these (or any other) objects with other operations, such as CopyObject and PutObject , the new objects\n\t\t\t\tget a unique version ID. For more information, see Retaining multiple versions of objects with S3 Versioning . Bucket policy A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that you can use to\n\t\t\t\tgrant access permissions to your bucket and the objects in it. Only the bucket owner\n\t\t\t\tcan associate a policy with a bucket. The permissions attached to the bucket apply\n\t\t\t\tto all of the objects in the bucket that are owned by the bucket owner. Bucket\n\t\t\t\tpolicies are limited to 20 KB in size. Bucket policies use JSON-based access policy language that is standard across\n\t\t\t\tAWS. You can use bucket policies to add or deny permissions for the objects in a\n\t\t\t\tbucket. Bucket policies allow or deny requests based on the elements in the policy,\n\t\t\t\tincluding the requester, S3 actions, resources, and aspects or conditions of the\n\t\t\t\trequest (for example, the IP address used to make the request). For example, you can\n\t\t\t\tcreate a bucket policy that grants cross-account permissions to upload objects to an\n\t\t\t\tS3 bucket while ensuring that the bucket owner has full control of the uploaded\n\t\t\t\tobjects. For more information, see Examples of Amazon S3 bucket policies . In your bucket policy, you can use wildcard characters on Amazon Resource Names\n\t\t\t\t(ARNs) and other values to grant permissions to a subset of objects. For example,\n\t\t\t\tyou can control access to groups of objects that begin with a common prefix or end with a given extension, such as .html . S3 Access Points Amazon S3 Access Points are named network endpoints with dedicated access policies that\n\t\t\t\tdescribe how data can be accessed using that endpoint. Access Points are attached to\n\t\t\t\tbuckets that you can use to perform S3 object operations, such as GetObject and\n\t\t\t\tPutObject. Access Points simplify managing data access at scale for shared datasets\n\t\t\t\tin Amazon S3. Each access point has its own access point policy. You can configure Block Public Access settings\n\t\t\t\tfor each access point. To restrict Amazon S3 data access to a private network, you can\n\t\t\t\talso configure any access point to accept requests only from a virtual private cloud\n\t\t\t\t(VPC). For more information, see Managing access to shared datasets with access points . Access control lists (ACLs) You can use ACLs to grant read and write permissions to authorized users for\n\t\t\t\tindividual buckets and objects. Each bucket and object has an ACL attached to it as\n\t\t\t\ta subresource. The ACL defines which AWS accounts or groups are granted access and\n\t\t\t\tthe type of access. ACLs are an access control mechanism that predates IAM. For\n\t\t\t\tmore information about ACLs, see Access control list (ACL) overview . S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to both control ownership of the objects that are \n    uploaded to your bucket and to disable or enable ACLs. By default, Object Ownership is set to the Bucket owner enforced setting, \n    and all ACLs are disabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and manages access to them \n    exclusively by using access-management policies. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except \n\tin unusual circumstances where you need to control access for each object individually. With ACLs disabled, you can use policies \n\tto control access to all objects in your bucket, regardless of who uploaded the objects to your bucket. \n\tFor more information, see Controlling ownership of objects and disabling ACLs\n\t\t\tfor your bucket . Regions You can choose the geographical AWS Region where Amazon S3 stores the buckets that\n\t\t\t\tyou create. You might choose a Region to optimize latency, minimize costs, or\n\t\t\t\taddress regulatory requirements. Objects stored in an AWS Region never leave the\n\t\t\t\tRegion unless you explicitly transfer or replicate them to another Region. For example, objects stored in the Europe (Ireland) Region\n\t\t\t\t\tnever leave it. Note You can access Amazon S3 and its features only in the AWS Regions that are\n\t\t\t\t\tenabled for your account. For more information about enabling a Region to create\n\t\t\t\t\tand manage AWS resources, see Managing AWS Regions in\n\t\t\t\t\tthe AWS General Reference . For a list of Amazon S3 Regions and endpoints, see Regions and endpoints in the AWS General Reference . Amazon S3 data consistency model Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of\n\t\t\tobjects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes\n\t\t\tto new objects as well as PUT requests that overwrite existing objects and DELETE\n\t\t\trequests. In addition, read operations on Amazon S3 Select, Amazon S3 access controls lists\n\t\t\t(ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object) are\n\t\t\tstrongly consistent. Updates to a single key are atomic. For example, if you make a PUT request to an\n\t\t\texisting key from one thread and perform a GET request on the same key from a second\n\t\t\tthread concurrently, you will get either the old data or the new data, but never partial\n\t\t\tor corrupt data. Amazon S3 achieves high availability by replicating data across multiple servers within\n\t\t\tAWS data centers. If a PUT request is successful, your data is safely stored. Any read\n\t\t\t(GET or LIST request) that is initiated following the receipt of a successful PUT\n\t\t\tresponse will return the data written by the PUT request. Here are examples of this\n\t\t\tbehavior: A process writes a new object to Amazon S3 and immediately lists keys within its\n\t\t\t\t\tbucket. The new object appears in the list. A process replaces an existing object and immediately tries to read it. Amazon S3\n\t\t\t\t\treturns the new data. A process deletes an existing object and immediately tries to read it. Amazon S3\n\t\t\t\t\tdoes not return any data because the object has been deleted. A process deletes an existing object and immediately lists keys within its\n\t\t\t\t\tbucket. The object does not appear in the listing. Note Amazon S3 does not support object locking for concurrent writers. If two PUT\n\t\t\t\t\t\trequests are simultaneously made to the same key, the request with the\n\t\t\t\t\t\tlatest timestamp wins. If this is an issue, you must build an object-locking\n\t\t\t\t\t\tmechanism into your application. Updates are key-based. There is no way to make atomic updates across keys.\n\t\t\t\t\t\tFor example, you cannot make the update of one key dependent on the update\n\t\t\t\t\t\tof another key unless you design this functionality into your\n\t\t\t\t\t\tapplication. Bucket configurations have an eventual consistency model. Specifically, this means\n\t\t\tthat: If you delete a bucket and immediately list all buckets, the deleted bucket\n\t\t\t\t\tmight still appear in the list. If you enable versioning on a bucket for the first time, it might take a short\n\t\t\t\t\tamount of time for the change to be fully propagated. We recommend that you wait\n\t\t\t\t\tfor 15 minutes after enabling versioning before issuing write operations (PUT or\n\t\t\t\t\tDELETE requests) on objects in the bucket. Concurrent applications This section provides examples of behavior to be expected from Amazon S3 when multiple\n\t\t\t\tclients are writing to the same items. In this example, both W1 (write 1) and W2 (write 2) finish before the start of R1\n\t\t\t\t(read 1) and R2 (read 2). Because S3 is strongly consistent, R1 and R2 both return color = ruby . In the next example, W2 does not finish before the start of R1. Therefore, R1\n\t\t\t\tmight return color = ruby or color = garnet . However,\n\t\t\t\tbecause W1 and W2 finish before the start of R2, R2 returns color =\n\t\t\t\t\tgarnet . In the last example, W2 begins before W1 has received an acknowledgment.\n\t\t\t\tTherefore, these writes are considered concurrent. Amazon S3 internally uses\n\t\t\t\tlast-writer-wins semantics to determine which write takes precedence. However, the\n\t\t\t\torder in which Amazon S3 receives the requests and the order in which applications\n\t\t\t\treceive acknowledgments cannot be predicted because of various factors, such as\n\t\t\t\tnetwork latency. For example, W2 might be initiated by an Amazon EC2 instance in the same\n\t\t\t\tRegion, while W1 might be initiated by a host that is farther away. The best way to\n\t\t\t\tdetermine the final value is to perform a read after both writes have been\n\t\t\t\tacknowledged. Related services After you load your data into Amazon S3, you can use it with other AWS services. The\n\t\t\tfollowing are the services that you might use most frequently: Amazon Elastic Compute Cloud\n\t\t\t\t\t\t\t(Amazon EC2) \u2013 Provides secure and scalable computing\n\t\t\t\t\tcapacity in the AWS Cloud. Using Amazon EC2 eliminates your need to invest in\n\t\t\t\t\thardware upfront, so you can develop and deploy applications faster. You can\n\t\t\t\t\tuse Amazon EC2 to launch as many or as few virtual servers as you need, configure\n\t\t\t\t\tsecurity and networking, and manage storage. Amazon EMR \u2013 Helps businesses, researchers, data\n\t\t\t\t\tanalysts, and developers easily and cost-effectively process vast amounts of\n\t\t\t\t\tdata. Amazon EMR uses a hosted Hadoop framework running on the web-scale\n\t\t\t\t\tinfrastructure of Amazon EC2 and Amazon S3. AWS Snow\n\t\t\t\t\t\t\tFamily \u2013 Helps customers that need to run\n\t\t\t\t\toperations in austere, non-data center environments, and in locations where\n\t\t\t\t\tthere's a lack of consistent network connectivity. You can use AWS Snow Family\n\t\t\t\t\tdevices to locally and cost-effectively access the storage and compute power of\n\t\t\t\t\tthe AWS Cloud in places where an internet connection might not be an option. AWS Transfer Family \u2013 Provides fully managed support for\n\t\t\t\t\tfile transfers directly into and out of Amazon S3 or Amazon Elastic File System (Amazon EFS) using Secure\n\t\t\t\t\tShell (SSH) File Transfer Protocol (SFTP), File Transfer Protocol over SSL\n\t\t\t\t\t(FTPS), and File Transfer Protocol (FTP). Accessing Amazon S3 You can work with Amazon S3 in any of the following ways: AWS Management Console The console is a web-based user interface for managing Amazon S3 and AWS resources.\n\t\t\t\tIf you've signed up for an AWS account, you can access the Amazon S3 console by signing\n\t\t\t\tinto the AWS Management Console and choosing S3 from the AWS Management Console home\n\t\t\t\tpage. AWS Command Line Interface You can use the AWS command line tools to issue commands or build scripts at\n\t\t\t\tyour system's command line to perform AWS (including S3) tasks. The AWS Command Line Interface (AWS CLI) provides commands\n\t\t\t\tfor a broad set of AWS services. The AWS CLI is supported on Windows, macOS, and\n\t\t\t\tLinux. To get started, see the AWS Command Line Interface User Guide . For more information about the commands for\n\t\t\t\tAmazon S3, see s3api and s3control in the AWS CLI Command Reference . AWS SDKs AWS provides SDKs (software development kits) that consist of libraries and sample code\n\t\t\t\tfor various programming languages and platforms (Java, Python, Ruby, .NET, iOS,\n\t\t\t\tAndroid, and so on). The AWS SDKs provide a convenient way to create programmatic\n\t\t\t\taccess to S3 and AWS. Amazon S3 is a REST service. You can send requests to Amazon S3 using\n\t\t\t\tthe AWS SDK libraries, which wrap the underlying Amazon S3 REST API and simplify your\n\t\t\t\tprogramming tasks. For example, the SDKs take care of tasks such as calculating\n\t\t\t\tsignatures, cryptographically signing requests, managing errors, and retrying\n\t\t\t\trequests automatically. For information about the AWS SDKs, including how to\n\t\t\t\tdownload and install them, see Tools for\n\t\t\t\t\tAWS . Every interaction with Amazon S3 is either authenticated or anonymous. If you are using\n\t\t\t\tthe AWS SDKs, the libraries compute the signature for authentication from the keys\n\t\t\t\tthat you provide. For more information about how to make requests to Amazon S3, see Making requests . Amazon S3 REST API The architecture of Amazon S3 is designed to be programming language-neutral, using\n\t\t\t\tAWS-supported interfaces to store and retrieve objects. You can access S3 and\n\t\t\t\tAWS programmatically by using the Amazon S3 REST API. The REST API is an HTTP interface\n\t\t\t\tto Amazon S3. With the REST API, you use standard HTTP requests to create, fetch, and\n\t\t\t\tdelete buckets and objects. To use the REST API, you can use any toolkit that supports HTTP. You can even use\n\t\t\t\ta browser to fetch objects, as long as they are anonymously readable. The REST API uses standard HTTP headers and status codes, so that standard\n\t\t\t\tbrowsers and toolkits work as expected. In some areas, we have added functionality\n\t\t\t\tto HTTP (for example, we added headers to support access control). In these cases,\n\t\t\t\twe have done our best to add the new functionality in a way that matches the style\n\t\t\t\tof standard HTTP usage. If you make direct REST API calls in your application, you must write the code to\n\t\t\t\tcompute the signature and add it to the request. For more information about how to\n\t\t\t\tmake requests to Amazon S3, see Making requests in the Amazon S3 API Reference . Note SOAP API support over HTTP is deprecated, but it is still available over\n\t\t\t\t\tHTTPS. Newer Amazon S3 features are not supported for SOAP. We recommend that you use\n\t\t\t\t\teither the REST API or the AWS SDKs. Paying for Amazon S3 Pricing for Amazon S3 is designed so that you don't have to plan for the storage\n\t\t\trequirements of your application. Most storage providers require you to purchase a\n\t\t\tpredetermined amount of storage and network transfer capacity. In this scenario, if you\n\t\t\texceed that capacity, your service is shut off or you are charged high overage fees. If\n\t\t\tyou do not exceed that capacity, you pay as though you used it all. Amazon S3 charges you only for what you actually use, with no hidden fees and no overage\n\t\t\tcharges. This model gives you a variable-cost service that can grow with your business\n\t\t\twhile giving you the cost advantages of the AWS infrastructure. For more information,\n\t\t\tsee Amazon S3 Pricing . When you sign up for AWS, your AWS account is automatically signed up for all\n\t\t\tservices in AWS, including Amazon S3. However, you are charged only for the services that\n\t\t\tyou use. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For\n\t\t\tmore information, see AWS free tier . To see your bill, go to the Billing and Cost Management Dashboard in the AWS Billing and Cost Management console . To learn more about AWS account billing, see the AWS Billing User Guide . If you have\n\t\t\tquestions concerning AWS billing and AWS accounts, contact AWS Support . PCI DSS compliance Amazon S3 supports the processing, storage, and transmission \nof credit card data by a merchant or service provider, and has been \nvalidated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). \nFor more information about PCI DSS, including how to request a copy of the AWS PCI Compliance Package, \nsee PCI DSS Level 1 . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Getting started"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-userguide.pdf", "content": "No main content found."}, {"title": "Welcome - Amazon Simple Storage Service", "url": "https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html", "content": "Welcome PDF Welcome to the Amazon Simple Storage Service API Reference . This guide explains the Amazon Simple Storage Service (Amazon S3)\n\t\tapplication programming interface (API). You can use any toolkit that supports HTTP to use the REST API. You can even use a browser\n\t\tto fetch objects, as long as they are anonymously readable. The REST API uses the standard HTTP headers and status codes, so that standard browsers and\n\t\ttoolkits work as expected. In some areas, we have added functionality to HTTP (for example, we\n\t\tadded headers to support access control). In these cases, we have done our best to add the new\n\t\tfunctionality in a way that matched the style of standard HTTP usage. Version The current version of the Amazon S3 API is 2006-03-01 . Type Amazon S3 supports the REST API. Note Support for SOAP over HTTP is deprecated, but it is still available over HTTPS.\n\t\t\tHowever, new Amazon S3 features will not be supported for SOAP. We recommend that\n\t\t\tyou use either this REST API or the AWS SDKs at the following link: https://aws.amazon.com/developer/tools/ This REST API reference includes: S3 API Reference \u2014 which contains Actions (operations) and Data Types Headers \u2014 Common Request Headers and Common Response Headers Error responses Browser-Based Uploads Using\n\t\t\tPOST (AWS Signature Version 4) Important Read the following about authentication and access control before going to specific API\n\t\ttopics. Requests to Amazon S3 can be authenticated or anonymous. Authenticated access requires credentials that AWS can use to authenticate your requests. API call recommendations Making REST API calls directly from your code can be cumbersome. It requires you to write\n\t\tthe necessary code to calculate a valid signature to authenticate your requests. We recommend the\n\t\tfollowing alternatives instead: Use the AWS SDKs to send your requests. Also, see the Sample Code and\n\t\t\t\t\tLibraries . If you use the SDKs, you don't need\n\t\t\t\t\tto write code to calculate a signature for request authentication because the SDK\n\t\t\t\t\tclients authenticate your requests by using access keys that you provide. Unless\n\t\t\t\t\tyou have a good reason not to, you should always use the AWS SDKs. Use the AWS CLI to make Amazon S3 API calls. For information about setting up the AWS\n\t\t\t\t\tCLI and example Amazon S3 commands see the following topics: Set Up the AWS CLI in the Amazon Simple Storage Service User Guide . Using\n\t\t\t\t\t\tAmazon S3 with the AWS Command Line Interface in the AWS Command Line Interface User Guide . Making direct REST API calls Note The PUT request header is limited to 8 KB in size. Within the PUT request header, the system-defined metadata is limited to 2 KB in size. The size of system-defined metadata is measured by taking the sum of the number of bytes in the US-ASCII encoding of each key and value. If you'd like to make your own REST API calls instead of using one of the above alternatives, there\n\t\t\tare some things to keep in mind. To make direct REST API calls from your code, create a signature using valid credentials and include the\n\t\t\tsignature in your request. For information about various authentication methods and signature calculations, see Authenticating Requests (AWS Signature Version\n\t\t4) . The REST API uses standard HTTP headers and status codes, so\n        standard browsers and toolkits work as expected. In some areas, we have added functionality to HTTP\n        (for example, we added headers to support access control). In these cases, we have done our best to\n        add the new functionality in a way that matches the style of standard HTTP usage. For more information\n        about making requests, see Making requests . Permissions You can have valid credentials to authenticate your requests, but unless you have S3 permissions from the account owner or bucket owner\n\t\tyou cannot create or access Amazon S3 resources. These permissions are typically granted through an AWS Identity and Access Management (IAM) policy , such as a bucket policy. For example, you must have permissions to create\n\t\tan S3 bucket or get an object in a bucket. For a complete list of S3 permissions, see Actions, resources, and condition keys for Amazon S3 . For more information about the permissions to S3 API operations by S3 resource types, see Required permissions for Amazon S3 API operations in the Amazon Simple Storage Service User Guide . If you use the root user credentials of your\n\t\tAWS account, you have all the permissions. However, using root user credentials is not\n\t\trecommended.\n\t\tInstead,\n\t\twe recommend that you create AWS Identity and Access Management (IAM) roles in your account and manage user permissions. For\n\t\tmore information, see Access Management in the Amazon Simple Storage Service User Guide . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions S3 API Reference"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonS3/latest/API/s3-api.pdf", "content": "No main content found."}, {"title": "What Is Amazon S3 Glacier? - Amazon S3 Glacier", "url": "https://docs.aws.amazon.com/amazonglacier/latest/dev/index.html", "content": "This page is only for existing customers of the S3 Glacier service using Vaults and the original REST API from 2012. If you're looking for archival storage solutions we suggest using the S3 Glacier storage classes in Amazon S3, S3 Glacier Instant Retrieval , S3 Glacier Flexible Retrieval , and S3 Glacier Deep Archive . To learn more about these storage options, see S3 Glacier storage classes and Long-term data storage using S3 Glacier storage classes in the Amazon S3 User\n\t\t\t\t\tGuide . These storage classes use the Amazon S3 API, are available in all regions, and can be managed within the Amazon S3 console. They offer features like Storage Cost Analysis, Storage Lens, advanced optional encryption features, and more. This page is only for existing customers of the S3 Glacier service using Vaults and the original REST API from 2012. If you're looking for archival storage solutions we suggest using the S3 Glacier storage classes in Amazon S3, S3 Glacier Instant Retrieval , S3 Glacier Flexible Retrieval , and S3 Glacier Deep Archive . To learn more about these storage options, see S3 Glacier storage classes and Long-term data storage using S3 Glacier storage classes in the Amazon S3 User\n\t\t\t\t\tGuide . These storage classes use the Amazon S3 API, are available in all regions, and can be managed within the Amazon S3 console. They offer features like Storage Cost Analysis, Storage Lens, advanced optional encryption features, and more. What Is Amazon S3 Glacier? PDF RSS If you're currently using the Amazon S3 Glacier (S3 Glacier) service and want to learn more, you'll\n\t\tfind the information that you need in this guide. S3 Glacier is a secure and durable service\n\t\tfor low-cost data archiving and long-term backup using vaults. For more information about\n\t\tS3 Glacier service pricing, see S3 Glacier\n\t\t\tpricing . Topics Do You Currently Use\n\t\t\t\tS3 Glacier? Amazon S3 Glacier Data Model Supported Operations in\n\t\t\t\tS3 Glacier Accessing Amazon S3 Glacier Do You Currently Use\n\t\t\t\tS3 Glacier? Note This section is about the S3 Glacier service. If you currently use the S3 Glacier storage classes\n\t\t\t\t\t( S3 Glacier Instant Retrieval , S3 Glacier Flexible Retrieval , and S3 Glacier Deep Archive ), see Storage classes for archiving objects in the Amazon S3\n\t\t\t\t\tUser Guide . If you currently use the S3 Glacier service and want to learn more, we recommend that\n\t\t\tyou begin by reading the following sections: What is Amazon S3 Glacier \u2013 The rest of this\n\t\t\t\t\tsection describes the underlying data model, the operations it supports, and the\n\t\t\t\t\tAWS SDKs that you can use to interact with the service. Getting Started \u2013 The Getting Started with Amazon S3 Glacier section walks you through\n\t\t\t\t\tthe process of creating a vault, uploading archives, creating jobs to download\n\t\t\t\t\tarchives, retrieving the job output, and deleting archives. Important S3 Glacier does provide a console. However, any archive operation, such as\n\t\t\t\t\t\tupload, download, or deletion, requires you to use the AWS Command Line Interface (AWS CLI) or\n\t\t\t\t\t\twrite code. There is no console support for archive operations. For example,\n\t\t\t\t\t\tto upload data, such as photos, videos, and other documents, you must either\n\t\t\t\t\t\tuse the AWS CLI or write code to make requests, by using either the REST API\n\t\t\t\t\t\tdirectly or by using the AWS SDKs. To install the AWS CLI, see AWS Command Line Interface . For more information about using S3 Glacier with the\n\t\t\t\t\t\tAWS CLI, see the AWS CLI\n\t\t\t\t\t\t\tReference for S3 Glacier . For examples of using the AWS CLI to\n\t\t\t\t\t\tupload archives to S3 Glacier, see Using S3 Glacier with the AWS Command Line Interface . Beyond the getting started section, you'll probably want to learn more about S3 Glacier\n\t\t\toperations. The following sections provide detailed information about working with\n\t\t\tS3 Glacier by using the REST API and the AWS SDKs for Java and Microsoft .NET: Using the AWS SDKs with Amazon S3 Glacier This section provides an overview of the AWS SDKs used in various code\n\t\t\t\t\texamples in this guide. A review of this section will help when reading the\n\t\t\t\t\tfollowing sections. It includes an overview of the high-level and the low-level\n\t\t\t\t\tAPIs that these SDKs offer, when to use them, and common steps for running the\n\t\t\t\t\tcode examples provided in this guide. Working with Vaults in Amazon S3 Glacier This section provides details of various vault operations, such as creating a\n\t\t\t\t\tvault, retrieving vault metadata, using jobs to retrieve vault inventory, and\n\t\t\t\t\tconfiguring vault notifications. In addition to using the S3 Glacier console, you\n\t\t\t\t\tcan use the AWS SDKs for various vault operations. This section describes the\n\t\t\t\t\tAPI and provides working samples by using the AWS SDK for Java and the\n\t\t\t\t\tAWS SDK for .NET. Working with Archives in Amazon S3 Glacier This section provides details of archive operations, such as uploading an\n\t\t\t\t\tarchive in a single request or using a multipart upload operation to upload\n\t\t\t\t\tlarge archives in parts. The section also explains how to create jobs to\n\t\t\t\t\tdownload archives asynchronously. The section provides examples by using the\n\t\t\t\t\tAWS SDK for Java and the AWS SDK for .NET. API Reference for Amazon S3 Glacier S3 Glacier is a RESTful service. This section describes the REST operations,\n\t\t\t\t\tincluding the syntax, and example requests and responses for all the operations.\n\t\t\t\t\tThe AWS SDK libraries wrap this API, simplifying your programming tasks. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Data Model"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-dg.pdf", "content": "No main content found."}, {"title": "What is Amazon S3 on Outposts? - Amazon S3 on Outposts", "url": "https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/index.html", "content": "What is Amazon S3 on Outposts? PDF AWS Outposts is a fully managed service that offers the same AWS infrastructure, AWS\n\t\tservices, APIs, and tools to virtually any data center, co-location space, or on-premises\n\t\tfacility for a truly consistent hybrid experience. AWS Outposts is ideal for workloads that require\n\t\tlow-latency access to on-premises systems, local data processing, data residency, and\n\t\tmigration of applications with local system interdependencies. For more information, see What is\n\t\t\tAWS Outposts? in the AWS Outposts User Guide . With Amazon S3 on Outposts, you can create S3 buckets on your Outposts and easily store and\n\t\tretrieve objects on premises. S3 on Outposts provides a new storage class, OUTPOSTS , which uses the Amazon S3 APIs and is designed to store data durably\n\t\tand redundantly across multiple devices and servers on your Outposts. You communicate with\n\t\tyour Outposts bucket by using an access point and endpoint connection over a virtual private\n\t\tcloud (VPC). You can use the same APIs and features on Outposts buckets as you do on Amazon S3, including\n\t\taccess policies, encryption, and tagging. You can use S3 on Outposts through the AWS Management Console,\n\t\tAWS Command Line Interface (AWS CLI), AWS SDKs, or REST API. How S3 on Outposts works Features of S3 on Outposts Related services Accessing S3 on Outposts Paying for S3 on Outposts Next steps How S3 on Outposts works S3 on Outposts is an object storage service that stores data as objects within buckets\n\t\t\ton your Outpost. An object is a data file and any\n\t\t\tmetadata that describes the file. A bucket is a\n\t\t\tcontainer for objects. To store your data in S3 on Outposts, you first create a bucket. When you create the\n\t\t\tbucket, you specify a bucket name and the Outpost that will hold the bucket. To access\n\t\t\tyour S3 on Outposts bucket and perform object operations, you next create and configure an\n\t\t\taccess point. You must also create an endpoint to route requests to your access point. Access points simplify data access for any AWS service or customer application that\n\t\t\tstores data in S3. Access points are named network endpoints that are attached to buckets\n\t\t\tand can be used to perform object operations, such as GetObject and PutObject . Each access point has distinct permissions and network\n\t\t\tcontrols. You can create and manage your S3 on Outposts buckets, access points, and endpoints by\n\t\t\tusing the AWS Management Console, AWS CLI, AWS SDKs, or REST API. To upload and manage objects in\n\t\t\tyour S3 on Outposts bucket, you can use the AWS CLI, AWS SDKs, or REST API. Regions During AWS Outposts provisioning, you or AWS creates a service link connection that\n\t\t\t\tconnects your Outpost back to your chosen AWS Region or Outposts home Region for\n\t\t\t\tbucket operations and telemetry. An Outpost relies on connectivity to the parent\n\t\t\t\tAWS Region. The Outposts rack is not designed for disconnected operations or\n\t\t\t\tenvironments with limited to no connectivity. For more information, see Outpost connectivity to AWS Regions in the AWS Outposts User Guide . Buckets A bucket is a container for objects stored in S3 on Outposts. You can store any\n\t\t\t\tnumber of objects in a bucket and can have up to 100 buckets per account per\n\t\t\t\tOutpost. When you create a bucket, you enter a bucket name and choose the Outpost where the\n\t\t\t\tbucket will reside. After you create a bucket, you cannot change the bucket name or\n\t\t\t\tmove the bucket to a different Outpost. Bucket names must follow Amazon S3\n\t\t\t\t\tbucket naming rules . In S3 on Outposts, bucket names are unique to an\n\t\t\t\tOutpost and AWS account. S3 on Outposts buckets require the outpost-id , account-id , and bucket name to identify them. The following example shows the Amazon Resource Name (ARN) format for S3 on Outposts\n\t\t\t\tbuckets. The ARN is comprised of the Region your Outpost is homed to, your Outpost\n\t\t\t\taccount, the Outpost ID, and the bucket name. arn:aws:s3-outposts: region : account-id :outpost/ outpost-id /bucket/ bucket-name Every object is contained in a bucket. You must use access points to access any object in\n\t\t\t\tan Outposts bucket. When you specify the bucket for object operations, you use the\n\t\t\t\taccess point ARN or access point alias. For more information about access point aliases, see Using a bucket-style alias for your\n            S3 on Outposts bucket access point . The following example shows the access point ARN format for S3 on Outposts, which\n\t\t\t\tincludes the outpost-id , account-id , and access point\n\t\t\t\tname: arn:aws:s3-outposts: region : account-id :outpost/ outpost-id /accesspoint/ accesspoint-name For more information about buckets, see Working with S3 on Outposts buckets . Objects Objects are the fundamental entities stored in S3 on Outposts. Objects consist of\n\t\t\t\tobject data and metadata. The metadata is a set of name-value pairs that describe\n\t\t\t\tthe object. These pairs include some default metadata, such as the date last\n\t\t\t\tmodified, and standard HTTP metadata, such as Content-Type . You can\n\t\t\t\talso specify custom metadata at the time that the object is stored. An object is\n\t\t\t\tuniquely identified within a bucket by a key (or\n\t\t\t\t\tname). With Amazon S3 on Outposts, object data is always stored on the Outpost. When AWS installs an Outpost rack, your data stays local to \n        your Outpost to meet data-residency requirements. Your objects never leave your\n\t\t\t\tOutpost and are not in an AWS Region. Because the AWS Management Console is hosted in-Region, you can't use the console to upload or manage objects in\n\t\t\t\tyour Outpost. However, you can use the REST API, AWS Command Line Interface (AWS CLI), and AWS SDKs to upload and manage your objects through your access points. Keys An object key (or key\n\t\t\t\t\tname ) is the unique identifier for an object within a bucket. Every\n\t\t\t\tobject in a bucket has exactly one key. The combination of a bucket and object key\n\t\t\t\tuniquely identifies each object. The following example shows the ARN format for S3 on Outposts objects, which\n\t\t\t\tincludes the AWS Region code for the Region that the Outpost is homed to,\n\t\t\t\tAWS account ID, Outpost ID, bucket name, and object key: arn:aws:s3-outposts: us-west-2 : 123456789012 :\u200boutpost/ op-01ac5d28a6a232904 /bucket/ amzn -s 3 -demo-bucket 1 /object/myobject For more information about object keys, see Working with S3 on Outposts objects . S3 Versioning You can use S3 Versioning on Outposts buckets to keep multiple variants of an\n\t\t\t\tobject in the same bucket. With S3 Versioning, you can preserve, retrieve, and\n\t\t\t\trestore every version of every object stored in your buckets. S3 Versioning helps you\n\t\t\t\trecover from unintended user actions and application failures. For more information, see Managing S3 Versioning for your S3 on Outposts\n            bucket . Version ID When you enable S3 Versioning in a bucket, S3 on Outposts generates a unique version\n\t\t\t\tID for each object added to the bucket. Objects that already existed in the bucket\n\t\t\t\tat the time that you enable versioning have a version ID of null . If\n\t\t\t\tyou modify these (or any other) objects with other operations, such as PutObject , the new objects get a unique version ID. For more information, see Managing S3 Versioning for your S3 on Outposts\n            bucket . Storage class and encryption S3 on Outposts provides a new storage class, S3 Outposts ( OUTPOSTS ).\n\t\t\t\tThe S3 Outposts storage class is available only for objects stored in buckets on\n\t\t\t\tAWS Outposts. If you try to use other S3 storage classes with S3 on Outposts, S3 on Outposts\n\t\t\t\treturns the InvalidStorageClass error. By default, objects stored in the S3 Outposts ( OUTPOSTS ) storage\n\t\t\t\tclass are encrypted using server-side encryption with Amazon S3 managed encryption keys\n\t\t\t\t(SSE-S3). For more information, see Data encryption in S3 on Outposts . Bucket policy A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that you can use to\n\t\t\t\tgrant access permissions to your bucket and the objects in it. Only the bucket owner\n\t\t\t\tcan associate a policy with a bucket. The permissions attached to the bucket apply\n\t\t\t\tto all of the objects in the bucket that are owned by the bucket owner. Bucket\n\t\t\t\tpolicies are limited to 20 KB in size. Bucket policies use JSON-based IAM policy language that is standard across\n\t\t\t\tAWS. You can use bucket policies to add or deny permissions for the objects in a\n\t\t\t\tbucket. Bucket policies allow or deny requests based on the elements in the policy.\n\t\t\t\tThese elements can include the requester, S3 on Outposts actions, resources, and\n\t\t\t\taspects or conditions of the request (for example, the IP address used to make the\n\t\t\t\trequest). For example, you can create a bucket policy that grants cross-account\n\t\t\t\tpermissions to upload objects to an S3 on Outposts bucket while ensuring that the\n\t\t\t\tbucket owner has full control of the uploaded objects. In your bucket policy, you can use wildcard characters ( * ) in ARNs\n\t\t\t\tand other values to grant permissions to a subset of objects. For example, you can\n\t\t\t\tcontrol access to groups of objects that begin with a common prefix or end with a given extension, such as .html . S3 on Outposts access points S3 on Outposts access points are named network endpoints with dedicated access policies that\n\t\t\t\tdescribe how data can be accessed using that endpoint. Access points simplify managing\n\t\t\t\tdata access at scale for shared datasets in S3 on Outposts. Access points are attached to\n\t\t\t\tbuckets that you can use to perform S3 object operations, such as GetObject and PutObject . When you specify the bucket for object operations, you use the access point ARN or access point alias.\n\t\t\t\tFor more information about access point aliases, see Using a bucket-style alias for your\n            S3 on Outposts bucket access point . Access points have distinct permissions and network controls that S3 on Outposts\n\t\t\t\tapplies for any request that is made through that access point. Each access point enforces a\n\t\t\t\tcustomized access point policy that works in conjunction with the bucket policy that is\n\t\t\t\tattached to the underlying bucket. For more information, see Accessing your S3 on Outposts buckets and\n                objects . Features of S3 on Outposts Access management S3 on Outposts provides features for auditing and managing access to your buckets\n\t\t\t\tand objects. By default, S3 on Outposts buckets and the objects in them are private.\n\t\t\t\tYou have access only to the S3 on Outposts resources that you create. To grant granular resource permissions that support your specific use case or to\n\t\t\t\taudit the permissions of your S3 on Outposts resources, you can use the following\n\t\t\t\tfeatures. S3 Block Public Access \u2013 Block public access to buckets\n\t\t\t\t\t\tand objects. For buckets on Outposts, Block Public Access is always enabled\n\t\t\t\t\t\tby default. AWS Identity and Access Management\n\t\t\t\t\t\t\t(IAM) \u2013 IAM is a web service that helps you securely\n\t\t\t\t\t\tcontrol access to AWS resources, including your S3 on Outposts resources.\n\t\t\t\t\t\tWith IAM, you can centrally manage permissions that control which AWS\n\t\t\t\t\t\tresources users can access. You use IAM to control who is authenticated\n\t\t\t\t\t\t(signed in) and authorized (has permissions) to use resources. S3 on Outposts access points \u2013 Manage data access for shared\n\t\t\t\t\t\tdatasets in S3 on Outposts. Access points are named network endpoints with dedicated\n\t\t\t\t\t\taccess policies. Access points are attached to buckets and can be used to\n\t\t\t\t\t\tperform object operations, such as GetObject and PutObject . Bucket\n\t\t\t\t\t\t\tpolicies \u2013 Use IAM-based policy language to configure\n\t\t\t\t\t\tresource-based permissions for your S3 buckets and the objects in\n\t\t\t\t\t\tthem. AWS Resource Access Manager\n\t\t\t\t\t\t\t(AWS RAM) \u2013 Securely share your S3 on Outposts capacity\n\t\t\t\t\t\tacross AWS accounts, within your organization or organizational units\n\t\t\t\t\t\t(OUs) in AWS Organizations. Storage logging and\n\t\t\t\t\tmonitoring S3 on Outposts provides logging and monitoring tools that you can use to monitor and\n\t\t\t\tcontrol how your S3 on Outposts resources are being used. For more information, see Monitoring\n\t\t\t\ttools . Amazon CloudWatch metrics for S3 on Outposts \u2013 Track the\n\t\t\t\t\t\toperational health of your resources and understand your capacity\n\t\t\t\t\t\tavailability. Amazon CloudWatch Events events for S3 on Outposts \u2013 Create a rule for\n\t\t\t\t\t\tany S3 on Outposts API event to receive notifications through all supported\n\t\t\t\t\t\tCloudWatch Events targets, including Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS), and\n\t\t\t\t\t\tAWS Lambda. AWS CloudTrail logs\n\t\t\t\t\t\t\tfor S3 on Outposts \u2013 Record actions taken by a user, a\n\t\t\t\t\t\trole, or an AWS service in S3 on Outposts. CloudTrail logs provide you with\n\t\t\t\t\t\tdetailed API tracking for S3 bucket-level and object-level\n\t\t\t\t\t\toperations. Strong consistency S3 on Outposts provides strong read-after-write consistency for PUT and DELETE requests of\n\t\t\t\tobjects in your S3 on Outposts bucket in all AWS Regions. This behavior applies to\n\t\t\t\tboth writes of new objects and to PUT requests that overwrite existing objects and\n\t\t\t\tto DELETE requests. In addition, S3 on Outposts object tags and object metadata (for\n\t\t\t\texample, the HEAD object) are strongly consistent. For more information, see Amazon S3 data\n\t\t\t\t\tconsistency model in the Amazon S3 User Guide . Related services After you load your data into S3 on Outposts, you can use it with other AWS services.\n\t\t\tThe following are the services that you might use most frequently: Amazon Elastic Compute Cloud (Amazon EC2) \u2013 Provides secure and scalable computing capacity in the AWS Cloud.\n\t\t\t\t\tUsing Amazon EC2 lessens your need to invest in hardware up front, so you can develop\n\t\t\t\t\tand deploy applications faster. You can use Amazon EC2 to launch as many or as few\n\t\t\t\t\tvirtual servers as you need, configure security and networking, and manage\n\t\t\t\t\tstorage. Amazon Elastic Block Store (Amazon EBS) on\n\t\t\t\t\t\tOutposts \u2013 Use Amazon EBS local snapshots on Outposts to store\n\t\t\t\t\tsnapshots of volumes on an Outpost locally in S3 on Outposts. Amazon Relational Database Service (Amazon RDS) on\n\t\t\t\t\t\tOutposts \u2013 Use Amazon RDS local backups to store your Amazon RDS backups\n\t\t\t\t\tlocally on your Outpost. AWS DataSync \u2013 Automate transferring data between your Outposts and AWS Regions,\n\t\t\t\t\tchoosing what to transfer, when to transfer, and how much network bandwidth to\n\t\t\t\t\tuse. S3 on Outposts is integrated with AWS DataSync. For on-premises applications\n\t\t\t\t\tthat require high-throughput local processing, S3 on Outposts provides on-premises\n\t\t\t\t\tobject storage to minimize data transfers and buffer from network variations,\n\t\t\t\t\twhile providing you the ability to easily transfer data between Outposts and\n\t\t\t\t\tAWS Regions. Accessing S3 on Outposts You can work with S3 on Outposts in any of the following ways: AWS Management Console The console is a web-based user interface for managing S3 on Outposts and AWS\n\t\t\t\tresources. If you've signed up for an AWS account, you can access S3 on Outposts by\n\t\t\t\tsigning into the AWS Management Console and choosing S3 from the AWS Management Console\n\t\t\t\thome page. Then, choose Outposts buckets from the left\n\t\t\t\tnavigation pane. AWS Command Line Interface You can use the AWS command line tools to issue commands or build scripts at\n\t\t\t\tyour system's command line to perform AWS (including S3) tasks. The AWS Command Line Interface (AWS CLI) provides commands\n\t\t\t\tfor a broad set of AWS services. The AWS CLI is supported on Windows, macOS, and\n\t\t\t\tLinux. To get started, see the AWS Command Line Interface User Guide . For more information about\n\t\t\t\tthe commands that you can use with S3 on Outposts, see s3api , s3control , and s3outposts in the AWS CLI Command Reference . AWS SDKs AWS provides SDKs (software development kits) that consist of libraries and\n\t\t\t\tsample code for various programming languages and platforms (Java, Python, Ruby,\n\t\t\t\t.NET, iOS, Android, and so on). The AWS SDKs provide a convenient way to create\n\t\t\t\tprogrammatic access to S3 on Outposts and AWS. Because S3 on Outposts uses the same\n\t\t\t\tSDKs as Amazon S3, S3 on Outposts provides a consistent experience using the same S3 APIs,\n\t\t\t\tautomation, and tools. S3 on Outposts is a REST service. You can send requests to S3 on Outposts by using the\n\t\t\t\tAWS SDK libraries, which wrap the underlying REST API and simplify your\n\t\t\t\tprogramming tasks. For example, the SDKs take care of tasks such as calculating\n\t\t\t\tsignatures, cryptographically signing requests, managing errors, and retrying\n\t\t\t\trequests automatically. For information about the AWS SDKs, including how to\n\t\t\t\tdownload and install them, see Tools to Build\n\t\t\t\t\ton AWS . Paying for S3 on Outposts You can purchase a variety of AWS Outposts rack configurations featuring a combination of\n\t\t\tAmazon EC2 instance types, Amazon EBS General Purpose solid state drive (SSD) volumes\n\t\t\t\t( gp2 ), and S3 on Outposts. Pricing includes delivery, installation,\n\t\t\tinfrastructure service maintenance, and software patches and upgrades. For more information, see AWS Outposts\n\t\t\t\track pricing . Next steps For more information about working with S3 on Outposts, see the following topics: Setting up your Outpost How is Amazon S3 on Outposts different from\n            Amazon S3? Getting started with Amazon S3 on Outposts Networking for S3 on Outposts Working with S3 on Outposts buckets Working with S3 on Outposts objects Security in S3 on Outposts Managing S3 on Outposts storage Developing with Amazon S3 on Outposts Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Setting up your Outpost"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/s3-outposts.pdf", "content": "No main content found."}, {"title": "Amazon Simple Storage Service Documentation", "url": "https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs#amazon-s3", "content": "No main content found."}, {"title": "Amazon Simple Storage Service Documentation", "url": "https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs#amazon-s3-glacier", "content": "No main content found."}, {"title": "Amazon Simple Storage Service Documentation", "url": "https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs#amazon-s3-on-outposts", "content": "No main content found."}, {"title": "Amazon Simple Storage Service Documentation", "url": "https://docs.aws.amazon.com/s3/?icmpid=docs_homepage_featuredsvcs#best-practices", "content": "No main content found."}, {"title": "AWS General Reference - AWS General Reference", "url": "http://docs.aws.amazon.com/general/latest/gr/Welcome.html", "content": "AWS General Reference PDF The AWS General Reference provides AWS service endpoint and quota information for Amazon Web Services. Additionally, you can find links to other common topics. Contents AWS security credentials AWS IP address ranges AWS APIs AWS services endpoints and quotas AWS Glossary AWS security credentials When you interact with AWS, you specify your AWS security\n      credentials to verify who you are and whether you have permission to access the\n      resources that you are requesting. AWS uses the security credentials to authenticate and\n      authorize your requests. For more information, see the following resources: AWS security credentials in the IAM User Guide AWS\n        security audit guidelines in the IAM User Guide AWS IP address ranges AWS publishes its current IP address ranges in JSON format. You can download\n      a .json file to view current ranges. The IP address ranges that you bring to AWS through bring your own IP addresses (BYOIP)\n      are not included in the .json file. For more information, see the following resources: AWS IP address ranges in the Amazon VPC User Guide AWS services that support IPv6 in the Amazon VPC User Guide AWS APIs The following pages provide information that is useful when using an AWS API: Retry behavior in the AWS SDKs and Tools Reference Guide Signing AWS API requests in the IAM User Guide AWS services endpoints and quotas You can learn about the endpoints and service quotas in the following pages: AWS service endpoints AWS service quotas Service endpoints and quotas Specifying which AWS Regions your account can use in the AWS Account Management Guide AWS Glossary For the latest AWS terminology, see the AWS Glossary . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions AWS service endpoints"}, {"title": "Amazon DynamoDB Documentation", "url": "https://docs.aws.amazon.com/dynamodb/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=DynamoDB&topic_url=https://docs.aws.amazon.com/dynamodb/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "What is Amazon DynamoDB? - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/index.html", "content": "What is Amazon DynamoDB? PDF RSS Amazon DynamoDB is a serverless, NoSQL, fully managed database with single-digit millisecond performance at any scale. DynamoDB addresses your needs to overcome scaling and operational complexities of relational databases. DynamoDB is purpose-built and optimized for operational\n        workloads that require consistent performance at any scale. For example, DynamoDB delivers consistent single-digit millisecond performance for a shopping cart \n        use case, whether you've 10 or 100 million users. Launched in 2012 , \n        DynamoDB continues to help you move away from relational databases while reducing cost and improving performance at scale. Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB scales to support tables of virtually any size while providing consistent single-digit millisecond performance and high availability. For events, such as Amazon Prime Day , DynamoDB powers multiple high-traffic Amazon properties and systems,\n        including Alexa , Amazon.com sites, and all Amazon fulfillment centers .\n        For such events, DynamoDB APIs have handled trillions of calls from Amazon properties and systems. DynamoDB continuously serves hundreds of customers with tables that have peak traffic of over half a million requests per second. It \n        also serves hundreds of customers whose table sizes exceed 200 TB, and processes over one billion requests per hour. Topics Characteristics of DynamoDB DynamoDB use cases Capabilities of DynamoDB Service integrations Security Resilience Accessing DynamoDB DynamoDB pricing Getting started with DynamoDB Characteristics of DynamoDB Serverless With DynamoDB, you don't need to provision any servers, or patch, manage, install, maintain, or operate any software. DynamoDB provides zero downtime maintenance. It has no versions (major, minor, or patch), and there are no maintenance windows. DynamoDB's on-demand capacity mode offers pay-as-you-go pricing for read and write requests so you only pay for what you use. With on-demand, DynamoDB instantly scales up or down your tables to adjust for \n                capacity and maintains performance with zero administration. It also scales down to zero so you don't pay for throughput when your table doesn't have traffic and there are no cold starts. NoSQL As a NoSQL database, DynamoDB is purpose-built to deliver improved performance, scalability, manageability, and flexibility compared to traditional relational databases. To support a wide variety of use cases, DynamoDB supports both key-value and document data models. Unlike relational databases, DynamoDB doesn't support a JOIN operator. We recommend that you denormalize your data model to reduce database round trips and processing power needed to answer queries. As a NoSQL database, DynamoDB provides strong read consistency and ACID transactions to build enterprise-grade applications. Fully managed As a fully managed database service, DynamoDB handles the undifferentiated heavy lifting of managing a database so that you can focus on building value for your customers. It handles setup, configurations, maintenance, high availability, hardware provisioning, security, backups,\n                monitoring, and more. This ensures that when you create a DynamoDB table, it's instantly ready for production workloads. DynamoDB constantly improves its availability, reliability, performance, security, and functionality without requiring upgrades or downtime. Single-digit millisecond performance at any scale DynamoDB was purpose-built to improve upon the performance and scalability of relational databases to deliver single-digit millisecond performance at any scale. To achieve this scale and performance, DynamoDB is optimized for high-performance workloads and provides APIs that encourage efficient \n                database usage. It omits features that are inefficient and non-performing at scale, for example, JOIN operations. DynamoDB delivers consistent single-digit millisecond performance for your application, whether you've 100 or 100 million users. DynamoDB use cases Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB is ideal for use cases that require consistent performance at any scale with little to zero operational overhead. The following list presents some use cases where you can use DynamoDB: Financial service applications \u2013 Suppose you're a financial services company building applications, such as live trading and routing, loan management, token generation, and transaction ledgers. With DynamoDB global tables , your applications can respond to events and serve traffic from your chosen AWS Regions with fast, local read and write performance. DynamoDB is suitable for applications with the most stringent availability requirements. It removes the operational burden of manually scaling instances for increased storage or throughput, versioning, and licensing. You can use DynamoDB transactions to achieve atomicity, consistency, isolation, and durability (ACID) across one or more tables with a single request. (ACID) transactions suit workloads that include processing financial transactions or fulfilling orders. DynamoDB instantly accommodates your workloads as they ramp up or down, enabling you to efficiently scale your database for market conditions, such as trading hours. Gaming applications \u2013 As a gaming company, you can use DynamoDB for all parts of game platforms, for example, game state, player data, session history, and leaderboards. Choose DynamoDB for its scale, consistent performance, and the ease of operations provided by its serverless architecture. DynamoDB is well suited for scale-out architectures needed to support successful games. It quickly scales your game\u2019s throughput both in and out (scale to zero with no cold start). This scalability optimizes your architecture's efficiency whether you\u2019re scaling out for peak traffic or scaling back when gameplay usage is low. Streaming applications \u2013 Media and entertainment companies use DynamoDB as a metadata index for content, content management service, or to serve near real-time sports statistics. They also use DynamoDB to run user watchlist and bookmarking services and process billions of daily customer events for generating recommendations. These customers benefit from DynamoDB's scalability, performance, and resiliency. DynamoDB scales to workload changes as they ramp up or down, enabling streaming media use cases that can support any levels of demand. To learn more about how customers from different industries use DynamoDB, see Amazon DynamoDB Customers and This is My Architecture . Capabilities of DynamoDB Multi-active replication with global tables Global tables provide multi-active replication of your data across your chosen AWS Regions with 99.999% availability . Global tables deliver a fully managed solution for deploying a multi-Region, multi-active database, without building and maintaining your own replication solution. With global tables, you can specify the AWS Regions where you want the tables to be available. DynamoDB replicates ongoing data changes to all of these tables. Your globally distributed applications can access data locally in your selected Regions to achieve single-digit millisecond read and write performance. Because global tables are multi-active, you don't need a primary table. This means there are no complicated or delayed fail-overs, or database downtime when failing over an application between Regions. ACID transactions DynamoDB is built for mission-critical workloads. It includes (ACID) transactions support for applications that require complex business logic. DynamoDB provides native, server-side support for transactions, simplifying the developer experience of making coordinated, all-or-nothing changes to multiple items within and across tables. Change data capture for event-driven architectures DynamoDB supports streaming of item-level change data capture (CDC) records in near-real time. It offers two streaming models for CDC: DynamoDB Streams and Kinesis Data Streams for DynamoDB . Whenever an application creates, updates, or deletes items in a table, streams records a time-ordered sequence of every item-level change in near-real time. This makes DynamoDB Streams ideal for applications with event-driven architecture to consume and act upon the changes. Secondary indexes DynamoDB offers the option to create both global and local secondary indexes , which let you query the table data using an alternate key. With these secondary indexes, you can access data with attributes other than the primary key, giving you maximum flexibility in accessing your data. Service integrations DynamoDB broadly integrates with several AWS services to help you get more value from your data, eliminate undifferentiated heavy lifting, and operate your workloads at scale. Some examples are: AWS CloudFormation, Amazon CloudWatch, Amazon S3, AWS Identity and Access Management (IAM), and AWS Auto Scaling. The following sections describe some of the service integrations that you can perform using DynamoDB: Serverless integrations To build end-to-end serverless applications, DynamoDB integrates natively with a number of serverless AWS services. For example, you can integrate DynamoDB with AWS Lambda to create triggers , which are pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build event-driven applications that react to data modifications in DynamoDB tables. \n                For cost optimization, you can filter events that Lambda processes from a DynamoDB stream. The following list presents some examples of serverless integrations with DynamoDB: AWS AppSync for creating GraphQL APIs Amazon API Gateway for creating REST APIs Lambda for serverless compute Amazon Kinesis Data Streams for change data capture (CDC) Importing and exporting data to Amazon S3 Integrating DynamoDB with Amazon S3 enables you to easily export data to an Amazon S3 bucket for analytics and machine learning. DynamoDB supports full table exports and incremental exports to export changed, updated, or deleted data \n                between a specified time period. You can also import data from Amazon S3 into a new DynamoDB table. Zero-ETL integration DynamoDB supports zero-ETL integration with Amazon Redshift and Amazon OpenSearch Service .\n                These integrations enable you to run complex analytics and use advanced search capabilities on your DynamoDB table data. For example, you can perform full-text and vector search, and semantic search on your DynamoDB data.\n                Zero-ETL integrations have no impact on production workloads running on DynamoDB. Caching DynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for DynamoDB. DAX delivers up to 10 times performance improvement \u2013 from milliseconds \n                to microseconds \u2013 even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring you to manage cache invalidation, data population, or cluster management. Security DynamoDB utilizes IAM to help you securely control access to your DynamoDB resources. With IAM, you can centrally \n            manage permissions that control which DynamoDB users can access resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. Because DynamoDB utilizes IAM, there are no user names or passwords for accessing DynamoDB.\n            Because you don't have any complicated password rotation policies to manage, it simplifies your security posture. With IAM, you can also enable fine-grained \n                access control to provide authorization at the attribute level. You can also define resource-based policies with support for IAM Access Analyzer and Block Public Access (BPA) to simplify policy management. By default, DynamoDB encrypts all customer data at rest. Encryption at rest enhances the security of your data by using encryption keys stored in AWS Key Management Service (AWS KMS).\n            With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements. When you access an encrypted table, DynamoDB decrypts the table data transparently. You don't have to change any code or applications to use or manage encrypted tables.\n            DynamoDB continues to deliver the same single-digit millisecond latency that you've come to expect, and all DynamoDB queries work seamlessly on your encrypted data. You can specify whether DynamoDB should use an AWS owned key (default encryption type), AWS managed key, or a Customer managed key to encrypt user data. The default encryption using AWS-owned KMS keys is available at no additional charge. For client-side encryption, you can use the AWS Database Encryption SDK . DynamoDB also adheres to several compliance standards , including HIPAA, PCI DSS, and GDPR, which enables you to meet regulatory requirements. Resilience By default, DynamoDB automatically replicates your data across three Availability Zones to provide high durability and a 99.99% availability SLA. DynamoDB also provides additional capabilities to help you achieve your business \n            continuity and disaster recovery objectives. DynamoDB includes the following features to help support your data resiliency and backup needs: Features Global tables Continuous backups and point-in-time recovery On-demand backup and restore Global tables DynamoDB global tables enable a 99.999% availability SLA and multi-Region resilience. This helps you build resilient applications and optimize them for the lowest recovery time objective (RTO) and recovery point objective (RPO). Global tables also\n                integrates with AWS Fault Injection Service (AWS FIS) to perform fault injection experiments on your global table workloads. For example, pausing global table replication to any replica table. Continuous backups and point-in-time recovery Continuous backups provide you per-second granularity and the ability to initiate a point-in-time recovery. With point-in-time recovery, you can restore a table to any point in time up to the second during the last 35 days. Continuous backups and initiating a point-in-time restore doesn't use provisioned capacity. They also don't have any impact on the performance or availability of your applications. On-demand backup and restore On-demand backup and restore let you create full backups of a table for long-term retention and archival for regulatory compliance needs. Backups don't impact the performance of your table and you can back up tables of any size. With AWS Backup \n                integration , you can use AWS Backup to schedule, copy, tag, and manage the life cycle of your DynamoDB on-demand backups automatically. Using AWS Backup, you can copy on-demand backups across accounts and Regions, and transition older backups to cold storage for cost-optimization. Accessing DynamoDB You can work with DynamoDB using the AWS Management Console , the AWS Command Line Interface , NoSQL Workbench for DynamoDB , or DynamoDB APIs . For more information, see Accessing DynamoDB . DynamoDB pricing DynamoDB charges for reading, writing, and storing data in your tables, along with any optional features you choose to enable. DynamoDB has two capacity modes with their respective billing options for processing reads and writes on your tables: on-demand and provisioned . DynamoDB also offers a free tier that provides 25 GB of storage. The free tier also includes 25 provisioned Write and 25 provisioned Read Capacity Units (WCU, RCU) which is enough to handle 200 M requests per month. For more information, see Amazon DynamoDB pricing . Getting started with DynamoDB If you're a first-time user of DynamoDB, we recommend that you begin by reading the following topics: Getting started with DynamoDB \u2013 Walks you through the process of setting up DynamoDB, creating sample tables, and uploading data. This topic also provides information about performing some basic database operations using the AWS Management Console, AWS CLI, NoSQL Workbench, and DynamoDB APIs. DynamoDB core components \u2013 Describes the basic DynamoDB concepts. Best practices for designing and architecting with\n      DynamoDB \u2013 Provides recommendations about  NoSQL design, DynamoDB Well-Architected Lens, table design and several other DynamoDB features. These best practices help you maximize performance and minimize throughput costs when working with DynamoDB. We also recommend that you review the following tutorials that present complete end-to-end procedures to familiarize yourself with DynamoDB. You can complete these tutorials with the free tier of AWS. Create and Query a NoSQL Table with Amazon DynamoDB Build an Application Using a NoSQL Key-Value Data Store For information about resources, tools, and strategies to migrate to DynamoDB, see Migrating to DynamoDB . To read the latest blogs and whitepapers, see Amazon DynamoDB resources . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Getting started with DynamoDB"}, {"title": "Welcome - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/index.html", "content": "Welcome PDF Amazon DynamoDB provides low-level API actions for managing database tables and\n        indexes, and for creating, reading, updating and deleting data. DynamoDB also\n        provides API actions for accessing and processing stream records. Note This API Reference describes the low-level API for Amazon DynamoDB. Instead of\n            making requests to the low-level API directly from your application, we recommend that\n            you use one of the AWS Software Development Kits (SDKs) for your\n            programming language. The AWS SDKs take care of request authentication,\n            serialization, and connection management. For more information, see Overview of AWS SDK\n                Support for DynamoDB in the Amazon DynamoDB Developer\n            Guide. At the end of each API action description there are links to the equivalent CLI command\n        and programming-specific language method. Similarly, at the end of each API datatype\n        description, there are links to the equivalent programming-specific language type. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "Cheat sheet for DynamoDB - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CheatSheet.html", "content": "Cheat sheet for DynamoDB PDF RSS This cheat sheet provides a quick reference for working with Amazon DynamoDB and its\n        various AWS SDKs. Initial setup Sign up\n                        for AWS . Get an\n                        AWS access key to access DynamoDB programmatically. Configure your DynamoDB credentials . See also: Setting up DynamoDB (web service) Getting started with DynamoDB Basic overview of core components SDK or CLI Choose your preferred SDK ,\n            or set up the AWS CLI . Note When you use the AWS CLI on Windows, a backslash (\\) that is not inside a quote is\n                treated as a carriage return. Also, you must escape any quotes and braces inside\n                other quotes. For an example, see the Windows tab in \"Create a\n                table\" in the next section. See also: AWS CLI with DynamoDB Getting started with DynamoDB - step 2 Basic actions This section provides code for basic DynamoDB tasks. For more information about these tasks, see Getting started with DynamoDB and the AWS SDKs . Create a table Default aws dynamodb create - table \\ --table-name Music \\ --attribute-definitions \\ AttributeName = Artist,AttributeType = S \\\n        AttributeName = SongTitle,AttributeType = S \\ --key-schema \\ AttributeName = Artist,KeyType = HASH \\\n        AttributeName = SongTitle,KeyType = RANGE \\ --provisioned-throughput \\ ReadCapacityUnits = 10 ,WriteCapacityUnits = 5 Windows aws dynamodb create-table ^ --table-name Music ^ --attribute-definitions ^ AttributeName=Artist,AttributeType=S ^ AttributeName=SongTitle,AttributeType=S ^ --key-schema ^ AttributeName=Artist,KeyType=HASH ^ AttributeName=SongTitle,KeyType=RANGE ^ --provisioned-throughput ^ ReadCapacityUnits= 10 ,WriteCapacityUnits= 5 anchor anchor Default Windows aws dynamodb create - table \\ --table-name Music \\ --attribute-definitions \\ AttributeName = Artist,AttributeType = S \\\n        AttributeName = SongTitle,AttributeType = S \\ --key-schema \\ AttributeName = Artist,KeyType = HASH \\\n        AttributeName = SongTitle,KeyType = RANGE \\ --provisioned-throughput \\ ReadCapacityUnits = 10 ,WriteCapacityUnits = 5 Write item to a table aws dynamodb put-item \\ --table-name Music \\ --item file: //item.json Read item from a table aws dynamodb get -item \\ --table-name Music \\ --item file: //item.json Delete item from a table aws dynamodb delete -item --table-name Music --key file: //key.json Query a table aws dynamodb query --table-name Music --key-condition-expression \"ArtistName=:Artist and SongName=:Songtitle\" Delete a table aws dynamodb delete - table --table-name Music List table names aws dynamodb list-tables Naming rules All names must be encoded using UTF-8 and are case sensitive. Table names and index names must be between 3 and 255 characters long, and can\n                    contain only the following characters: a-z A-Z 0-9 _ (underscore) - (dash) . (dot) Attribute names must be at least one character long, and less than 64 KB in\n                    size. For more information, see Naming\n                rules . Service quota basics Read and write units Read capacity unit (RCU) \u2013 One strongly\n                    consistent read per second, or two eventually consistent reads per second, for\n                    items up to 4 KB in size. Write capacity unit (WCU) \u2013 One write\n                    per second, for items up to 1 KB in size. Table limits Table size \u2013 There is no practical\n                    limit on table size. Tables are unconstrained in terms of the number of items or\n                    the number of bytes. Number of tables \u2013 For any AWS\n                    account, there is an initial quota of 2,500 tables per AWS Region. Page size limit for query and scan \u2013\n                    There is a limit of 1 MB per page, per query or scan. If your query parameters\n                    or scan operation on a table result in more than 1 MB of data, DynamoDB returns the\n                    initial matching items. It also returns a LastEvaluatedKey property\n                    that you can use in a new request to read the next page. Indexes Local secondary indexes (LSIs) \u2013 You\n                    can define a maximum of five local secondary indexes. LSIs are primarily useful\n                    when an index must have strong consistency with the base table. Global secondary indexes (GSIs) \u2013\n                    There is a default quota of 20 global secondary indexes per table. Projected secondary index attributes per\n                        table \u2013 You can project a total of up to 100 attributes\n                    into all of a table's local and global secondary indexes. This only applies to\n                    user-specified projected attributes. Partition keys The minimum length of a partition key value is 1 byte. The maximum length is\n                    2048 bytes. There is no practical limit on the number of distinct partition key values,\n                    for tables or for secondary indexes. The minimum length of a sort key value is 1 byte. The maximum length is 1024\n                    bytes. In general, there is no practical limit on the number of distinct sort key\n                    values per partition key value. The exception is for tables with secondary\n                    indexes. For more information on secondary indexes, partition key design, and sort key design,\n            see Best practices . Limits for commonly used data types String \u2013 The length of a string is\n                    constrained by the maximum item size of 400 KB. Strings are Unicode with UTF-8\n                    binary encoding. Number \u2013 A number can have up to 38\n                    digits of precision, and can be positive, negative, or zero. Binary \u2013 The length of a binary is\n                    constrained by the maximum item size of 400 KB. Applications that work with\n                    binary attributes must encode the data in base64 encoding before sending it to\n                    DynamoDB. For a full list of supported data types, see Data types .\n            For more information, also see Service\n                quotas . Items, attributes, and expression\n                    parameters The maximum item size in DynamoDB is 400 KB, which includes both attribute name\n                binary length (UTF-8 length) and attribute value binary lengths (UTF-8 length). The\n                attribute name counts towards the size limit. There is no limit on the number of values in a list, map, or set, as long as the\n                item that contains the values fits within the 400-KB item size limit. For expression parameters, the maximum length of any expression string is 4\n                KB. For more information about item size, attributes, and expression parameters, see Service quotas . More information Security Monitoring and logging Working with streams Backups and Point-in-time recovery Integrating with other AWS services API reference Architecture Center: Database Best Practices Video tutorials DynamoDB\n                    forum Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How it works Core components"}, {"title": "Programming Amazon DynamoDB with Python and Boto3 - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/programming-with-python.html", "content": "Programming Amazon DynamoDB with Python and Boto3 PDF RSS This guide provides an orientation to programmers wanting to use Amazon DynamoDB with\n        Python. Learn about the different abstraction layers, configuration management, error\n        handling, controlling retry policies, managing keep-alive, and more. Topics About Boto Using the Boto documentation Understanding the client and\n                resource abstraction layers Using the table resource\n                batch_writer Additional code examples that\n                explore the client and resource layers Understanding how the Client\n                and Resource objects interact with sessions and threads Customizing the Config object Error handling Logging Event hooks Pagination and the Paginator Waiters About Boto You can access DynamoDB from Python by using the official AWS SDK for Python, commonly\n            referred to as Boto3 . The name Boto (pronounced\n            boh-toh) comes from a freshwater dolphin native to the Amazon River. The Boto3 library\n            is the library\u2019s third major version, first released in 2015. The Boto3 library is quite\n            large, as it supports all AWS services, not just DynamoDB. This orientation targets only\n            the parts of Boto3 relevant to DynamoDB. Boto is maintained and published by AWS as open-source project hosted on GitHub.\n            It\u2019s split into two packages: Botocore and Boto3 . Botocore provides the low-level\n                    functionality. In Botocore you\u2019ll find the client, session, credentials, config,\n                    and exception classes. Boto3 builds on top of Botocore. It offers a\n                    higher-level, more Pythonic interface. Specifically, it exposes a DynamoDB table as\n                    a Resource and offers a simpler, more elegant interface compared to the\n                    lower-level, service-oriented client interface. Because these projects are hosted on GitHub, you can view the source code, track open\n            issues, or submit your own issues. Using the Boto documentation Get started with the Boto documentation with the following resources: Begin with the Quickstart section that provides a solid starting point for the\n                    package installation. Go there for instructions on getting Boto3 installed if\n                    it\u2019s not already (Boto3 is often automatically available within AWS services\n                    such as AWS Lambda). After that, focus on the documentation\u2019s DynamoDB guide . It shows you how to perform the basic DynamoDB\n                    activities: create and delete a table, manipulate items, run batch operations,\n                    run a query, and perform a scan. Its examples use the resource interface. When you see boto3.resource('dynamodb') that indicates you\u2019re using the\n                    higher-level resource interface. After the guide, you can review the DynamoDB reference . This landing page provides an exhaustive list of\n                    the classes and methods available to you. At the top, you\u2019ll see the DynamoDB.Client class. This provides low-level access to all\n                    the control-plane and data-plane operations. At the bottom, look at the DynamoDB.ServiceResource class. This is the higher-level\n                    Pythonic interface. With it you can create a table, do batch operations across\n                    tables, or obtain a DynamoDB.ServiceResource.Table instance for\n                    table-specific actions. Understanding the client and\n                resource abstraction layers The two interfaces you'll be working with are the client interface and the resource interface. The low-level client interface provides a\n                    1-to-1 mapping to the underlying service API. Every API offered by DynamoDB is\n                    available through the client. This means the client interface can provide\n                    complete functionality, but it's often more verbose and complex to use. The higher-level resource interface does not\n                    provide a 1-to-1 mapping of the underlying service API. However, it provides\n                    methods that make it more convenient for you to access the service such as batch_writer . Here\u2019s an example of inserting an item using the client interface. Notice how all\n            values are passed as a map with the key indicating their type ('S' for string, 'N' for\n            number) and their value as a string. This is known as DynamoDB JSON format. import boto3\n\ndynamodb = boto3.client( 'dynamodb' )\n\ndynamodb.put_item(\n    TableName= 'YourTableName' ,\n    Item= { 'pk' : { 'S' : 'id#1' }, 'sk' : { 'S' : 'cart#123' }, 'name' : { 'S' : 'SomeName' }, 'inventory' : { 'N' : '500' }, # ... more attributes ... }\n) Here's the same PutItem operation using the resource interface. The data\n            typing is implicit: import boto3\n\ndynamodb = boto3.resource( 'dynamodb' )\n\ntable = dynamodb.Table( 'YourTableName' )\n\ntable.put_item(\n    Item= { 'pk' : 'id#1' , 'sk' : 'cart#123' , 'name' : 'SomeName' , 'inventory' : 500 , # ... more attributes ... }\n) If needed, you can convert between regular JSON and DynamoDB JSON using the TypeSerializer and TypeDeserializer classes provided with\n            boto3: def dynamo_to_python ( dynamo_object: dict ) -> dict : deserializer = TypeDeserializer() return { k: deserializer.deserialize(v) for k, v in dynamo_object.items()\n    } def python_to_dynamo ( python_object: dict ) -> dict : serializer = TypeSerializer() return { k: serializer.serialize(v) for k, v in python_object.items()\n    } Here is how to perform a query using the client interface. It expresses the query as a\n            JSON construct. It uses a KeyConditionExpression string which requires\n            variable substitution to handle any potential keyword conflicts: import boto3\n\nclient = boto3.client( 'dynamodb' ) # Construct the query response = client.query(\n    TableName= 'YourTableName' ,\n    KeyConditionExpression= 'pk = :pk_val AND begins_with(sk, :sk_val)' ,\n    FilterExpression= '#name = :name_val' ,\n    ExpressionAttributeValues= { ':pk_val' : { 'S' : 'id#1' }, ':sk_val' : { 'S' : 'cart#' }, ':name_val' : { 'S' : 'SomeName' },\n    },\n    ExpressionAttributeNames= { '#name' : 'name' ,\n    }\n) The same query operation using the resource interface can be shortened and\n            simplified: import boto3 from boto3.dynamodb.conditions import Key, Attr\n\ndynamodb = boto3.resource( 'dynamodb' )\ntable = dynamodb.Table( 'YourTableName' )\n\nresponse = table.query(\n    KeyConditionExpression=Key( 'pk' ).eq( 'id#1' ) & Key( 'sk' ).begins_with( 'cart#' ),\n    FilterExpression=Attr( 'name' ).eq( 'SomeName' )\n) As a final example, imagine you want to get the approximate size of a table (which is\n            metadata kept on the table that is updated about every 6 hours). With the client\n            interface, you do a describe_table() operation and pull the answer from the\n            JSON structure returned: import boto3\n\ndynamodb = boto3.client( 'dynamodb' )\n\nresponse = dynamodb.describe_table(TableName= 'YourTableName' )\nsize = response[ 'Table' ][ 'TableSizeBytes' ] With the resource interface, the table performs the describe operation implicitly and\n            presents the data directly as an attribute: import boto3\n\ndynamodb = boto3.resource( 'dynamodb' )\n\ntable = dynamodb.Table( 'YourTableName' )\nsize = table.table_size_bytes Note When considering whether to develop using the client or resource interface, be\n                aware that new features will not be added to the resource interface per the resource documentation : \u201cThe AWS Python SDK team does not intend to\n                add new features to the resources interface in boto3. Existing interfaces will\n                continue to operate during boto3\u2019s lifecycle. Customers can find access to newer\n                service features through the client interface.\u201d Using the table resource\n                batch_writer One convenience available only with the higher-level table resource is the batch_writer . DynamoDB supports batch write operations allowing up to 25\n            put or delete operations in one network request. Batching like this improves efficiency\n            by minimizing network round trips. With the low-level client library, you use the client.batch_write_item() operation to run batches. You must manually split your work into batches of 25. After\n            each operation, you also have to request to receive a list of unprocessed items (some of\n            the write operations may succeed while others could fail). You then have to pass those\n            unprocessed items again into a later batch_write_item() operation. There's\n            a significant amount of boilerplate code. The Table.batch_writer method creates a context manager for writing objects in\n            a batch. It presents an interface where it seems as if you're writing items one at a\n            time, but internally it's buffering and sending the items in batches. It also handles\n            unprocessed item retries implicitly. dynamodb = boto3.resource( 'dynamodb' )\n\ntable = dynamodb.Table( 'YourTableName' )\n\nmovies = # long list of movies in { 'pk': 'val', 'sk': 'val', etc} format with table.batch_writer() as writer: for movie in movies:\n        writer.put_item(Item=movie) Additional code examples that\n                explore the client and resource layers You can also refer to the following code sample repositories that explore usage of the various functions, using both client and resource: Official\n                        AWS single-action code examples. Official AWS scenario-oriented code examples. Community-maintained single-action code examples. Understanding how the Client\n                and Resource objects interact with sessions and threads The Resource object is not thread safe and should not be shared across threads or\n            processes. Refer to the guide on Resource for more details. The Client object, in contrast, is generally thread safe, except for specific advanced\n            features. Refer to the guide on Clients for more details. The Session object is not thread safe. So, each time you make a Client or Resource in\n            a multi-threaded environment you should create a new Session first and then make the\n            Client or Resource from the Session. Refer to the guide on Sessions for more details. When you call the boto3.resource() , you\u2019re implicitly using the default\n            Session. This is convenient for writing single-threaded code. When writing\n            multi-threaded code, you\u2019ll want to first construct a new Session for each thread and\n            then retrieve the resource from that Session: # Explicitly create a new Session for this thread session = boto3.Session()\ndynamodb = session.resource( 'dynamodb' ) Customizing the Config object When constructing a Client or Resource object, you can pass optional named parameters\n            to customize behavior. The parameter named config unlocks a variety of\n            functionality. It\u2019s an instance of botocore.client.Config and the reference documentation for Config shows everything it exposes for you to\n            control. The guide to Configuration provides a good overview. Note You can modify many of these behavioral settings at the Session level, within the\n                AWS configuration file, or as environment variables. Config for timeouts One use of a custom config is to adjust networking behaviors: connect_timeout (float or int) \u2013 The time in\n                    seconds till a timeout exception is thrown when attempting to make a connection.\n                    The default is 60 seconds. read_timeout (float or int) \u2013 The time in\n                    seconds till a timeout exception is thrown when attempting to read from a\n                    connection. The default is 60 seconds. Timeouts of 60 seconds are excessive for DynamoDB. It means a transient network glitch\n            will cause a minute\u2019s delay for the client before it can try again. The following code\n            shortens the timeouts to a second: import boto3 from botocore.config import Config\n\nmy_config = Config(\n   connect_timeout = 1.0 ,\n   read_timeout = 1.0 )\ndynamodb = boto3.resource( 'dynamodb' , config=my_config) For more discussion about timeouts, see Tuning AWS Java SDK HTTP request settings for latency-aware DynamoDB\n                applications . Note the Java SDK has more timeout configurations than\n            Python. Config for keep-alive If you're using botocore 1.27.84 or later, you can also control TCP Keep-Alive : tcp_keepalive (bool) - Enables the TCP\n                    Keep-Alive socket option used when creating new connections if set to True ( defaults to False ). This is only available\n                    starting with botocore 1.27.84. Setting TCP Keep-Alive to True can reduce average latencies. Here's\n            sample code that conditionally sets TCP Keep-Alive to true when you have the right\n            botocore version: import botocore import boto3 from botocore.config import Config from distutils.version import LooseVersion\n\nrequired_version = \"1.27.84\" current_version = botocore.__version__\n\nmy_config = Config(\n   connect_timeout = 0.5 ,\n   read_timeout = 0.5 ) if LooseVersion(current_version) > LooseVersion(required_version):\n    my_config = my_config.merge(Config(tcp_keepalive = True ))\n\ndynamodb = boto3.resource( 'dynamodb' , config=my_config) Note TCP Keep-Alive is different than HTTP Keep-Alive. With TCP Keep-Alive, small\n                packets are sent by the underlying operating system over the socket connection to\n                keep the connection alive and immediately detect any drops. With HTTP Keep-Alive,\n                the web connection built on the underlying socket gets reused. HTTP Keep-Alive is\n                always enabled with boto3. There's a limit to how long an idle connection can be kept alive. Consider sending\n            periodic requests (say every minute) if you have an idle connection but want the next\n            request to use an already-established connection. Config for retries The config also accepts a dictionary called retries where you can specify your desired retry behavior. Retries happen within the SDK when\n            the SDK receives an error and the error is of a transient type. If an error is retried\n            internally (and a retry eventually produces a successful response), there's no error\n            seen from the calling code's perspective, just a slightly elevated latency. Here are the\n            values you can specify: max_attempts \u2013 An integer representing the\n                    maximum number of retry attempts that will be made on a single request. For\n                    example, setting this value to 2 will result in the request being retried at\n                    most two times after the initial request. Setting this value to 0 will result in\n                    no retries ever being attempted after the initial request. total_max_attempts \u2013 An integer representing\n                    the maximum number of total attempts that will be made on a single request. This\n                    includes the initial request, so a value of 1 indicates that no requests will be\n                    retried. If total_max_attempts and max_attempts are\n                    both provided, total_max_attempts takes precedence. total_max_attempts is preferred over max_attempts because it maps to the AWS_MAX_ATTEMPTS environment variable and\n                    the max_attempts config file value. mode \u2013 A string representing the type of\n                    retry mode botocore should use. Valid values are: legacy \u2013 The default mode. Waits 50ms\n                            the first retry, then uses exponential backoff with a base factor of 2.\n                            For DynamoDB, it performs up to 10 total max attempts (unless overridden\n                            with the above). Note With exponential backoff, the last attempt will wait almost 13\n                                seconds. standard \u2013 Named standard because\n                            it\u2019s more consistent with other AWS SDKs. Waits a random time from 0ms\n                            to 1,000ms for the first retry. If another retry is necessary, it picks\n                            another random time from 0ms to 1,000ms and multiplies it by 2. If an\n                            additional retry is necessary, it does the same random pick multiplied\n                            by 4, and so on. Each wait is capped at 20 seconds. This mode will\n                            perform retries on more detected failure conditions than the legacy mode. For DynamoDB, it performs up to 3 total max\n                            attempts (unless overridden with the above). adaptive - An experimental retry mode\n                            that includes all the functionality of standard mode but adds automatic\n                            client-side throttling. With adaptive rate limiting, SDKs can slow down\n                            the rate at which requests are sent to better accommodate the capacity\n                            of AWS services. This is a provisional mode whose behavior might\n                            change. An expanded definition of these retry modes can be found in the guide to retries as well as in the Retry behavior topic in the\n                SDK reference . Here\u2019s an example that explicitly uses the legacy retry policy with a\n            maximum of 3 total requests (2 retries): import boto3 from botocore.config import Config\n\nmy_config = Config(\n   connect_timeout = 1.0 ,\n   read_timeout = 1.0 ,\n   retries = { 'mode' : 'legacy' , 'total_max_attempts' : 3 }\n)\ndynamodb = boto3.resource( 'dynamodb' , config=my_config) Because DynamoDB is a highly-available, low-latency system, you may want to be more\n            aggressive with the speed of retries than the built-in retry policies allow. You can\n            implement your own retry policy by setting the max attempts to 0, catching the\n            exceptions yourself, and retrying as appropriate from your own code instead of relying\n            on boto3 to do implicit retries. If you manage your own retry policy, you'll want to differentiate between throttles\n            and errors: A throttle (indicated by a ProvisionedThroughputExceededException or ThrottlingException ) indicates a healthy service that's\n                    informing you that you've exceeded your read or write capacity on a DynamoDB table\n                    or partition. Every millisecond that passes, a bit more read or write capacity\n                    is made available, so you can retry quickly (such as every 50ms) to attempt to\n                    access that newly released capacity. With throttles, you don't especially need\n                    exponential backoff because throttles are lightweight for DynamoDB to return and\n                    incur no per-request charge to you. Exponential backoff assigns longer delays to\n                    client threads that have already waited the longest, which statistically extends\n                    the p50 and p99 outward. An error (indicated by an InternalServerError or a ServiceUnavailable , among\n                    others) indicates a transient issue with the service. This can be for the whole\n                    table or possibly just the partition you're reading from or writing to. With\n                    errors, you can pause longer before retries (such as 250ms or 500ms) and use\n                    jitter to stagger the retries. Config for max pool connections Lastly, the config lets you control the connection pool size: max_pool_connections (int) \u2013 The maximum\n                    number of connections to keep in a connection pool. If this value is not set,\n                    the default value of 10 is used. This option controls the maximum number of HTTP connections to keep pooled for reuse.\n            A different pool is kept per Session. If you anticipate more than 10 threads going\n            against clients or resources built off the same Session, you should consider raising\n            this, so threads don't have to wait on other threads using a pooled connection. import boto3 from botocore.config import Config\n\nmy_config = Config(\n   max_pool_connections = 20 ) # Setup a single session holding up to 20 pooled connections session = boto3.Session(my_config) # Create up to 20 resources against that session for handing to threads # Notice the single-threaded access to the Session and each Resource resource1 = session.resource( 'dynamodb' )\nresource2 = session.resource( 'dynamodb' ) # etc Error handling AWS service exceptions aren\u2019t all statically defined in Boto3. This is because\n            errors and exceptions from AWS services vary widely and are subject to change. Boto3\n            wraps all service exceptions as a ClientError and exposes the details as\n            structured JSON. For example, an error response might be structured like this: { 'Error': { 'Code': 'SomeServiceException',\n        'Message': 'Details/context around the exception or error'\n    },\n    'ResponseMetadata': { 'RequestId': ' 1234567890 ABCDEF',\n        'HostId': 'host ID data will appear here as a hash',\n        'HTTPStatusCode': 400 ,\n        'HTTPHeaders': { 'header metadata key/values will appear here'},\n        'RetryAttempts': 0 }\n} The following code catches any ClientError exceptions and looks at the\n            string value of the Code within the Error to determine what\n            action to take: import botocore\nimport boto3\n\ndynamodb = boto3.client('dynamodb')\n\ntry:\n    response = dynamodb.put_item(...)\n\nexcept botocore.exceptions.ClientError as err:\n    print('Error Code: { }'.format(err.response['Error']['Code']))\n    print('Error Message: { }'.format(err.response['Error']['Message']))\n    print('Http Code: { }'.format(err.response['ResponseMetadata']['HTTPStatusCode']))\n    print('Request ID: { }'.format(err.response['ResponseMetadata']['RequestId']))\n\n    if err.response['Error']['Code'] in ('ProvisionedThroughputExceededException', 'ThrottlingException'):\n        print( \"Received a throttle\" )\n    elif err.response['Error']['Code'] == 'InternalServerError':\n        print( \"Received a server error\" )\n    else:\n        raise err Some (but not all) exception codes have been materialized as top-level classes. You\n            can choose to handle these directly. When using the Client interface, these exceptions\n            are dynamically populated on your client and you catch these exceptions using your\n            client instance, like this: except ddb_client.exceptions.ProvisionedThroughputExceededException: When using the Resource interface, you have to use .meta.client to\n            traverse from the resource to the underlying Client to access the exceptions, like\n            this: except ddb_resource.meta.client.exceptions.ProvisionedThroughputExceededException: To review the list of materialized exception types, you can generate the list\n            dynamically: ddb = boto3.client( \"dynamodb\" ) print ([e for e in dir (ddb.exceptions) if e.endswith( 'Exception' ) or e.endswith( 'Error' )]) When doing a write operation with a condition expression, you can request that if the\n            expression fails the value of the item should be returned in the error response. try :\n    response = table.put_item(\n        Item=item,\n        ConditionExpression= 'attribute_not_exists(pk)' ,\n        ReturnValuesOnConditionCheckFailure= 'ALL_OLD' ) except table.meta.client.exceptions.ConditionalCheckFailedException as e: print ( 'Item already exists:' , e.response[ 'Item' ]) For further reading on error handling and exceptions: The boto3 guide on error handling has more information on error\n                    handling techniques. The DynamoDB developer guide section on\n                        programming errors lists what errors you might encounter. The Common Errors\n                        section in the API reference . The documentation on each API operation lists what errors that call might\n                    generate (for example BatchWriteItem ). Logging The boto3 library integrates with Python's built-in logging module for tracking what\n            happens during a session. To control logging levels, you can configure the logging\n            module: import logging\n\nlogging.basicConfig(level=logging.INFO) This configures the root logger to log INFO and above level messages.\n            Logging messages which are less severe than level will be ignored. Logging levels\n            include DEBUG , INFO , WARNING , ERROR ,\n            and CRITICAL . The default is WARNING . Loggers in boto3 are hierarchical. The library uses a few different loggers, each\n            corresponding to different parts of the library. You can separately control the behavior\n            of each: boto3 : The main logger for the boto3\n                    module. botocore : The main logger for the botocore\n                    package. botocore.auth : Used for logging AWS\n                    signature creation for requests. botocore.credentials : Used for logging the\n                    process of credential fetching and refresh. botocore.endpoint : Used for logging request\n                    creation before it's sent over the network. botocore.hooks : Used for logging events\n                    triggered in the library. botocore.loaders : Used for logging when parts\n                    of AWS service models are loaded. botocore.parsers : Used for logging AWS\n                    service responses before they're parsed. botocore.retryhandler : Used for logging the\n                    processing of AWS service request retries (legacy mode). botocore.retries.standard : Used for logging\n                    the processing of AWS service request retries (standard or adaptive\n                    mode). botocore.utils : Used for logging\n                    miscellaneous activities in the library. botocore.waiter : Used for logging the\n                    functionality of waiters, which poll an AWS service until a certain state is\n                    reached. Other libraries log as well. Internally, boto3 uses the third party urllib3 for HTTP\n            connection handling. When latency is important, you can watch its logs to ensure your\n            pool is being well utilized by seeing when urllib3 establishes a new connection or\n            closes an idle one down. urllib3.connectionpool: Use for logging\n                    connection pool handling events. The following code snippet sets most logging to INFO with DEBUG logging for endpoint and connection pool activity: import logging\n\nlogging.getLogger( 'boto3' ).setLevel(logging.INFO)\nlogging.getLogger( 'botocore' ).setLevel(logging.INFO)\nlogging.getLogger( 'botocore.endpoint' ).setLevel(logging.DEBUG)\nlogging.getLogger( 'urllib3.connectionpool' ).setLevel(logging.DEBUG) Event hooks Botocore emits events during various parts of its execution. You can register handlers\n            for these events so that whenever an event is emitted, your handler will be called. This\n            lets you extend the behavior of botocore without having to modify the internals. For instance, let's say you want to keep track of every time a PutItem operation is called on any DynamoDB table in your application. You might register on the 'provide-client-params.dynamodb.PutItem' event to catch and log every\n            time a PutItem operation is invoked on the associated Session. Here's an\n            example: import boto3 import botocore import logging def log_put_params ( params, **kwargs ): if 'TableName' in params and 'Item' in params:\n        logging.info( f\"PutItem on table { params[ 'TableName' ]} : { params[ 'Item' ]} \" )\n\nlogging.basicConfig(level=logging.INFO)\n\nsession = boto3.Session()\nevent_system = session.events # Register our interest in hooking in when the parameters are provided to PutItem event_system.register( 'provide-client-params.dynamodb.PutItem' , log_put_params) # Now, every time you use this session to put an item in DynamoDB, # it will log the table name and item data. dynamodb = session.resource( 'dynamodb' )\ntable = dynamodb.Table( 'YourTableName' )\ntable.put_item(\n    Item= { 'pk' : '123' , 'sk' : 'cart#123' , 'item_data' : 'YourItemData' , # ... more attributes ... }\n) Within the handler, you can even manipulate the params programmatically to change\n            behavior: params[ 'TableName' ] = \"NewTableName\" For more information on events, see the botocore documentation on events and the boto3 documentation on events . Pagination and the Paginator Some requests, such as Query and Scan, limit the size of data returned on a single\n            request and require you to make repeated requests to pull subsequent pages. You can control the maximum number of items to be read for each page with the limit parameter. For example, if you want the last 10 items, you can\n            use limit to retrieve only the last 10. Note the limit is how much should\n            be read from the table before any filtering is applied. There's no way to specify you\n            want exactly 10 after filtering; you can only control the pre-filtered count and check\n            client-side when you've actually retrieved 10. Regardless of the limit, every response\n            always has a maximum size of 1 MB. If the response includes a LastEvaluatedKey , it indicates the response\n            ended because it hit a count or size limit. The key is the last key evaluated for the\n            response. You can retrieve this LastEvaluatedKey and pass it to a follow-up\n            call as ExclusiveStartKey to read the next chunk from that starting point.\n            When there's no LastEvaluatedKey returned that, means there are no more\n            items matching the Query or Scan. Here's a simple example (using the Resource interface, but the Client interface has\n            the same pattern) that reads at most 100 items per page and loops until all items have\n            been read. import boto3\n\ndynamodb = boto3.resource( 'dynamodb' )\ntable = dynamodb.Table( 'YourTableName' )\n\nquery_params = { 'KeyConditionExpression' : Key( 'pk' ).eq( '123' ) & Key( 'sk' ).gt( 1000 ), 'Limit' : 100 } while True :\n    response = table.query(**query_params) # Process the items however you like for item in response[ 'Items' ]: print (item) # No LastEvaluatedKey means no more items to retrieve if 'LastEvaluatedKey' not in response: break # If there are possibly more items, update the start key for the next page query_params[ 'ExclusiveStartKey' ] = response[ 'LastEvaluatedKey' ] For convenience, boto3 can do this for you with Paginators. However, it only works\n            with the Client interface. Here's the code rewritten to use Paginators: import boto3\n\ndynamodb = boto3.client( 'dynamodb' )\n\npaginator = dynamodb.get_paginator( 'query' )\n\nquery_params = { 'TableName' : 'YourTableName' , 'KeyConditionExpression' : 'pk = :pk_val AND sk > :sk_val' , 'ExpressionAttributeValues' : { ':pk_val' : { 'S' : '123' }, ':sk_val' : { 'N' : '1000' },\n    }, 'Limit' : 100 }\n\npage_iterator = paginator.paginate(**query_params) for page in page_iterator: # Process the items however you like for item in page[ 'Items' ]: print (item) For more information, see the Guide on Paginators and the API reference for DynamoDB.Paginator.Query . Note Paginators also have their own configuration settings named MaxItems , StartingToken , and PageSize . For paginating with\n                DynamoDB, you should ignore these settings. Waiters Waiters provide the ability to wait for something to complete before proceeding. At\n            present, they only support waiting for a table to be created or deleted. In the\n            background, the waiter operation does a check for you every 20 seconds up to 25 times.\n            You could do this yourself, but using a waiter is elegant when writing\n            automation. This code shows how to wait for a particular table to have been created: # Create a table, wait until it exists, and print its ARN response = client.create_table(...)\nwaiter = client.get_waiter( 'table_exists' )\nwaiter.wait(TableName= 'YourTableName' ) print ( 'Table created:' , response[ 'TableDescription' ][ 'TableArn' ] For more information, see the Guide to Waiters and Reference on Waiters . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Low-level API Programming with JavaScript"}, {"title": "Programming DynamoDB with the AWS SDK for Java 2.x - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProgrammingWithJava.html", "content": "Programming\n            DynamoDB\n            with the AWS SDK for Java 2.x PDF RSS This\n        programming\n        guide provides an orientation for programmers who want to use Amazon DynamoDB\n        with Java. The guide covers different concepts including abstraction layers, configuration\n        management,\n        error\n        handling, controlling retry policies, and managing keep-alive. Topics About the AWS SDK for Java 2.x Getting\n                started SDK for Java 2.x\n                documentation Supported interfaces Additional\n                code examples Sync and async\n                programming HTTP clients Config Error handling AWS request ID Logging Pagination Data class annotations About the AWS SDK for Java 2.x You can access DynamoDB from Java using the official AWS SDK for Java. The SDK for Java has two\n            versions: 1.x and 2.x. The end-of-support for 1.x was announced on January 12, 2024. It will\n            enter\n            maintenance mode on July 31, 2024 and its\n            end-of-support is due on December 31, 2025. For new development, we highly recommend\n            that you use 2.x, which was first released in 2018. This guide exclusively targets 2.x\n            and focuses only on the parts of the SDK relevant to DynamoDB. For information about maintenance and support for the AWS SDKs, see AWS SDK and Tools\n                maintenance policy and AWS SDKs and Tools version\n                support matrix in the AWS SDKs and Tools Reference Guide . The AWS SDK for Java 2.x is a major rewrite of the 1.x code\n            base.\n            The SDK for Java 2.x supports modern Java\n            features, such as the non-blocking I/O introduced in Java 8. The SDK for Java 2.x also adds\n            support for pluggable HTTP client implementations to provide more\n            network\n            connection flexibility and configuration\n            options. A noticeable change between the SDK for Java 1.x and the SDK for Java 2.x is the use of a new\n            package name. The Java 1.x SDK uses the com.amazonaws package name,\n            while the Java 2.x SDK uses software.amazon.awssdk . Similarly, Maven\n            artifacts for the Java 1.x SDK use the com.amazonaws groupId , while Java 2.x SDK artifacts use the software.amazon.awssdk groupId . Important The AWS SDK for Java 1.x has a DynamoDB package named com.amazonaws.dynamodbv2 . The \"v2\" in the package name doesn't indicate\n                that it's for Java 2 (J2SE). Rather, \"v2\" indicates that the package supports the second version of the DynamoDB low-level API\n                instead of the original version of the\n                low-level API. Support for Java versions The AWS SDK for Java 2.x provides full support for long-term support (LTS) Java releases . Getting started with the\n                AWS SDK for Java 2.x The following tutorial shows you how to use Apache Maven for defining dependencies for the SDK for Java 2.x. This tutorial also\n            shows you how to write the code that connects to DynamoDB for listing the available DynamoDB\n            tables. The tutorial in this guide is based on the tutorial Get started with the\n                AWS SDK for Java 2.x in the AWS SDK for Java 2.x Developer Guide . We've edited\n            this tutorial to make calls to DynamoDB instead of Amazon S3. To\n            complete this tutorial, do the following: Step 1: Set up for this tutorial Step 2: Create the project Step 3: Write the code Step 4: Build and run the application Step 1: Set up for this tutorial Before\n                you begin this tutorial, you need the following: Permission to access DynamoDB. A Java development environment that's configured\n                        with\n                        single sign-on access to AWS services using the\n                        AWS access portal. To\n                set\n                up for this tutorial, follow the instructions in Setup\n                    overview in the AWS SDK for Java 2.x Developer Guide . After you configure\n                    your development environment with single sign-on access for the Java SDK\n                and you have an active\n                    AWS access portal session , then continue to Step 2 of this tutorial. Step 2: Create the project To create the project for this tutorial, you run a Maven command that prompts you\n                for input on how to configure the project. After all input is entered and confirmed,\n                Maven finishes building out the project by creating a pom.xml file and creating stub Java files. Open a terminal or command prompt window and navigate to a directory of\n                        your choice, for example, your Desktop or Home folder. Enter the following command at the terminal, and then press Enter . mvn archetype:generate \\\n   -DarchetypeGroupId=software.amazon.awssdk \\\n   -DarchetypeArtifactId=archetype-app-quickstart \\\n   -DarchetypeVersion=2.22.0 For each prompt, enter the value listed in the second column. Prompt Value to enter Define value for property 'service': dynamodb Define value for property\n                                        'httpClient' : apache-client Define value for property\n                                        'nativeImage' : false Define value for property\n                                            'credentialProvider' identity-center Define value for property 'groupId': org.example Define value for property\n                                        'artifactId': getstarted Define value for property 'version'\n                                            1.0-SNAPSHOT: <Enter> Define value for property 'package'\n                                            org.example: <Enter> After you enter the last value, Maven lists the choices that you made. To\n                        confirm, enter Y . Or, enter N , and then re-enter your choices. Maven creates a project folder named getstarted based on the artifactId value that you entered. Inside the getstarted folder, find a file named README.md that you can review, a pom.xml file, and a src directory. Maven builds the following directory tree. getstarted\n \u251c\u2500\u2500 README.md\n \u251c\u2500\u2500 pom.xml\n \u2514\u2500\u2500 src\n     \u251c\u2500\u2500 main\n     \u2502   \u251c\u2500\u2500 java\n     \u2502   \u2502   \u2514\u2500\u2500 org\n     \u2502   \u2502       \u2514\u2500\u2500 example\n     \u2502   \u2502           \u251c\u2500\u2500 App.java\n     \u2502   \u2502           \u251c\u2500\u2500 DependencyFactory.java\n     \u2502   \u2502           \u2514\u2500\u2500 Handler.java\n     \u2502   \u2514\u2500\u2500 resources\n     \u2502       \u2514\u2500\u2500 simplelogger.properties\n     \u2514\u2500\u2500 test\n         \u2514\u2500\u2500 java\n             \u2514\u2500\u2500 org\n                 \u2514\u2500\u2500 example\n                     \u2514\u2500\u2500 HandlerTest.java\n \n 10 directories, 7 files The following shows the contents of the pom.xml project\n                file. The dependencyManagement section contains a dependency to the\n                        AWS SDK for Java 2.x, and the dependencies section has a dependency for\n                        DynamoDB. Specifying these dependencies forces Maven to include the relevant .jar files in your Java class path. By default, the AWS SDK doesn't include all\n                        the classes for all AWS services. For\n                        DynamoDB,\n                        if you use the low-level interface, then you should have\n                        a dependency on the dynamodb artifact. Or, if you use the\n                        high-level interface, on the dynamodb-enhanced artifact. If you\n                        don't include the relevant dependencies, then your code\n                        can't\n                        compile. The project uses Java 1.8 because of the 1.8 value in\n                        the maven.compiler.source and maven.compiler.target properties. <?xml version=\"1.0\" encoding=\"UTF-8\"?> < project xmlns = \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation = \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > < modelVersion > 4.0.0 </ modelVersion > < groupId > org.example </ groupId > < artifactId > getstarted </ artifactId > < version > 1.0-SNAPSHOT </ version > < packaging > jar </ packaging > < properties > < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > < maven.compiler.source > 1.8 </ maven.compiler.source > < maven.compiler.target > 1.8 </ maven.compiler.target > < maven.shade.plugin.version > 3.2.1 </ maven.shade.plugin.version > < maven.compiler.plugin.version > 3.6.1 </ maven.compiler.plugin.version > < exec-maven-plugin.version > 1.6.0 </ exec-maven-plugin.version > < aws.java.sdk.version > 2.22.0 </ aws.java.sdk.version > <-------- SDK version picked up from archetype version . < slf4j.version > 1.7.28 </ slf4j.version > < junit5.version > 5.8.1 </ junit5.version > </ properties > < dependencyManagement > < dependencies > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > bom </ artifactId > < version > $ { aws.java.sdk.version} </ version > < type > pom </ type > < scope > import </ scope > </ dependency > </ dependencies > </ dependencyManagement > < dependencies > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > dynamodb </ artifactId > <-------- DynamoDB dependency < exclusions > < exclusion > < groupId > software.amazon.awssdk </ groupId > < artifactId > netty-nio-client </ artifactId > </ exclusion > < exclusion > < groupId > software.amazon.awssdk </ groupId > < artifactId > apache-client </ artifactId > </ exclusion > </ exclusions > </ dependency > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > sso </ artifactId > <-------- Required for identity center authentication. </ dependency > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > ssooidc </ artifactId > <-------- Required for identity center authentication. </ dependency > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > apache-client </ artifactId > <-------- HTTP client specified. < exclusions > < exclusion > < groupId > commons-logging </ groupId > < artifactId > commons-logging </ artifactId > </ exclusion > </ exclusions > </ dependency > < dependency > < groupId > org.slf4j </ groupId > < artifactId > slf4j-api </ artifactId > < version > $ { slf4j.version} </ version > </ dependency > < dependency > < groupId > org.slf4j </ groupId > < artifactId > slf4j-simple </ artifactId > < version > $ { slf4j.version} </ version > </ dependency > <!-- Needed to adapt Apache Commons Logging used by Apache HTTP Client to Slf4j to avoid\n         ClassNotFoundException: org.apache.commons.logging.impl.LogFactoryImpl during runtime --> < dependency > < groupId > org.slf4j </ groupId > < artifactId > jcl-over-slf4j </ artifactId > < version > $ { slf4j.version} </ version > </ dependency > <!-- Test Dependencies --> < dependency > < groupId > org.junit.jupiter </ groupId > < artifactId > junit-jupiter </ artifactId > < version > $ { junit5.version} </ version > < scope > test </ scope > </ dependency > </ dependencies > < build > < plugins > < plugin > < groupId > org.apache.maven.plugins </ groupId > < artifactId > maven-compiler-plugin </ artifactId > < version > $ { maven.compiler.plugin.version} </ version > </ plugin > </ plugins > </ build > </ project > pom.xml The dependencyManagement section contains a dependency to the\n                        AWS SDK for Java 2.x, and the dependencies section has a dependency for\n                        DynamoDB. Specifying these dependencies forces Maven to include the relevant .jar files in your Java class path. By default, the AWS SDK doesn't include all\n                        the classes for all AWS services. For\n                        DynamoDB,\n                        if you use the low-level interface, then you should have\n                        a dependency on the dynamodb artifact. Or, if you use the\n                        high-level interface, on the dynamodb-enhanced artifact. If you\n                        don't include the relevant dependencies, then your code\n                        can't\n                        compile. The project uses Java 1.8 because of the 1.8 value in\n                        the maven.compiler.source and maven.compiler.target properties. <?xml version=\"1.0\" encoding=\"UTF-8\"?> < project xmlns = \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation = \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > < modelVersion > 4.0.0 </ modelVersion > < groupId > org.example </ groupId > < artifactId > getstarted </ artifactId > < version > 1.0-SNAPSHOT </ version > < packaging > jar </ packaging > < properties > < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > < maven.compiler.source > 1.8 </ maven.compiler.source > < maven.compiler.target > 1.8 </ maven.compiler.target > < maven.shade.plugin.version > 3.2.1 </ maven.shade.plugin.version > < maven.compiler.plugin.version > 3.6.1 </ maven.compiler.plugin.version > < exec-maven-plugin.version > 1.6.0 </ exec-maven-plugin.version > < aws.java.sdk.version > 2.22.0 </ aws.java.sdk.version > <-------- SDK version picked up from archetype version . < slf4j.version > 1.7.28 </ slf4j.version > < junit5.version > 5.8.1 </ junit5.version > </ properties > < dependencyManagement > < dependencies > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > bom </ artifactId > < version > $ { aws.java.sdk.version} </ version > < type > pom </ type > < scope > import </ scope > </ dependency > </ dependencies > </ dependencyManagement > < dependencies > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > dynamodb </ artifactId > <-------- DynamoDB dependency < exclusions > < exclusion > < groupId > software.amazon.awssdk </ groupId > < artifactId > netty-nio-client </ artifactId > </ exclusion > < exclusion > < groupId > software.amazon.awssdk </ groupId > < artifactId > apache-client </ artifactId > </ exclusion > </ exclusions > </ dependency > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > sso </ artifactId > <-------- Required for identity center authentication. </ dependency > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > ssooidc </ artifactId > <-------- Required for identity center authentication. </ dependency > < dependency > < groupId > software.amazon.awssdk </ groupId > < artifactId > apache-client </ artifactId > <-------- HTTP client specified. < exclusions > < exclusion > < groupId > commons-logging </ groupId > < artifactId > commons-logging </ artifactId > </ exclusion > </ exclusions > </ dependency > < dependency > < groupId > org.slf4j </ groupId > < artifactId > slf4j-api </ artifactId > < version > $ { slf4j.version} </ version > </ dependency > < dependency > < groupId > org.slf4j </ groupId > < artifactId > slf4j-simple </ artifactId > < version > $ { slf4j.version} </ version > </ dependency > <!-- Needed to adapt Apache Commons Logging used by Apache HTTP Client to Slf4j to avoid\n         ClassNotFoundException: org.apache.commons.logging.impl.LogFactoryImpl during runtime --> < dependency > < groupId > org.slf4j </ groupId > < artifactId > jcl-over-slf4j </ artifactId > < version > $ { slf4j.version} </ version > </ dependency > <!-- Test Dependencies --> < dependency > < groupId > org.junit.jupiter </ groupId > < artifactId > junit-jupiter </ artifactId > < version > $ { junit5.version} </ version > < scope > test </ scope > </ dependency > </ dependencies > < build > < plugins > < plugin > < groupId > org.apache.maven.plugins </ groupId > < artifactId > maven-compiler-plugin </ artifactId > < version > $ { maven.compiler.plugin.version} </ version > </ plugin > </ plugins > </ build > </ project > Step 3: Write the code The following code shows the App class that Maven creates. The main method is the entry point into the application, which creates\n                an instance of the Handler class and then calls its sendRequest method. package org.example; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class App { private static final Logger logger = LoggerFactory.getLogger(App.class); public static void main (String... args) { logger.info( \"Application starts\" );\n \n         Handler handler = new Handler();\n         handler.sendRequest();\n \n         logger.info( \"Application ends\" );\n     }\n } App\n                            class package org.example; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class App { private static final Logger logger = LoggerFactory.getLogger(App.class); public static void main (String... args) { logger.info( \"Application starts\" );\n \n         Handler handler = new Handler();\n         handler.sendRequest();\n \n         logger.info( \"Application ends\" );\n     }\n } The DependencyFactory class\n                that\n                Maven creates contains the dynamoDbClient factory\n                method that builds and returns an DynamoDbClient instance. The DynamoDbClient instance uses an instance of the Apache-based HTTP\n                client. This is because you specified apache-client when Maven prompted\n                you for which HTTP client to use. The following code shows the DependencyFactory class. package org.example; import software.amazon.awssdk.http.apache.ApacheHttpClient; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; /**\n  * The module containing all dependencies required by the { @link Handler}.\n  */ public class DependencyFactory { private DependencyFactory () { } /**\n      * @return an instance of DynamoDbClient\n      */ public static DynamoDbClient dynamoDbClient () { return DynamoDbClient.builder()\n                        .httpClientBuilder(ApacheHttpClient.builder())\n                        .build();\n     }\n } DependencyFactory\n                            class package org.example; import software.amazon.awssdk.http.apache.ApacheHttpClient; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; /**\n  * The module containing all dependencies required by the { @link Handler}.\n  */ public class DependencyFactory { private DependencyFactory () { } /**\n      * @return an instance of DynamoDbClient\n      */ public static DynamoDbClient dynamoDbClient () { return DynamoDbClient.builder()\n                        .httpClientBuilder(ApacheHttpClient.builder())\n                        .build();\n     }\n } The Handler class contains the main logic of your program. When an\n                instance of Handler is created in the App class, the DependencyFactory furnishes the DynamoDbClient service\n                client. Your code uses the DynamoDbClient instance to call\n                DynamoDB. Maven generates the following Handler class with a TODO comment. The next step in the\n                tutorial replaces the TODO comment with\n                code. package org.example; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; public class Handler { private final DynamoDbClient dynamoDbClient; public Handler () { dynamoDbClient = DependencyFactory.dynamoDbClient();\n     } public void sendRequest () { // TODO: invoking the API calls using dynamoDbClient. }\n } Handler\n                            class, Maven-generated package org.example; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; public class Handler { private final DynamoDbClient dynamoDbClient; public Handler () { dynamoDbClient = DependencyFactory.dynamoDbClient();\n     } public void sendRequest () { // TODO: invoking the API calls using dynamoDbClient. }\n } To fill in the logic, replace the entire contents of the Handler class with the following code. The sendRequest method is filled in and\n                the necessary imports are added. The following code uses the DynamoDbClient instance to retrieve a list of\n                        existing tables. If tables exist for a given account and\n                        AWS Region,\n                        then the code uses the Logger instance to log the names of\n                        these tables. package org.example; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; import software.amazon.awssdk.services.dynamodb.model.ListTablesResponse; public class Handler { private final DynamoDbClient dynamoDbClient; public Handler () { dynamoDbClient = DependencyFactory.dynamoDbClient();\n     } public void sendRequest () { Logger logger = LoggerFactory.getLogger(Handler.class);\n \n         logger.info( \"calling the DynamoDB API to get a list of existing tables\" );\n         ListTablesResponse response = dynamoDbClient.listTables(); if (!response.hasTableNames()) { logger.info( \"No existing tables found for the configured account & region\" );\n         } else { response.tableNames().forEach(tableName -> logger.info( \"Table: \" + tableName));\n         }\n     }\n } Handler\n                            class, implemented The following code uses the DynamoDbClient instance to retrieve a list of\n                        existing tables. If tables exist for a given account and\n                        AWS Region,\n                        then the code uses the Logger instance to log the names of\n                        these tables. package org.example; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; import software.amazon.awssdk.services.dynamodb.model.ListTablesResponse; public class Handler { private final DynamoDbClient dynamoDbClient; public Handler () { dynamoDbClient = DependencyFactory.dynamoDbClient();\n     } public void sendRequest () { Logger logger = LoggerFactory.getLogger(Handler.class);\n \n         logger.info( \"calling the DynamoDB API to get a list of existing tables\" );\n         ListTablesResponse response = dynamoDbClient.listTables(); if (!response.hasTableNames()) { logger.info( \"No existing tables found for the configured account & region\" );\n         } else { response.tableNames().forEach(tableName -> logger.info( \"Table: \" + tableName));\n         }\n     }\n } Step 4: Build and run the application After you create the project and it contains the complete Handler class, build and run the application. Make sure that you have an active AWS IAM Identity Center session. To confirm, run the\n                        AWS Command Line Interface (AWS CLI) command aws sts get-caller-identity and check\n                        the response. If you don't have an active session, then see Sign in using the AWS CLI for instructions. Open a terminal or command prompt window and navigate to your project\n                        directory getstarted . To build your project, run the following command: mvn clean package To run the application, run the following command: mvn exec :java -Dexec.mainClass= \"org.example.App\" After you view the file, delete the object, and then\n                delete\n                the bucket. Success If your Maven project built and ran without error, then congratulations!\n                    You've successfully built your first Java application using the SDK for Java 2.x. Cleanup To clean up the resources that you created during this tutorial, delete the\n                    project folder getstarted . Reviewing the AWS SDK for Java 2.x\n                documentation The AWS SDK for Java 2.x Developer Guide covers all aspects of the SDK across all AWS services.\n            We\n            recommend that you review the following topics: Migrate from\n                        version 1.x to 2.x \u2013\n                    Includes a detailed explanation of the differences between 1.x and 2.x. This\n                    topic also contains instructions about how to use both major versions\n                    side-by-side. DynamoDB guide\n                        for Java 2.x SDK \u2013 Shows you how to perform basic DynamoDB\n                    operations: creating a table, manipulating items, and retrieving items. These\n                    examples use the low-level interface. Java has several interfaces, as explained\n                    in the following section: Supported interfaces . Tip After you review these topics, bookmark the AWS SDK for Java 2.x API Reference . It\n                covers all AWS services, and\n                we\n                recommend that you use it as your main API reference. Supported interfaces The AWS SDK for Java 2.x supports the following\n            interfaces,\n            depending on the level of abstraction that you want. Topics in this section Low-level interface High-level interface Document interface Comparing\n                    interfaces with a\n                        Query\n                    example Low-level interface The low-level interface provides a one-to-one mapping to the underlying service\n                API. Every DynamoDB API is available through this interface. This means that the\n                low-level interface can provide complete functionality, but it's often more verbose\n                and complex to use. For example, you have to use the .s() functions to\n                hold strings and the .n() functions to hold numbers. The following\n                example of PutItem inserts an item using the low-level interface. import org.slf4j.*; import software.amazon.awssdk.http.crt.AwsCrtHttpClient; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; import software.amazon.awssdk.services.dynamodb.model.*; import java.util.Map; public class PutItem { // Create a DynamoDB client with the default settings connected to the DynamoDB // endpoint in the default region based on the default credentials provider chain. private static final DynamoDbClient DYNAMODB_CLIENT = DynamoDbClient.create(); private static final Logger LOGGER = LoggerFactory.getLogger(PutItem.class); private void putItem () { PutItemResponse response = DYNAMODB_CLIENT.putItem(PutItemRequest.builder()\n                .item(Map.of( \"pk\" , AttributeValue.builder().s( \"123\" ).build(), \"sk\" , AttributeValue.builder().s( \"cart#123\" ).build(), \"item_data\" , AttributeValue.builder().s( \"YourItemData\" ).build(), \"inventory\" , AttributeValue.builder().n( \"500\" ).build() // ... more attributes ... ))\n                .returnConsumedCapacity(ReturnConsumedCapacity.TOTAL)\n                .tableName( \"YourTableName\" )\n                .build());\n        LOGGER.info( \"PutItem call consumed [\" + response.consumedCapacity().capacityUnits() + \"] Write Capacity Unites (WCU)\" );\n    }\n} High-level interface The high-level interface in the AWS SDK for Java 2.x is called the DynamoDB enhanced client.\n                This interface provides a more idiomatic code authoring experience. The enhanced client offers a way to map between client-side data classes and DynamoDB\n                tables designed to store that data. You define the relationships between tables and\n                their corresponding model classes in your code. Then, you can rely on the SDK to\n                manage the data type manipulation. For more information about the enhanced client,\n                see DynamoDB\n                    enhanced client API in the AWS SDK for Java 2.x Developer Guide . The following example of PutItem uses\n                the high-level interface. In this example,\n                the DynamoDbBean named YourItem creates\n                a TableSchema that\n                enables its direct use as input for the putItem() call. import org.slf4j.*; import software.amazon.awssdk.enhanced.dynamodb.*; import software.amazon.awssdk.enhanced.dynamodb.mapper.annotations.*; import software.amazon.awssdk.enhanced.dynamodb.model.*; import software.amazon.awssdk.services.dynamodb.model.ReturnConsumedCapacity; public class DynamoDbEnhancedClientPutItem { private static final DynamoDbEnhancedClient ENHANCED_DYNAMODB_CLIENT = DynamoDbEnhancedClient.builder().build(); private static final DynamoDbTable<YourItem> DYNAMODB_TABLE = ENHANCED_DYNAMODB_CLIENT.table( \"YourTableName\" , TableSchema.fromBean(YourItem.class)); private static final Logger LOGGER = LoggerFactory.getLogger(PutItem.class); private void putItem () { PutItemEnhancedResponse<YourItem> response = DYNAMODB_TABLE.putItemWithResponse(PutItemEnhancedRequest.builder(YourItem.class)\n                .item( new YourItem( \"123\" , \"cart#123\" , \"YourItemData\" , 500 ))\n                .returnConsumedCapacity(ReturnConsumedCapacity.TOTAL)\n                .build());\n        LOGGER.info( \"PutItem call consumed [\" + response.consumedCapacity().capacityUnits() + \"] Write Capacity Unites (WCU)\" );\n    } @DynamoDbBean public static class YourItem { public YourItem () { } public YourItem (String pk, String sk, String itemData, int inventory) { this .pk = pk; this .sk = sk; this .itemData = itemData; this .inventory = inventory;\n        } private String pk; private String sk; private String itemData; private int inventory; @DynamoDbPartitionKey public void setPk (String pk) { this .pk = pk;\n        } public String getPk () { return pk;\n        } @DynamoDbSortKey public void setSk (String sk) { this .sk = sk;\n        } public String getSk () { return sk;\n        } public void setItemData (String itemData) { this .itemData = itemData;\n        } public String getItemData () { return itemData;\n        } public void setInventory ( int inventory) { this .inventory = inventory;\n        } public int getInventory () { return inventory;\n        }\n    }\n} The AWS SDK for Java 1.x has its own high-level interface, which is often referred to by\n                its main class DynamoDBMapper . The AWS SDK for Java 2.x is published in a\n                separate package (and Maven artifact) named software.amazon.awssdk.enhanced.dynamodb . The Java 2.x SDK is\n                often referred to by its main class DynamoDbEnhancedClient . High-level interface\n                        using immutable data classes The mapping feature of the DynamoDB enhanced client API also works with immutable\n                    data classes. An immutable class has only getters and requires a builder class\n                    that the SDK uses to create instances of the class. Immutability in Java is a\n                    commonly used style that\n                    developers\n                    can use to create classes that have\n                    no\n                    side-effects. These classes are more predictable in their\n                    behavior in complex multi-threaded applications. Instead of using the @DynamoDbBean annotation as shown in the High-level interface example , immutable classes use\n                    the @DynamoDbImmutable annotation, which takes the builder class as\n                    its input. The following example takes the builder class DynamoDbEnhancedClientImmutablePutItem as input to create a\n                    table schema. The example then provides the schema as input for the PutItem API call. import org.slf4j.*; import software.amazon.awssdk.enhanced.dynamodb.*; import software.amazon.awssdk.enhanced.dynamodb.model.*; import software.amazon.awssdk.services.dynamodb.model.ReturnConsumedCapacity; public class DynamoDbEnhancedClientImmutablePutItem { private static final DynamoDbEnhancedClient ENHANCED_DYNAMODB_CLIENT = DynamoDbEnhancedClient.builder().build(); private static final DynamoDbTable<YourImmutableItem> DYNAMODB_TABLE = ENHANCED_DYNAMODB_CLIENT.table( \"YourTableName\" , TableSchema.fromImmutableClass(YourImmutableItem.class)); private static final Logger LOGGER = LoggerFactory.getLogger(DynamoDbEnhancedClientImmutablePutItem.class); private void putItem () { PutItemEnhancedResponse<YourImmutableItem> response = DYNAMODB_TABLE.putItemWithResponse(PutItemEnhancedRequest.builder(YourImmutableItem.class)\n                .item(YourImmutableItem.builder()\n                                        .pk( \"123\" )\n                                        .sk( \"cart#123\" )\n                                        .itemData( \"YourItemData\" )\n                                        .inventory( 500 )\n                                        .build())\n                .returnConsumedCapacity(ReturnConsumedCapacity.TOTAL)\n                .build());\n        LOGGER.info( \"PutItem call consumed [\" + response.consumedCapacity().capacityUnits() + \"] Write Capacity Unites (WCU)\" );\n    }\n} The following example shows the immutable data class. @DynamoDbImmutable(builder = YourImmutableItem.YourImmutableItemBuilder.class) class YourImmutableItem { private final String pk; private final String sk; private final String itemData; private final int inventory; public YourImmutableItem (YourImmutableItemBuilder builder) { this .pk = builder.pk; this .sk = builder.sk; this .itemData = builder.itemData; this .inventory = builder.inventory;\n    } public static YourImmutableItemBuilder builder () { return new YourImmutableItemBuilder(); } @DynamoDbPartitionKey public String getPk () { return pk;\n    } @DynamoDbSortKey public String getSk () { return sk;\n    } public String getItemData () { return itemData;\n    } public int getInventory () { return inventory;\n    } static final class YourImmutableItemBuilder { private String pk; private String sk; private String itemData; private int inventory; private YourImmutableItemBuilder () { } public YourImmutableItemBuilder pk (String pk) { this .pk = pk; return this ; } public YourImmutableItemBuilder sk (String sk) { this .sk = sk; return this ; } public YourImmutableItemBuilder itemData (String itemData) { this .itemData = itemData; return this ; } public YourImmutableItemBuilder inventory ( int inventory) { this .inventory = inventory; return this ; } public YourImmutableItem build () { return new YourImmutableItem( this ); }\n    }\n} High-level\n                        interface using immutable data classes and third-party boilerplate\n                        generation libraries Immutable data classes (shown in the previous example) require some\n                    boilerplate code. For example, the getter and setter logic on the data classes,\n                    in addition to the Builder classes. Third-party libraries, such as Project Lombok , can help you\n                    generate that type of boilerplate code. Reducing most of the boilerplate code\n                    helps you limit the amount of code needed for working with immutable data\n                    classes and the AWS SDK. This further results in improved productivity and\n                    readability of your code. For more information, see Use third-party libraries, such as Lombok in the AWS SDK for Java 2.x Developer Guide . The following example demonstrates how Project Lombok simplifies the code\n                    needed to use the DynamoDB enhanced client API. import org.slf4j.*; import software.amazon.awssdk.enhanced.dynamodb.*; import software.amazon.awssdk.enhanced.dynamodb.model.*; import software.amazon.awssdk.services.dynamodb.model.ReturnConsumedCapacity; public class DynamoDbEnhancedClientImmutableLombokPutItem { private static final DynamoDbEnhancedClient ENHANCED_DYNAMODB_CLIENT = DynamoDbEnhancedClient.builder().build(); private static final DynamoDbTable<YourImmutableLombokItem> DYNAMODB_TABLE = ENHANCED_DYNAMODB_CLIENT.table( \"YourTableName\" , TableSchema.fromImmutableClass(YourImmutableLombokItem.class)); private static final Logger LOGGER = LoggerFactory.getLogger(DynamoDbEnhancedClientImmutableLombokPutItem.class); private void putItem () { PutItemEnhancedResponse<YourImmutableLombokItem> response = DYNAMODB_TABLE.putItemWithResponse(PutItemEnhancedRequest.builder(YourImmutableLombokItem.class)\n                .item(YourImmutableLombokItem.builder()\n                        .pk( \"123\" )\n                        .sk( \"cart#123\" )\n                        .itemData( \"YourItemData\" )\n                        .inventory( 500 )\n                        .build())\n                .returnConsumedCapacity(ReturnConsumedCapacity.TOTAL)\n                .build());\n        LOGGER.info( \"PutItem call consumed [\" + response.consumedCapacity().capacityUnits() + \"] Write Capacity Unites (WCU)\" );\n    }\n} The following example shows the immutable data object of the immutable data\n                    class. import lombok.*; import software.amazon.awssdk.enhanced.dynamodb.mapper.annotations.*; @Builder @DynamoDbImmutable(builder = YourImmutableLombokItem.YourImmutableLombokItemBuilder.class) @Value public class YourImmutableLombokItem { @Getter(onMethod_=@DynamoDbPartitionKey) String pk; @Getter(onMethod_=@DynamoDbSortKey) String sk;\n    String itemData; int inventory;\n} The YourImmutableLombokItem class uses the following annotations\n                    that Project Lombok and the AWS SDK provide: @Builder \u2013 Produces complex builder APIs for data\n                            classes that Project Lombok provides. @DynamoDbImmutable \u2013 Identifies the DynamoDbImmutable class as a DynamoDB mappable entity\n                            annotation that the AWS SDK provides. @Value \u2013 The immutable variant of @Data . By default, all\n                            fields are made private and final, and setters are not generated.\n                            Project Lombok provides this annotation. Document interface The AWS SDK for Java 2.x Document interface avoids the need to specify data type\n                descriptors. The data types are implied by the semantics of the data itself. This\n                Document interface is similar to the AWS SDK for Java 1.x, Document interface, but with a\n                redesigned interface. The following Document interface example shows\n                the PutItem call expressed using the Document interface. The example\n                also uses EnhancedDocument. To\n                run\n                commands against a DynamoDB table using the enhanced document API, you must first\n                associate the table with your document table schema to create a DynamoDBTable resource object. The Document table schema builder\n                requires the primary index key and attribute converter providers. You can use AttributeConverterProvider.defaultProvider() to convert\n                document attributes of default types. You can change the overall default behavior\n                with a custom AttributeConverterProvider implementation. You can also\n                change the converter for a single attribute. The AWS SDKs and Tools Reference Guide provides more details and examples about how to\n                use custom converters. Their primary use is for attributes of your domain classes\n                that don't have a default converter available. Using a custom converter, you can\n                provide the SDK with the needed information to write or read to DynamoDB. import org.slf4j.*; import software.amazon.awssdk.enhanced.dynamodb.*; import software.amazon.awssdk.enhanced.dynamodb.document.EnhancedDocument; import software.amazon.awssdk.enhanced.dynamodb.model.*; import software.amazon.awssdk.services.dynamodb.model.ReturnConsumedCapacity; public class DynamoDbEnhancedDocumentClientPutItem { private static final DynamoDbEnhancedClient ENHANCED_DYNAMODB_CLIENT = DynamoDbEnhancedClient.builder().build(); private static final DynamoDbTable<EnhancedDocument> DYNAMODB_TABLE =\n            ENHANCED_DYNAMODB_CLIENT.table( \"YourTableName\" , TableSchema.documentSchemaBuilder()\n                            .addIndexPartitionKey(TableMetadata.primaryIndexName(), \"pk\" , AttributeValueType.S)\n                            .addIndexSortKey(TableMetadata.primaryIndexName(), \"sk\" , AttributeValueType.S)\n                            .attributeConverterProviders(AttributeConverterProvider.defaultProvider())\n                            .build()); private static final Logger LOGGER = LoggerFactory.getLogger(DynamoDbEnhancedDocumentClientPutItem.class); private void putItem () { PutItemEnhancedResponse<EnhancedDocument> response = DYNAMODB_TABLE.putItemWithResponse(\n                        PutItemEnhancedRequest.builder(EnhancedDocument.class)\n                                .item(\n                                    EnhancedDocument.builder()\n                                            .attributeConverterProviders(AttributeConverterProvider.defaultProvider())\n                                            .putString( \"pk\" , \"123\" )\n                                            .putString( \"sk\" , \"cart#123\" )\n                                            .putString( \"item_data\" , \"YourItemData\" )\n                                            .putNumber( \"inventory\" , 500 )\n                                            .build())\n                                .returnConsumedCapacity(ReturnConsumedCapacity.TOTAL)\n                                .build());\n        LOGGER.info( \"PutItem call consumed [\" + response.consumedCapacity().capacityUnits() + \"] Write Capacity Unites (WCU)\" );\n    }\n\n} To convert JSON documents to and from the native Amazon DynamoDB data types, you can use\n                the following utility methods: EnhancedDocument.fromJson(String json) \u2013 Creates a new EnhancedDocument instance from a JSON string. EnhancedDocument.toJson() \u2013 Creates a\n                        JSON string representation of the document that you can use in your\n                        application like any other JSON object. Comparing\n                    interfaces with a Query example This section shows the same Query call expressed using the various interfaces. To fine\n                tune the results of these queries,\n                note\n                the following: DynamoDB targets one specific partition key value, so you must specify the\n                        partition key completely. To have the query target only cart items, the sort key has a key condition\n                        expression that uses begins_with . We use limit() to limit the query to a maximum of 100\n                        returned items. We set the scanIndexForward to false. The results are\n                        returned in order of UTF-8 bytes, which usually means the cart item with the\n                        lowest number is returned first. By setting the scanIndexForward to\n                        false,\n                        we reverse the order and the cart item with the highest number is returned\n                        first. We apply a filter to remove any result that does not match the criteria.\n                        The data being filtered consumes read capacity\n                        whether\n                        the item matches the filter. Example Query using the low-level interface The following example queries a table named YourTableName using a keyConditionExpression . This limits the query to a specific\n                    partition key value and sort key value that begin with a specific prefix value.\n                    These key conditions limit the amount of data read from DynamoDB. Finally, the\n                    query applies a filter on the data retrieved from DynamoDB using a filterExpression . import org.slf4j.*; import software.amazon.awssdk.services.dynamodb.DynamoDbClient; import software.amazon.awssdk.services.dynamodb.model.*; import java.util.Map; public class Query { // Create a DynamoDB client with the default settings connected to the DynamoDB // endpoint in the default region based on the default credentials provider chain. private static final DynamoDbClient DYNAMODB_CLIENT = DynamoDbClient.builder().build(); private static final Logger LOGGER = LoggerFactory.getLogger(Query.class); private static void query () { QueryResponse response = DYNAMODB_CLIENT.query(QueryRequest.builder()\n                .expressionAttributeNames(Map.of( \"#name\" , \"name\" ))\n                .expressionAttributeValues(Map.of( \":pk_val\" , AttributeValue.fromS( \"id#1\" ), \":sk_val\" , AttributeValue.fromS( \"cart#\" ), \":name_val\" , AttributeValue.fromS( \"SomeName\" )))\n                .filterExpression( \"#name = :name_val\" )\n                .keyConditionExpression( \"pk = :pk_val AND begins_with(sk, :sk_val)\" )\n                .limit( 100 )\n                .scanIndexForward( false )\n                .tableName( \"YourTableName\" )\n                .build());\n\n        LOGGER.info( \"nr of items: \" + response.count());\n        LOGGER.info( \"First item pk: \" + response.items().get( 0 ).get( \"pk\" ));\n        LOGGER.info( \"First item sk: \" + response.items().get( 0 ).get( \"sk\" ));\n    }\n} Example Query using the Document interface The following example queries a table named YourTableName using\n                    the Document interface. import org.slf4j.Logger; import org.slf4j.LoggerFactory; import software.amazon.awssdk.enhanced.dynamodb.*; import software.amazon.awssdk.enhanced.dynamodb.document.EnhancedDocument; import software.amazon.awssdk.enhanced.dynamodb.model.*; import java.util.Map; public class DynamoDbEnhancedDocumentClientQuery { // Create a DynamoDB client with the default settings connected to the DynamoDB // endpoint in the default region based on the default credentials provider chain. private static final DynamoDbEnhancedClient ENHANCED_DYNAMODB_CLIENT = DynamoDbEnhancedClient.builder().build(); private static final DynamoDbTable<EnhancedDocument> DYNAMODB_TABLE =\n            ENHANCED_DYNAMODB_CLIENT.table( \"YourTableName\" , TableSchema.documentSchemaBuilder()\n                    .addIndexPartitionKey(TableMetadata.primaryIndexName(), \"pk\" , AttributeValueType.S)\n                    .addIndexSortKey(TableMetadata.primaryIndexName(), \"sk\" , AttributeValueType.S)\n                    .attributeConverterProviders(AttributeConverterProvider.defaultProvider())\n                    .build()); private static final Logger LOGGER = LoggerFactory.getLogger(DynamoDbEnhancedDocumentClientQuery.class); private void query () { PageIterable<EnhancedDocument> response = DYNAMODB_TABLE.query(QueryEnhancedRequest.builder()\n                .filterExpression(Expression.builder()\n                        .expression( \"#name = :name_val\" )\n                        .expressionNames(Map.of( \"#name\" , \"name\" ))\n                        .expressionValues(Map.of( \":name_val\" , AttributeValue.fromS( \"SomeName\" )))\n                        .build())\n                .limit( 100 )\n                .queryConditional(QueryConditional.sortBeginsWith(Key.builder()\n                        .partitionValue( \"id#1\" )\n                        .sortValue( \"cart#\" )\n                        .build()))\n                .scanIndexForward( false )\n                .build());\n\n        LOGGER.info( \"nr of items: \" + response.items().stream().count());\n        LOGGER.info( \"First item pk: \" + response.items().iterator().next().getString( \"pk\" ));\n        LOGGER.info( \"First item sk: \" + response.items().iterator().next().getString( \"sk\" ));\n\n    }\n} Example Query using the high-level interface The following example queries a table named YourTableName using\n                    the DynamoDB enhanced client API. import org.slf4j.*; import software.amazon.awssdk.enhanced.dynamodb.*; import software.amazon.awssdk.enhanced.dynamodb.mapper.annotations.*; import software.amazon.awssdk.enhanced.dynamodb.model.*; import software.amazon.awssdk.services.dynamodb.model.AttributeValue; import java.util.Map; public class DynamoDbEnhancedClientQuery { private static final DynamoDbEnhancedClient ENHANCED_DYNAMODB_CLIENT = DynamoDbEnhancedClient.builder().build(); private static final DynamoDbTable<YourItem> DYNAMODB_TABLE = ENHANCED_DYNAMODB_CLIENT.table( \"YourTableName\" , TableSchema.fromBean(DynamoDbEnhancedClientQuery.YourItem.class)); private static final Logger LOGGER = LoggerFactory.getLogger(DynamoDbEnhancedClientQuery.class); private void query () { PageIterable<YourItem> response = DYNAMODB_TABLE.query(QueryEnhancedRequest.builder()\n                .filterExpression(Expression.builder()\n                        .expression( \"#name = :name_val\" )\n                        .expressionNames(Map.of( \"#name\" , \"name\" ))\n                        .expressionValues(Map.of( \":name_val\" , AttributeValue.fromS( \"SomeName\" )))\n                        .build())\n                .limit( 100 )\n                .queryConditional(QueryConditional.sortBeginsWith(Key.builder()\n                        .partitionValue( \"id#1\" )\n                        .sortValue( \"cart#\" )\n                        .build()))\n                .scanIndexForward( false )\n                .build());\n\n        LOGGER.info( \"nr of items: \" + response.items().stream().count());\n        LOGGER.info( \"First item pk: \" + response.items().iterator().next().getPk());\n        LOGGER.info( \"First item sk: \" + response.items().iterator().next().getSk());\n    } @DynamoDbBean public static class YourItem { public YourItem () { } public YourItem (String pk, String sk, String name) { this .pk = pk; this .sk = sk; this .name = name;\n        } private String pk; private String sk; private String name; @DynamoDbPartitionKey public void setPk (String pk) { this .pk = pk;\n        } public String getPk () { return pk;\n        } @DynamoDbSortKey public void setSk (String sk) { this .sk = sk;\n        } public String getSk () { return sk;\n        } public void setName (String name) { this .name = name;\n        } public String getName () { return name;\n        }\n    }\n} High-level interface using immutable data classes When you perform a Query with the high-level immutable data classes, the code is the same as the\n                        high-level interface example except for the construction of the entity class YourItem or YourImmutableItem . For more\n                        information, see the PutItem example. High-level interface using immutable data classes and third-party\n                        boilerplate generation libraries When you perform a Query with the high-level immutable data\n                        classes, the code is the same as the high-level interface example except for\n                        the construction of the entity class YourItem or YourImmutableLombokItem . For more information, see the PutItem example. Additional\n                code examples For additional examples of how to use DynamoDB with the SDK for Java 2.x, refer to the following\n            code example repositories: Official\n                        AWS single-action code examples Community-maintained single-action code examples Official AWS scenario-oriented code examples Synchronous and asynchronous programming The AWS SDK for Java 2.x provides both synchronous and asynchronous clients for AWS services, such as DynamoDB. The DynamoDbClient and DynamoDbEnhancedClient classes\n            provide synchronous methods that block your thread's\n            execution\n            until the client receives a response from the service. This client is the most\n            straightforward way of interacting with DynamoDB if you have no need for asynchronous\n            operations. The DynamoDbAsyncClient and DynamoDbEnhancedAsyncClient classes provide asynchronous methods that return immediately, and give control back to\n            the calling thread without waiting for a response. The non-blocking client has an\n            advantage that it\n            uses\n            for high concurrency across a few threads, which provides efficient handling of I/O\n            requests with minimal compute resources. This improves throughput and\n            responsiveness. The AWS SDK for Java 2.x uses the native support for non-blocking I/O. The AWS SDK for Java 1.x had\n            to simulate non-blocking I/O. The synchronous methods return before a response is available, so you need a way to\n            get the response when it's ready. The asynchronous methods in the AWS SDK for Java return a CompletableFuture object that contains the results of the\n            asynchronous operation in the future. When you call get() or join() on these CompletableFuture objects, your code\n            blocks\n            until\n            the result is available. If you call these at the same time that you make the request,\n            then the behavior is similar to a plain synchronous call. For\n            more information about asynchronous programming, see Use asynchronous\n                programming in the AWS SDK for Java 2.x Developer Guide . HTTP clients For supporting every client, there exists an HTTP client that handles communication\n            with the AWS services. You can plug in alternative HTTP clients, choosing one that has\n            the characteristics that best fit your application. Some are more lightweight; some have\n            more configuration options. Some HTTP clients support only synchronous use, while others support only asynchronous\n            use. For a flowchart that can help you select the optimal HTTP client for your workload,\n            see HTTP client recommendations in the AWS SDK for Java 2.x Developer Guide . The following list presents some of the possible HTTP clients: Topics Apache-based HTTP client URLConnection-based HTTP\n                    client Netty-based HTTP client AWS CRT-based HTTP client Apache-based HTTP client The ApacheHttpClient class supports synchronous service\n                clients. It's the default HTTP client for synchronous use. For information about\n                configuring the ApacheHttpClient class, see Configure the Apache-based HTTP client in the AWS SDK for Java 2.x Developer Guide . URLConnection -based HTTP\n                    client The UrlConnectionHttpClient class is another option for\n                synchronous clients. It loads more quickly than the Apache-based HTTP client, but\n                has fewer features. For information about configuring the UrlConnectionHttpClient class, see Configure\n                    the URLConnection-based HTTP client in the AWS SDK for Java 2.x Developer Guide . Netty-based HTTP client The NettyNioAsyncHttpClient class supports async clients. It's the\n                default choice for async use. For information about configuring the NettyNioAsyncHttpClient class, see Configure the Netty-based HTTP client in the AWS SDK for Java 2.x Developer Guide . AWS CRT-based HTTP client The newer AwsCrtHttpClient and AwsCrtAsyncHttpClient classes from the AWS Common Runtime (CRT) libraries are more options that support\n                synchronous and asynchronous clients. Compared to other HTTP clients, AWS CRT\n                offers: Faster SDK startup time Smaller memory footprint Reduced latency time Connection health management DNS load balancing For information about configuring the AwsCrtHttpClient and AwsCrtAsyncHttpClient classes, see Configure\n                    the AWS CRT-based HTTP clients in the AWS SDK for Java 2.x Developer Guide . The AWS CRT-based HTTP client isn't the default because that would break\n                backward compatibility for existing applications. However, for DynamoDB we recommend\n                that you use the AWS CRT-based HTTP client for both sync and async uses. For an introduction to the AWS CRT-based HTTP client, see Announcing availability of the AWS CRT HTTP Client in the\n                    AWS SDK for Java 2.x on the AWS Developer Tools\n                Blog . Configuring an HTTP client When configuring a client, you can provide various configuration options,\n            including: Setting timeouts for different aspects of API calls. Enabling TCP Keep-Alive. Controlling the retry policy when encountering\n                    errors. Specifying execution attributes that Execution interceptor instances can modify. Execution interceptors\n                    can write code that intercept the execution of your API requests and responses.\n                    This enables you to perform tasks such as publishing metrics and modifying\n                    requests in-flight. Adding or manipulating HTTP headers. Enabling the tracking of client-side\n                        performance metrics . Using this feature helps you to collect metrics\n                    about the service clients in your application and analyze the output in\n                    Amazon CloudWatch. Specifying an alternate executor service to be used for scheduling tasks, such\n                    as async retry attempts and timeout tasks. You control the configuration by providing a ClientOverrideConfiguration object to the service client Builder class. You'll see this in some code examples in the following\n            sections. The ClientOverrideConfiguration provides standard configuration choices.\n            The different pluggable HTTP clients have implementation-specific configuration\n            possibilities as\n            well. Topics in this section Timeout configuration RetryMode DefaultsMode Keep-Alive configuration Timeout configuration You can adjust the client configuration to control various timeouts related to the\n                service calls. DynamoDB provides lower latencies compared to other AWS services.\n                Therefore, you might want to adjust these properties to lower timeout values so that\n                you can fail fast if there's a networking issue. You can customize the latency related behavior using ClientOverrideConfiguration on the DynamoDB client or by changing\n                detailed configuration options on the underlying HTTP client implementation. You can configure the following impactful properties using ClientOverrideConfiguration : apiCallAttemptTimeout \u2013 The amount of time to wait for\n                        a single attempt for an HTTP request to complete before giving up and timing\n                        out. apiCallTimeout \u2013 The amount of time that the client\n                        has to completely execute an API call. This includes the request handler\n                        execution that consists of all the HTTP requests, including retries. The AWS SDK for Java 2.x provides default values for some timeout options, such as connection timeout and\n                socket timeouts. The SDK doesn't provide default values for API call timeouts or\n                individual API call attempt timeouts. If these timeouts aren't set in the ClientOverrideConfiguration , then the SDK uses the socket timeout\n                value instead for the overall API call timeout. The socket timeout has a default\n                value of 30 seconds. RetryMode Another configuration related to the timeout configuration that you should\n                consider is the RetryMode configuration object. This configuration\n                object contains a collection of retry behaviors. The SDK for Java 2.x supports the following retry modes: legacy \u2013 The default retry mode if you don't\n                        explicitly change it. This retry mode is specific to the Java SDK. It's\n                        characterized by up to three retries, or more for services such as DynamoDB,\n                        which has up to eight retries. standard \u2013 Named \"standard\" because it's more\n                        consistent with other AWS SDKs. This mode waits for a random amount of\n                        time ranging from 0ms to 1,000ms for the first retry. If another retry is\n                        necessary, then this mode picks another random time from 0ms to 1,000ms and\n                        multiplies it by two. If an additional retry is necessary, then it does the\n                        same random pick multiplied by four, and so on. Each wait is capped at 20\n                        seconds. This mode performs retries on more detected failure conditions than\n                        the legacy mode. For DynamoDB, it performs up to three total max\n                        attempts unless you override with numRetries . adaptive \u2013\n                        Builds on standard mode and dynamically limits the rate of\n                        AWS requests to maximize success rate. This can occur at the expense of\n                        request latency. We don't recommend adaptive retry mode when predictable\n                        latency is important. You can find an expanded definition of these retry modes in the Retry\n                    behavior topic in the AWS SDKs and Tools Reference Guide . Retry policies All RetryMode configurations have a RetryPolicy , which is built based on one or more RetryCondition configurations. The TokenBucketRetryCondition is especially important\n                    to the retry behavior of the DynamoDB SDK client implementation. This condition\n                    limits the number of retries that the SDK makes using a token bucket algorithm.\n                    Depending on the selected retry mode, throttling exceptions may or may not\n                    subtract tokens from the TokenBucket . When a client encounters a retryable error, such as a throttling exception or a\n                temporary server error, then the SDK automatically retries the request. You can\n                control how many times and how quickly these retries happen. When configuring a client, you can provide a RetryPolicy that\n                supports the following parameters: numRetries \u2013 The maximum number of retries that should be applied before a\n                        request is considered to be failed. The default value is 8 regardless of the\n                        retry mode that you use. Warning Make sure that you change this default value after due\n                            consideration. backoffStrategy \u2013 The BackoffStrategy to apply to the retries, with FullJitterBackoffStrategy being the default\n                        strategy. This strategy performs an exponential delay between additional\n                        retries based on the current number or retries, a base delay, and a maximum\n                        backoff time. It then adds jitter to provide a bit of randomness. The base\n                        delay used in the exponential delay is 25 ms regardless of the retry\n                        mode. retryCondition \u2013 The RetryCondition determines whether to retry a\n                        request at all. By default, it retries a specific set of HTTP status codes\n                        and exceptions that it believes are retryable. For most situations, the\n                        default configuration should be sufficient. The following code provides an alternative retry policy. It specifies a total of\n                five retries (six total requests). The first retry should occur after a delay of\n                approximately 100ms, with each additional retry doubling that time exponentially, up\n                to a maximum delay of one second. DynamoDbClient client = DynamoDbClient.builder()\n    .overrideConfiguration(ClientOverrideConfiguration.builder()\n        .retryPolicy(RetryPolicy.builder()\n            .backoffStrategy(FullJitterBackoffStrategy.builder()\n                .baseDelay(Duration.ofMillis( 100 ))\n                .maxBackoffTime(Duration.ofSeconds( 1 ))\n                .build())\n            .numRetries( 5 )\n            .build())\n        .build())\n    .build(); DefaultsMode The timeout properties that ClientOverrideConfiguration and the RetryMode don't manage are typically configured implicitly by\n                specifying a DefaultsMode . The AWS SDK for Java 2.x (version 2.17.102 or later) introduced support for DefaultsMode . This feature provides a set of default values for\n                common configurable settings, such as HTTP communication settings, retry behavior,\n                service Regional endpoint settings, and potentially any SDK-related configuration.\n                When you use this feature, you can get new configuration defaults tailored to common\n                usage scenarios. The default modes are standardized across all of the AWS SDKs. The SDK for Java 2.x\n                supports the following default modes: legacy \u2013 Provides default settings that vary by AWS\n                        SDK and that existed before DefaultsMode was\n                        established. standard \u2013 Provides default non-optimized settings for\n                        most scenarios. in-region \u2013 Builds on the standard mode and includes\n                        settings tailored for applications that call AWS services from within the\n                        same AWS Region. cross-region \u2013 Builds on the standard mode and\n                        includes settings with high timeouts for applications that call\n                        AWS services in a different\n                        Region. mobile \u2013 Builds on the standard mode and includes\n                        settings with high timeouts tailored for mobile applications with higher\n                        latencies. auto \u2013 Builds on the standard mode and includes\n                        experimental features. The SDK attempts to discover the runtime environment\n                        to determine the appropriate settings automatically. The auto-detection is\n                        heuristics-based and does not provide 100% accuracy. If the runtime\n                        environment can't be determined, then standard mode is used. The\n                        auto-detection might query Instance\n                            metadata and user data , which might introduce latency. If\n                        startup latency is critical to your application, we recommend choosing an\n                        explicit DefaultsMode instead. You can configure the defaults mode in the following ways: Directly on a client, through AwsClientBuilder.Builder#defaultsMode(DefaultsMode) . On a configuration profile, through the defaults_mode profile\n                        file property. Globally, through the aws.defaultsMode system\n                        property. Globally, through the AWS_DEFAULTS_MODE environment\n                        variable. Note For any mode other than legacy , the vended default values might\n                    change as best practices evolve. Therefore, if you're using a mode other than legacy , then we encourage you to perform testing when upgrading\n                    the SDK. The Smart configuration\n                    defaults in the AWS SDKs and Tools Reference Guide provides a\n                list of configuration properties and their default values in the different default\n                modes. You choose the defaults mode value based on your application's characteristics and\n                the AWS service that the application interacts with. These values are configured with a broad selection of AWS services in mind. For\n                a typical DynamoDB deployment in which both your DynamoDB\n                tables\n                and application are deployed in one Region, the in-region defaults mode\n                is most relevant among the standard default modes. Example DynamoDB SDK client configuration tuned for low-latency calls The following example adjusts the timeouts to lower values for an expected\n                    low-latency DynamoDB call. DynamoDbAsyncClient asyncClient = DynamoDbAsyncClient.builder()\n    .defaultsMode(DefaultsMode.IN_REGION)\n    .httpClientBuilder(AwsCrtAsyncHttpClient.builder())\n    .overrideConfiguration(ClientOverrideConfiguration.builder()\n        .apiCallTimeout(Duration.ofSeconds( 3 ))\n        .apiCallAttemptTimeout(Duration.ofMillis( 500 ))\n        .build())\n    .build(); The individual HTTP client implementation may provide you with even more\n                    granular control over the timeout and connection usage behavior. For example,\n                    for the AWS CRT-based client, you can enable ConnectionHealthConfiguration , which enables the client to\n                    actively monitor the health of the used connections. For more information, see Advanced configuration of AWS CRT-based HTTP clients in the AWS SDK for Java 2.x Developer Guide . Keep-Alive configuration Enabling\n                keep-alive\n                can reduce latencies by reusing connections. There are two different kinds of\n                keep-alive: HTTP Keep-Alive and TCP Keep-Alive. HTTP Keep-Alive attempts to maintain the HTTPS connection between the\n                        client and server so later requests can reuse that connection. This skips\n                        the heavyweight HTTPS authentication on later requests. HTTP Keep-Alive is\n                        enabled by default on all clients. TCP Keep-Alive requests that the underlying operating system sends small\n                        packets over the socket connection to provide extra assurance that the\n                        socket is kept alive and to immediately detect any drops. This ensures that\n                        a later request won't spend time trying to use a dropped socket. By default,\n                        TCP Keep-Alive is disabled on all clients. The following code examples show\n                        how to enable it on each HTTP client. When enabled for all non-CRT based\n                        HTTP clients, the actual Keep-Alive mechanism is dependent on the operating\n                        system. Therefore, you must configure additional TCP Keep-Alive values, such\n                        as timeout and number of packets, through the operating system. You can do\n                        this using sysctl on Linux or macOS, or using registry values\n                        on Windows. Example to enable TCP Keep-Alive on an Apache-based HTTP client DynamoDbClient client = DynamoDbClient.builder()\n    .httpClientBuilder(ApacheHttpClient.builder().tcpKeepAlive( true ))\n    .build(); URLConnection -based\n                    HTTP client Any synchronous client that uses the URLConnection -based HTTP\n                    client HttpURLConnection doesn't have a mechanism to enable keep-alive. Example to enable TCP Keep-Alive on a Netty-based HTTP client DynamoDbAsyncClient client = DynamoDbAsyncClient.builder()\n    .httpClientBuilder(NettyNioAsyncHttpClient.builder().tcpKeepAlive( true ))\n    .build(); Example to enable TCP Keep-Alive on an AWS CRT-based HTTP client With the AWS CRT-based HTTP client, you can enable TCP keep-alive and\n                    control the duration. DynamoDbClient client = DynamoDbClient.builder()\n    .httpClientBuilder(AwsCrtHttpClient.builder()\n    .tcpKeepAliveConfiguration(TcpKeepAliveConfiguration.builder()\n        .keepAliveInterval(Duration.ofSeconds( 50 ))\n        .keepAliveTimeout(Duration.ofSeconds( 5 ))\n        .build()))\n    .build(); When using the asynchronous DynamoDB client, you can enable TCP Keep-Alive as\n                    shown in the following code. DynamoDbAsyncClient client = DynamoDbAsyncClient.builder()\n    .httpClientBuilder(AwsCrtAsyncHttpClient.builder()\n    .tcpKeepAliveConfiguration(TcpKeepAliveConfiguration.builder()\n        .keepAliveInterval(Duration.ofSeconds( 50 ))\n        .keepAliveTimeout(Duration.ofSeconds( 5 ))\n        .build()))\n    .build(); Error handling When it comes to exception handling, the AWS SDK for Java 2.x uses runtime (unchecked)\n            exceptions. The base exception, covering all SDK exceptions, is SdkServiceException , which extends from the Java unchecked RuntimeException . If you catch this, you'll catch all exceptions that\n            the SDK throws. SdkServiceException has a subclass called AwsServiceException . This subclass indicates any issue in\n            communication with the AWS service. It has a subclass called DynamoDbException , which indicates an issue in\n            communication with DynamoDB. If you catch this, you'll catch all exceptions related to\n            DynamoDB, but no other SDK exceptions. There are more specific exception types under DynamoDbException . Some of these\n            exception types apply to control-plane operations such as TableAlreadyExistsException . Others apply to data-plane\n            operations. The following is an example of a common data-plane exception: ConditionalCheckFailedException \u2013 You\n                    specified a condition in the request that evaluated to false. For example, you\n                    might have tried to perform a conditional update on an item, but the actual\n                    value of the attribute did not match the expected value in the condition. A\n                    request that fails in this manner isn't retried. Other situations don't have a specific exception defined. For example, when your\n            requests get throttled the specific ProvisionedThroughputExceededException might get thrown, while in other cases the more generic DynamoDbException is thrown. In either case, you can determine if throttling caused the exception by\n            checking if isThrottlingException() returns true . Depending on your application needs, you can catch all AwsServiceException or DynamoDbException instances.\n            However, you often need different behavior in different situations. The logic to deal\n            with a condition check failure is different than that to handle throttling. Define which\n            exceptional paths you want to deal with and make sure to test the alternative paths.\n            This helps you make sure that you can deal with all relevant scenarios. For lists of common errors that you might encounter, see Error handling with DynamoDB . Also see Common Errors in the Amazon DynamoDB API Reference . The API\n            Reference also provides the exact errors possible for each\n            API\n            operation, such as for the Query operation. For information about handling exceptions, see Exception\n                handling for the AWS SDK for Java 2.x in the AWS SDK for Java 2.x Developer Guide . AWS request ID Each request includes a request ID, which can be useful to pull if you're working with\n            AWS Support to diagnose an issue. Each exception derived from SdkServiceException has a requestId() method available to retrieve the request\n            ID. Logging Using the logging provided that the SDK provides can be useful both for catching any\n            important messages from the client libraries and for more in-depth debugging purposes.\n            Loggers are hierarchical and the SDK uses software.amazon.awssdk as its\n            root logger. You can configure the level with one of TRACE , DEBUG , INFO , WARN , ERROR , ALL , or OFF . The configured level applies to that logger\n            and down into the logger hierarchy. For its logging, the AWS SDK for Java 2.x uses the Simple Logging Fa\u00e7ade for Java (SLF4J).\n            This acts as an abstraction layer around other loggers, and you can use it to plug in\n            the logger that you prefer. For instructions about plugging in loggers, see the SLF4J user manual . Each logger has a particular behavior. By default, the Log4j 2.x logger creates a ConsoleAppender , which appends log events to System.out and defaults to the ERROR log level. The SimpleLogger logger included in SLF4J outputs by default to System.err and defaults to the INFO log level. We recommend that you set the level to WARN for software.amazon.awssdk for any production deployments to catch any\n            important messages from the SDK's client libraries while limiting the output\n            quantity. If SLF4J can't find a supported logger on the class path (no SLF4J binding), then it\n            defaults to a no operation\n                implementation . This implementation results in logging messages to System.err explaining that SLF4J could not find a logger implementation\n            on the classpath. To prevent this situation, you must add a logger implementation. To do\n            this, you can add a dependency in your Apache Maven pom.xml on artifacts,\n            such as org.slf4j.slf4j-simple or org.apache.logging.log4j.log4j-slf4j2-imp . For information about how to configure the logging in the SDK, including adding\n            logging dependencies to your application configuration, see Logging with the\n                SDK for Java 2.x in the AWS SDK for Java Developer Guide . The following configuration in the Log4j2.xml file shows how to adjust\n            the logging behavior if you use the Apache Log4j 2 logger. This configuration sets the\n            root logger level to WARN . All loggers in the hierarchy inherit this log\n            level, including the software.amazon.awssdk logger. By default, the output goes to System.out . In the following example, we\n            still override the default output Log4j appender to apply a tailored Log4j PatternLayout . Example of a Log4j2.xml configuration file The following configuration logs messages to the console at the ERROR and WARN levels for all logger hierarchies. < Configuration status = \"WARN\" > < Appenders > < Console name = \"ConsoleAppender\" target = \"SYSTEM_OUT\" > < PatternLayout pattern = \"%d { YYYY-MM-dd HH:mm:ss} [%t] %-5p %c:%L - %m%n\" /> </ Console > </ Appenders > < Loggers > < Root level = \"WARN\" > < AppenderRef ref = \"ConsoleAppender\" /> </ Root > </ Loggers > </ Configuration > AWS request ID logging When something goes wrong, you can find request IDs within exceptions. However, if\n                you want the request IDs for requests that aren't generating exceptions, then you\n                can use logging. The software.amazon.awssdk.request logger outputs request IDs at the DEBUG level. The following example extends the previous configuration example to keep the root logger\n                level at ERROR , the software.amazon.awssdk at level WARN , and the software.amazon.awssdk.request at level DEBUG . Setting these levels helps to catch the request IDs and\n                other request-related details, such as the endpoint and status code. < Configuration status = \"WARN\" > < Appenders > < Console name = \"ConsoleAppender\" target = \"SYSTEM_OUT\" > < PatternLayout pattern = \"%d { YYYY-MM-dd HH:mm:ss} [%t] %-5p %c:%L - %m%n\" /> </ Console > </ Appenders > < Loggers > < Root level = \"ERROR\" > < AppenderRef ref = \"ConsoleAppender\" /> </ Root > < Logger name = \"software.amazon.awssdk\" level = \"WARN\" /> < Logger name = \"software.amazon.awssdk.request\" level = \"DEBUG\" /> </ Loggers > </ Configuration > Here is an example of the log output: 2022-09-23 16:02:08 [main] DEBUG software.amazon.awssdk.request:85 - Sending Request: DefaultSdkHttpFullRequest(httpMethod=POST, protocol=https, host=dynamodb.us-east-1.amazonaws.com, encodedPath=/, headers=[amz-sdk-invocation-id, Content-Length, Content-Type, User-Agent, X-Amz-Target], queryParameters=[])\n 2022-09-23 16:02:08 [main] DEBUG software.amazon.awssdk.request:85 - Received successful response: 200, Request ID: QS9DUMME2NHEDH8TGT9N5V53OJVV4KQNSO5AEMVJF66Q9ASUAAJG, Extended Request ID: not available Pagination Some requests, such as Query and Scan , limit the size of data returned on a single request\n            and require you make repeated requests to pull subsequent pages. You can control the maximum number of items to read for each page with the Limit parameter. For example, you can use the Limit parameter to retrieve only the last 10 items. This limit specifies how many items to\n            read from the table before any filtering is applied. If you want exactly 10 items after\n            filtering, there's no way to specify that. You can control only the pre-filtered count\n            and check client-side when you've actually retrieved 10 items. Regardless of the limit,\n            responses always have a maximum size of 1 MB. A LastEvaluatedKey might be included in the API response. This indicates\n            that the response ended because it reached a count limit or a size limit. This key is\n            the last key evaluated for that response. By interacting directly with the API, you can\n            retrieve this LastEvaluatedKey and pass it to a follow-up call as ExclusiveStartKey to read the next chunk from that starting point. If\n            no LastEvaluatedKey is returned, it means that there are no more items that\n            match the Query or Scan API call. The following example uses the low-level interface to limit the items to 100 based on\n            the keyConditionExpression parameter. QueryRequest.Builder queryRequestBuilder = QueryRequest.builder()\n        .expressionAttributeValues(Map.of( \":pk_val\" , AttributeValue.fromS( \"123\" ), \":sk_val\" , AttributeValue.fromN( \"1000\" )))\n        .keyConditionExpression( \"pk = :pk_val AND sk > :sk_val\" )\n        .limit( 100 )\n        .tableName(TABLE_NAME); while ( true ) { QueryResponse queryResponse = DYNAMODB_CLIENT.query(queryRequestBuilder.build());\n\n    queryResponse.items().forEach(item -> { LOGGER.info( \"item PK: [\" + item. get ( \"pk\" ) + \"] and SK: [\" + item. get ( \"sk\" ) + \"]\" );\n    }); if (!queryResponse.hasLastEvaluatedKey()) { break ;\n    }\n    queryRequestBuilder.exclusiveStartKey(queryResponse.lastEvaluatedKey());\n} The AWS SDK for Java 2.x can simplify this interaction with DynamoDB by providing auto-pagination\n            methods that make multiple service calls to automatically get the next pages of results\n            for you. This simplifies your code, but it takes away some control of resource usage\n            that you would keep by manually reading pages. By using the Iterable methods available in the DynamoDB client, such as QueryPaginator and ScanPaginator , the SDK takes care of the pagination. The\n            return type of these methods is a custom iterable that you can use to iterate through\n            all the pages. The SDK internally handles service calls for you. Using the Java Stream\n            API, you can handle the result of QueryPaginator as shown in the following\n            example. QueryPublisher queryPublisher =\n    DYNAMODB_CLIENT.queryPaginator(QueryRequest.builder()\n        .expressionAttributeValues(Map.of( \":pk_val\" , AttributeValue.fromS( \"123\" ), \":sk_val\" , AttributeValue.fromN( \"1000\" )))\n        .keyConditionExpression( \"pk = :pk_val AND sk > :sk_val\" )\n        .limit( 100 )\n        .tableName( \"YourTableName\" )\n        .build());\n\nqueryPublisher.items().subscribe(item ->\n    System. out .println(item. get ( \"itemData\" ))). join (); Data class annotations The Java SDK provides several annotations that you can put on the attributes of your\n            data class. These annotations influence how the SDK interacts with the attributes. By\n            adding an annotation, you can have an attribute behave as an implicit atomic counter,\n            maintain an auto-generated timestamp value, or track an item version number. For more\n            information, see Data class\n                annotations . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Programming with JavaScript Error handling"}, {"title": "Programming Amazon DynamoDB with JavaScript - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/programming-with-javascript.html", "content": "Programming Amazon DynamoDB with JavaScript PDF RSS This guide provides an orientation to programmers wanting to use Amazon DynamoDB with\n        JavaScript. Learn about the AWS SDK for JavaScript, abstraction layers available, configuring\n        connections, handling errors, defining retry policies, managing keep-alive, and more. Topics About AWS SDK for JavaScript Using the AWS SDK for JavaScript\n                V3 Accessing JavaScript\n                documentation Abstraction\n                layers Using the marshall\n                utility function Reading items Conditional\n                writes Pagination Specifying configuration Waiters Error handling Logging Considerations About AWS SDK for JavaScript The AWS SDK for JavaScript provides access to AWS services using either browser scripts or\n            Node.js. This documentation focuses on the latest version of the SDK (V3). The AWS SDK for JavaScript\n            V3 is maintained by AWS as an open-source project hosted on GitHub . Issues and feature requests are\n            public and you can access them on the issues page for the GitHub repository. JavaScript V2 is similar to V3, but contains syntax differences. V3 is more modular,\n            making it easier to ship smaller dependencies, and has first-class TypeScript support.\n            We recommend using the latest version of the SDK. Using the AWS SDK for JavaScript\n                V3 You can add the SDK to your Node.js application using the Node Package Manager. The\n            examples below show how to add the most common SDK packages for working with\n            DynamoDB. npm install @aws-sdk/client-dynamodb npm install @aws-sdk/lib-dynamodb npm install @aws-sdk/util-dynamodb Installing packages adds references to the dependency section of your package.json\n            project file. You have the option to use the newer ECMAScript module syntax. For further\n            details on these two approaches, see the Considerations section. Accessing JavaScript\n                documentation Get started with JavaScript documentation with the following resources: Access the Developer guide for core JavaScript documentation. Installation\n                    instructions are located in the Setting up section. Access the API reference documentation to explore all available classes and\n                    methods. The SDK for JavaScript supports many AWS services other than DynamoDB. Use the\n                    following procedure to locate specific API coverage for DynamoDB: From Services , choose DynamoDB and\n                                Libraries . This documents the low-level client. Choose lib-dynamodb . This documents the\n                            high-level client. The two clients represent two different abstraction\n                            layers that you have the choice to use. See the section below for more\n                            information about abstraction layers. Abstraction\n                layers The SDK for JavaScript V3 has a low-level client ( DynamoDBClient ) and a\n            high-level client ( DynamoDBDocumentClient ). Topics Low-level client\n                        (DynamoDBClient) High-level client\n                        (DynamoDBDocumentClient) Low-level client\n                        ( DynamoDBClient ) The low-level client provides no extra abstractions over the underlying wire\n                protocol. It gives you full control over all aspects of communication, but because\n                there are no abstractions, you must do things like provide item definitions using\n                the DynamoDB JSON format. As the example below shows, with this format data types must be stated explicitly.\n                An S indicates a string value and an N indicates a number value. Numbers on the wire are\n                always sent as strings tagged as number types to ensure no loss in precision. The\n                low-level API calls have a naming pattern such as PutItemCommand and GetItemCommand . The following example is using low-level client with Item defined\n                using DynamoDB JSON: const { DynamoDBClient, PutItemCommand } = require ( \"@aws-sdk/client-dynamodb\" ); const client = new DynamoDBClient( { }); async function addProduct ( ) { const params = { TableName : \"products\" , Item : { \"id\" : { S : \"Product01\" }, \"description\" : { S : \"Hiking Boots\" }, \"category\" : { S : \"footwear\" }, \"sku\" : { S : \"hiking-sku-01\" }, \"size\" : { N : \"9\" }\n    }\n  }; try { const data = await client.send( new PutItemCommand(params)); console .log( 'result : ' + JSON .stringify(data));\n  } catch (error) { console .error( \"Error:\" , error);\n  }\n}\naddProduct(); High-level client\n                        ( DynamoDBDocumentClient ) The high-level DynamoDB document client offers built-in convenience features, such as\n                eliminating the need to manually marshal data and allowing for direct reads and\n                writes using standard JavaScript objects. The documentation for lib-dynamodb provides the list of\n                advantages. To instantiate the DynamoDBDocumentClient , construct a low-level DynamoDBClient and then wrap it with a DynamoDBDocumentClient . The function naming convention differs\n                slightly between the two packages. For instance, the low-level uses PutItemCommand while the high-level uses PutCommand .\n                The distinct names allow both sets of functions to coexist in the same context,\n                allowing you to mix both in the same script. const { DynamoDBClient } = require ( \"@aws-sdk/client-dynamodb\" ); const { DynamoDBDocumentClient, PutCommand } = require ( \"@aws-sdk/lib-dynamodb\" ); const client = new DynamoDBClient( { }); const docClient = DynamoDBDocumentClient.from(client); async function addProduct ( ) { const params = { TableName : \"products\" , Item : { id : \"Product01\" , description : \"Hiking Boots\" , category : \"footwear\" , sku : \"hiking-sku-01\" , size : 9 ,\n    },\n  }; try { const data = await docClient.send( new PutCommand(params)); console .log( 'result : ' + JSON .stringify(data));\n  } catch (error) { console .error( \"Error:\" , error);\n  }\n}\n\naddProduct(); The pattern of usage is consistent when you're reading items using API operations\n                such as GetItem , Query , or Scan . Using the marshall\n                utility function You can use the low-level client and marshall or unmarshall the data types on your\n            own. The utility package, util-dynamodb , has a marshall() utility function that accepts\n            JSON and produces DynamoDB JSON, as well as an unmarshall() function, that\n            does the reverse. The following example uses the low-level client with data marshalling\n            handled by the marshall() call. const { DynamoDBClient, PutItemCommand } = require ( \"@aws-sdk/client-dynamodb\" ); const { marshall } = require ( \"@aws-sdk/util-dynamodb\" ); const client = new DynamoDBClient( { }); async function addProduct ( ) { const params = { TableName : \"products\" , Item : marshall( { id : \"Product01\" , description : \"Hiking Boots\" , category : \"footwear\" , sku : \"hiking-sku-01\" , size : 9 ,\n    }),\n  }; try { const data = await client.send( new PutItemCommand(params));\n  } catch (error) { console .error( \"Error:\" , error);\n  }\n}\naddProduct(); Reading items To read a single item from DynamoDB, you use the GetItem API operation.\n            Similar to the PutItem command, you have the choice to use either the\n            low-level client or the high-level Document client. The example below demonstrates using\n            the high-level Document client to retrieve an item. const { DynamoDBClient } = require ( \"@aws-sdk/client-dynamodb\" ); const { DynamoDBDocumentClient, GetCommand } = require ( \"@aws-sdk/lib-dynamodb\" ); const client = new DynamoDBClient( { }); const docClient = DynamoDBDocumentClient.from(client); async function getProduct ( ) { const params = { TableName : \"products\" , Key : { id : \"Product01\" ,\n    },\n  }; try { const data = await docClient.send( new GetCommand(params)); console .log( 'result : ' + JSON .stringify(data));\n  } catch (error) { console .error( \"Error:\" , error);\n  }\n}\n\ngetProduct(); Use the Query API operation to read multiple items. You can use the\n            low-level client or the Document client. The example below uses the high-level Document\n            client. const { DynamoDBClient } = require ( \"@aws-sdk/client-dynamodb\" ); const { DynamoDBDocumentClient,\n  QueryCommand,\n} = require ( \"@aws-sdk/lib-dynamodb\" ); const client = new DynamoDBClient( { }); const docClient = DynamoDBDocumentClient.from(client); async function productSearch ( ) { const params = { TableName : \"products\" , IndexName : \"GSI1\" , KeyConditionExpression : \"#category = :category and begins_with(#sku, :sku)\" , ExpressionAttributeNames : { \"#category\" : \"category\" , \"#sku\" : \"sku\" ,\n    }, ExpressionAttributeValues : { \":category\" : \"footwear\" , \":sku\" : \"hiking\" ,\n    },\n  }; try { const data = await docClient.send( new QueryCommand(params)); console .log( 'result : ' + JSON .stringify(data));\n  } catch (error) { console .error( \"Error:\" , error);\n  }\n}\n\nproductSearch(); Conditional\n                writes DynamoDB write operations can specify a logical condition expression that must evaluate\n            to true for the write to proceed. If the condition does not evaluate to true, the write\n            operation generates an exception. The condition expression can check if the item already\n            exists or if its attributes match certain constraints. ConditionExpression = \"version = :ver AND size(VideoClip) < :maxsize\" When the conditional expression fails, you can use ReturnValuesOnConditionCheckFailure to request that the error response\n            include the item that didn't satisfy the conditions to deduce what the problem was. For\n            more details, see Handle conditional write errors in high concurrency scenarios with\n                Amazon DynamoDB . try { const response = await client.send( new PutCommand( { TableName : \"YourTableName\" , Item : item, ConditionExpression : \"attribute_not_exists(pk)\" , ReturnValuesOnConditionCheckFailure : \"ALL_OLD\" }));\n  } catch (e) { if (e.name === 'ConditionalCheckFailedException' ) { console .log( 'Item already exists:' , e.Item);\n      } else { throw e;\n      }\n  } Additional code examples showing other aspects of JavsScript SDK V3 usage are\n            available in the JavaScript SDK V3 Documentation and under the DynamoDB-SDK-Examples GitHub repository . Pagination Topics Using the\n                        paginateScan convenience method Read requests such as Scan or Query will likely return\n            multiple items in a dataset. If you perform a Scan or Query with a Limit parameter, then once the system has read that many items, a\n            partial response will be sent, and you'll need to paginate to retrieve additional\n            items. The system will only read a maximum of 1 megabyte of data per request. If you're\n            including a Filter expression, the system will still read a megabyte, at\n            maximum, of data from disk, but will return the items of that megabyte that match the\n            filter. The filter operation could return 0 items for a page, but still require further\n            pagination before the search is exhausted. You should look for LastEvaluatedKey in the response and using it as the ExclusiveStartKey parameter in a subsequent request to continue data\n            retrieval. This serves as a bookmark as noted in the following example. Note The sample passes a null lastEvaluatedKey as the ExclusiveStartKey on the first iteration and this is\n                allowed. Example using the LastEvaluatedKey : const { DynamoDBClient, ScanCommand } = require ( \"@aws-sdk/client-dynamodb\" ); const client = new DynamoDBClient( { }); async function paginatedScan ( ) { let lastEvaluatedKey; let pageCount = 0 ; do { const params = { TableName : \"products\" , ExclusiveStartKey : lastEvaluatedKey,\n    }; const response = await client.send( new ScanCommand(params));\n    pageCount++; console .log( `Page $ { pageCount} , Items:` , response.Items);\n    lastEvaluatedKey = response.LastEvaluatedKey;\n  } while (lastEvaluatedKey);\n}\n\npaginatedScan().catch( ( err ) => { console .error(err);\n}); Using the paginateScan convenience method The SDK provides convenience methods called paginateScan and paginateQuery that do this work for you and makes the repeated\n                requests behind the scenes. Specify the max number of items to read per request\n                using the standard Limit parameter. const { DynamoDBClient, paginateScan } = require ( \"@aws-sdk/client-dynamodb\" ); const client = new DynamoDBClient( { }); async function paginatedScanUsingPaginator ( ) { const params = { TableName : \"products\" , Limit : 100 }; const paginator = paginateScan( { client}, params); let pageCount = 0 ; for await ( const page of paginator) { pageCount++; console .log( `Page $ { pageCount} , Items:` , page.Items);\n  }\n}\n\npaginatedScanUsingPaginator().catch( ( err ) => { console .error(err);\n}); Note Performing full table scans regularly is not a recommended access pattern unless\n                the table is small. Specifying configuration Topics Config for\n                    timeouts Config for\n                    keep-alive Config for\n                    retries When setting up the DynamoDBClient , you can specify various configuration\n            overrides by passing a configuration object to the constructor. For example, you can\n            specify the Region to connect to if it's not already known to the calling context or the\n            endpoint URL to use. This is useful if you want to target a DynamoDB Local instance for\n            development purposes. const client = new DynamoDBClient( { region : \"eu-west-1\" , endpoint : \"http://localhost:8000\" ,\n}); Config for\n                    timeouts DynamoDB uses HTTPS for client-server communication. You can control some aspects of\n                the HTTP layer by providing a NodeHttpHandler object. For example, you\n                can adjust the key timeout values connectionTimeout and requestTimeout . The connectionTimeout is the maximum\n                duration, in milliseconds, that the client will wait while trying to establish a\n                connection before giving up. The requestTimeout defines how long the client will wait for a\n                response after a request has been sent, also in milliseconds. The defaults for both\n                are zero, meaning the timeout is disabled and there's no limit on how long the\n                client will wait if the response does not arrive. You should set the timeouts to\n                something reasonable so in the event of a network issue the request will error out\n                and a new request can be initiated. For example: import { DynamoDBClient } from \"@aws-sdk/client-dynamodb\" ; import { NodeHttpHandler } from \"@smithy/node-http-handler\" ; const requestHandler = new NodeHttpHandler( { connectionTimeout : 2000 , requestTimeout : 2000 ,\n}); const client = new DynamoDBClient( { requestHandler\n}); Note The example provided uses the Smithy import. Smithy is a language for defining services and SDKs,\n                    open-source and maintained by AWS. In addition to configuring timeout values, you can set the maximum number of\n                sockets, which allows for an increased number of concurrent connections per origin.\n                The developer guide includes details on configuring the maxSockets parameter . Config for\n                    keep-alive When using HTTPS, the first request always takes some back-and-forth communication\n                to establish a secure connection. HTTP Keep-Alive allows subsequent requests to\n                reuse the already-established connection, making the requests more efficient and\n                lowering latency. HTTP Keep-Alive is enabled by default with JavaScript V3. There's a limit to how long an idle connection can be kept alive. Consider sending\n                periodic requests, maybe every minute, if you have an idle connection but want the\n                next request to use an already-established connection. Note Note that in the older V2 of the SDK, keep-alive was off by default, meaning\n                    each connection would get closed immediately after use. If using V2, you can\n                    override this setting. Config for\n                    retries When the SDK receives an error response and the error is resumable as determined\n                by the SDK, such as a throttling exception or a temporary service exception, it will\n                retry again. This happens invisibly to you as the caller, except that you might\n                notice the request took longer to succeed. The SDK for JavaScript V3 will make 3 total requests, by default, before giving up\n                and passing the error into the calling context. You can adjust the number and\n                frequency of these retries. The DynamoDBClient constructor accepts a maxAttempts setting that limits how many attempts will happen. The below example raises the\n                value from the default of 3 to a total of 5. If you set it to 0 or 1, that indicates\n                you don't want any automatic retries and want to handle any resumable errors\n                yourself within your catch block. const client = new DynamoDBClient( { maxAttempts : 5 ,\n}); You can also control the timing of the retries with a custom retry strategy. To do\n                this, import the util-retry utility package and create a custom backoff\n                function that calculates the wait time between retries based on the current retry\n                count. The example below says to make a maximum of 5 attempts with delays of 15, 30, 90,\n                and 360 milliseconds should the first attempt fail. The custom backoff function, calculateRetryBackoff , calculates the delays by accepting the\n                retry attempt number (starts with 1 for the first retry) and returns how many\n                milliseconds to wait for that request. const { ConfiguredRetryStrategy } = require ( \"@aws-sdk/util-retry\" ); const calculateRetryBackoff = ( attempt ) => { const backoffTimes = [ 15 , 30 , 90 , 360 ]; return backoffTimes[attempt - 1 ] || 0 ;\n}; const client = new DynamoDBClient( { retryStrategy : new ConfiguredRetryStrategy( 5 , // max attempts. calculateRetryBackoff // backoff function. ),\n}); Waiters The DynamoDB client includes two useful waiter functions that can be used when creating, modifying, or deleting\n            tables when you want your code to wait to proceed until the table modification has\n            finished. For example, you can deploy a table, call the waitUntilTableExists function, and the code will block until the table\n            has been made ACTIVE . The waiter internally polls the DynamoDB service\n            with a describe-table every 20 seconds. import { waitUntilTableExists, waitUntilTableNotExists} from \"@aws-sdk/client-dynamodb\" ;\n\n\u2026 <create table details> const results = await waitUntilTableExists( { client : client, maxWaitTime : 180 }, { TableName : \"products\" }); if (results.state == 'SUCCESS' ) { return results.reason.Table\n} console .error( ` $ { results.state} $ { results.reason} ` ); The waitUntilTableExists feature returns control only when it can perform\n            a describe-table command that shows the table status ACTIVE . This ensures that you can use waitUntilTableExists to wait for the completion of creation, as well as\n            modifications such as adding a GSI index, which may take some time to apply before the\n            table returns to ACTIVE status. Error handling In the early examples here, we've caught all errors broadly. However, in practical\n            applications, it's important to discern between various error types and implement more\n            precise error handling. DynamoDB error responses contain metadata, including the name of the error. You can catch\n            errors then match against the possible string names of error conditions to determine how\n            to proceed. For server-side errors, you can leverage the instanceof operator with the error types exported by the @aws-sdk/client-dynamodb package to manage error handling efficiently. It's important to note that these errors only manifest after all retries have been\n            exhausted. If an error is retried and is eventually followed by a successful call, from\n            the code's perspective, there's no error just a slightly elevated latency. Retries will\n            show up in Amazon CloudWatch charts as unsuccessful requests, such as throttle or error requests.\n            If the client reaches the maximum retry count, it will give up and generate an\n            exception. This is the client's way of saying it's not going to retry. Below is a snippet to catch the error and take action based on the type of error that\n            was returned. import { ResourceNotFoundException\n  ProvisionedThroughputExceededException,\n  DynamoDBServiceException,\n} from \"@aws-sdk/client-dynamodb\" ; try { await client.send(someCommand);\n} catch (e) { if (e instanceof ResourceNotFoundException) { // Handle ResourceNotFoundException } else if (e instanceof ProvisionedThroughputExceededException) { // Handle ProvisionedThroughputExceededException } else if (e instanceof DynamoDBServiceException) { // Handle DynamoDBServiceException } else { // Other errors such as those from the SDK if (e.name === \"TimeoutError\" ) { // Handle SDK TimeoutError. } else { // Handle other errors. }\n    }\n} See Error handling with DynamoDB for common error strings in the DynamoDB Developer Guide . The exact errors possible with\n            any particular API call can be found in the documentation for that API call, such as the Query API docs . The metadata of errors include additional properties, depending on the error. For\n                a TimeoutError , the metadata includes the number of attempts that were\n            made and the totalRetryDelay , as shown below. { \"name\" : \"TimeoutError\" , \"$metadata\" : { \"attempts\" : 3 , \"totalRetryDelay\" : 199 }\n} If you manage your own retry policy, you'll want to differentiate between throttles\n            and errors: A throttle (indicated by a ProvisionedThroughputExceededException or ThrottlingException ) indicates a healthy service that's\n                    informing you that you've exceeded your read or write capacity on a DynamoDB table\n                    or partition. Every millisecond that passes, a bit more read or write capacity\n                    is made available, and so you can retry quickly, such as every 50ms, to attempt\n                    to access that newly released capacity. With throttles you don't especially need exponential backoff because\n                    throttles are lightweight for DynamoDB to return and incur no per-request charge to\n                    you. Exponential backoff assigns longer delays to client threads that have\n                    already waited the longest, which statistically extends the p50 and p99\n                    outward. An error (indicated by an InternalServerError or a ServiceUnavailable , among\n                    others) indicates a transient issue with the service, possibly the whole table\n                    or just the partition you're reading from or writing to. With errors, you can\n                    pause longer before retries, such as 250ms or 500ms, and use jitter to stagger\n                    the retries. Logging Turn on logging to get more details about what the SDK is doing. You can set a\n            parameter on the DynamoDBClient as shown in the example below. More log\n            information will appear in the console and includes metadata such as the status code and\n            the consumed capacity. If you run the code locally in a terminal window, the logs appear\n            there. If you run the code in AWS Lambda, and you have Amazon CloudWatch logs set up, then the\n            console output will be written there. const client = new DynamoDBClient( { logger : console }); You can also hook into the internal SDK activities and perform custom logging as\n            certain events happen. The example below uses the client's middlewareStack to intercept each request as it's being sent from the SDK and logs it as it's\n            happening. const client = new DynamoDBClient( { });\n\nclient.middlewareStack.add( ( next ) => async (args) => { console .log( \"Sending request from AWS SDK\" , { request : args.request }); return next(args);\n  }, { step : \"build\" , name : \"log-ddb-calls\" ,\n  }\n); The MiddlewareStack provides a powerful hook for observing and\n            controlling SDK behavior. See the blog Introducing Middleware Stack in Modular AWS SDK for JavaScript , for more\n            information. Considerations When implementing the AWS SDK for JavaScript in your project, here are some further factors to\n            consider. Module systems The SDK supports two module systems, CommonJS and ES (ECMAScript).\n                        CommonJS uses the require function, while ES uses the import keyword. Common JS \u2013 const { DynamoDBClient, PutItemCommand } =\n                                    require(\"@aws-sdk/client-dynamodb\"); ES (ECMAScript \u2013 import { DynamoDBClient, PutItemCommand }\n                                    from\n                                \"@aws-sdk/client-dynamodb\"; The project type dictates the module system to be used and is specified in\n                        the type section of your package.json file. The default is CommonJS. Use \"type\": \"module\" to indicate an ES project. If you have an\n                        existing Node.JS project that uses the CommonJS package format, you can\n                        still add functions with the more modern SDK V3 Import syntax by naming your\n                        function files with the .mjs extension. This will allow the code file to be\n                        treated as ES (ECMAScript). Asynchronous operations You'll see many code samples using callbacks and promises to handle the\n                        result of DynamoDB operations. With modern JavaScript this complexity is no\n                        longer needed and developers can take advantage of the more succinct and\n                        readable async/await syntax for asynchronous operations. Web browser runtime Web and mobile developers building with React or React Native can use the\n                        SDK for JavaScript in their projects. With the earlier V2 of the SDK, web\n                        developers would have to load the full SDK into the browser, referencing an\n                        SDK image hosted at https://sdk.amazonaws.com/js/. With V3, it's possible to bundle just the required V3 client modules and\n                        all required JavaScript functions into a single JavaScript file using\n                        Webpack, and add it in a script tag in the <head> of your\n                        HTML pages, as explained in the Getting started in a browser script section of the SDK\n                        documentation. DAX data plane operations The SDK for JavaScript V3 does not at this time provide support for the\n                        Amazon DynamoDB Streams Accelerator (DAX) data plane operations. If you request DAX\n                        support, consider using the SDK for JavaScript V2 which supports DAX data\n                        plane operations. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Programming with Python Programming with the AWS SDK for Java 2.x"}, {"title": "What is Amazon DynamoDB? - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AdditionalResources.KnowledgeCenter.html", "content": "What is Amazon DynamoDB? PDF RSS Amazon DynamoDB is a serverless, NoSQL, fully managed database with single-digit millisecond performance at any scale. DynamoDB addresses your needs to overcome scaling and operational complexities of relational databases. DynamoDB is purpose-built and optimized for operational\n        workloads that require consistent performance at any scale. For example, DynamoDB delivers consistent single-digit millisecond performance for a shopping cart \n        use case, whether you've 10 or 100 million users. Launched in 2012 , \n        DynamoDB continues to help you move away from relational databases while reducing cost and improving performance at scale. Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB scales to support tables of virtually any size while providing consistent single-digit millisecond performance and high availability. For events, such as Amazon Prime Day , DynamoDB powers multiple high-traffic Amazon properties and systems,\n        including Alexa , Amazon.com sites, and all Amazon fulfillment centers .\n        For such events, DynamoDB APIs have handled trillions of calls from Amazon properties and systems. DynamoDB continuously serves hundreds of customers with tables that have peak traffic of over half a million requests per second. It \n        also serves hundreds of customers whose table sizes exceed 200 TB, and processes over one billion requests per hour. Topics Characteristics of DynamoDB DynamoDB use cases Capabilities of DynamoDB Service integrations Security Resilience Accessing DynamoDB DynamoDB pricing Getting started with DynamoDB Characteristics of DynamoDB Serverless With DynamoDB, you don't need to provision any servers, or patch, manage, install, maintain, or operate any software. DynamoDB provides zero downtime maintenance. It has no versions (major, minor, or patch), and there are no maintenance windows. DynamoDB's on-demand capacity mode offers pay-as-you-go pricing for read and write requests so you only pay for what you use. With on-demand, DynamoDB instantly scales up or down your tables to adjust for \n                capacity and maintains performance with zero administration. It also scales down to zero so you don't pay for throughput when your table doesn't have traffic and there are no cold starts. NoSQL As a NoSQL database, DynamoDB is purpose-built to deliver improved performance, scalability, manageability, and flexibility compared to traditional relational databases. To support a wide variety of use cases, DynamoDB supports both key-value and document data models. Unlike relational databases, DynamoDB doesn't support a JOIN operator. We recommend that you denormalize your data model to reduce database round trips and processing power needed to answer queries. As a NoSQL database, DynamoDB provides strong read consistency and ACID transactions to build enterprise-grade applications. Fully managed As a fully managed database service, DynamoDB handles the undifferentiated heavy lifting of managing a database so that you can focus on building value for your customers. It handles setup, configurations, maintenance, high availability, hardware provisioning, security, backups,\n                monitoring, and more. This ensures that when you create a DynamoDB table, it's instantly ready for production workloads. DynamoDB constantly improves its availability, reliability, performance, security, and functionality without requiring upgrades or downtime. Single-digit millisecond performance at any scale DynamoDB was purpose-built to improve upon the performance and scalability of relational databases to deliver single-digit millisecond performance at any scale. To achieve this scale and performance, DynamoDB is optimized for high-performance workloads and provides APIs that encourage efficient \n                database usage. It omits features that are inefficient and non-performing at scale, for example, JOIN operations. DynamoDB delivers consistent single-digit millisecond performance for your application, whether you've 100 or 100 million users. DynamoDB use cases Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB is ideal for use cases that require consistent performance at any scale with little to zero operational overhead. The following list presents some use cases where you can use DynamoDB: Financial service applications \u2013 Suppose you're a financial services company building applications, such as live trading and routing, loan management, token generation, and transaction ledgers. With DynamoDB global tables , your applications can respond to events and serve traffic from your chosen AWS Regions with fast, local read and write performance. DynamoDB is suitable for applications with the most stringent availability requirements. It removes the operational burden of manually scaling instances for increased storage or throughput, versioning, and licensing. You can use DynamoDB transactions to achieve atomicity, consistency, isolation, and durability (ACID) across one or more tables with a single request. (ACID) transactions suit workloads that include processing financial transactions or fulfilling orders. DynamoDB instantly accommodates your workloads as they ramp up or down, enabling you to efficiently scale your database for market conditions, such as trading hours. Gaming applications \u2013 As a gaming company, you can use DynamoDB for all parts of game platforms, for example, game state, player data, session history, and leaderboards. Choose DynamoDB for its scale, consistent performance, and the ease of operations provided by its serverless architecture. DynamoDB is well suited for scale-out architectures needed to support successful games. It quickly scales your game\u2019s throughput both in and out (scale to zero with no cold start). This scalability optimizes your architecture's efficiency whether you\u2019re scaling out for peak traffic or scaling back when gameplay usage is low. Streaming applications \u2013 Media and entertainment companies use DynamoDB as a metadata index for content, content management service, or to serve near real-time sports statistics. They also use DynamoDB to run user watchlist and bookmarking services and process billions of daily customer events for generating recommendations. These customers benefit from DynamoDB's scalability, performance, and resiliency. DynamoDB scales to workload changes as they ramp up or down, enabling streaming media use cases that can support any levels of demand. To learn more about how customers from different industries use DynamoDB, see Amazon DynamoDB Customers and This is My Architecture . Capabilities of DynamoDB Multi-active replication with global tables Global tables provide multi-active replication of your data across your chosen AWS Regions with 99.999% availability . Global tables deliver a fully managed solution for deploying a multi-Region, multi-active database, without building and maintaining your own replication solution. With global tables, you can specify the AWS Regions where you want the tables to be available. DynamoDB replicates ongoing data changes to all of these tables. Your globally distributed applications can access data locally in your selected Regions to achieve single-digit millisecond read and write performance. Because global tables are multi-active, you don't need a primary table. This means there are no complicated or delayed fail-overs, or database downtime when failing over an application between Regions. ACID transactions DynamoDB is built for mission-critical workloads. It includes (ACID) transactions support for applications that require complex business logic. DynamoDB provides native, server-side support for transactions, simplifying the developer experience of making coordinated, all-or-nothing changes to multiple items within and across tables. Change data capture for event-driven architectures DynamoDB supports streaming of item-level change data capture (CDC) records in near-real time. It offers two streaming models for CDC: DynamoDB Streams and Kinesis Data Streams for DynamoDB . Whenever an application creates, updates, or deletes items in a table, streams records a time-ordered sequence of every item-level change in near-real time. This makes DynamoDB Streams ideal for applications with event-driven architecture to consume and act upon the changes. Secondary indexes DynamoDB offers the option to create both global and local secondary indexes , which let you query the table data using an alternate key. With these secondary indexes, you can access data with attributes other than the primary key, giving you maximum flexibility in accessing your data. Service integrations DynamoDB broadly integrates with several AWS services to help you get more value from your data, eliminate undifferentiated heavy lifting, and operate your workloads at scale. Some examples are: AWS CloudFormation, Amazon CloudWatch, Amazon S3, AWS Identity and Access Management (IAM), and AWS Auto Scaling. The following sections describe some of the service integrations that you can perform using DynamoDB: Serverless integrations To build end-to-end serverless applications, DynamoDB integrates natively with a number of serverless AWS services. For example, you can integrate DynamoDB with AWS Lambda to create triggers , which are pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build event-driven applications that react to data modifications in DynamoDB tables. \n                For cost optimization, you can filter events that Lambda processes from a DynamoDB stream. The following list presents some examples of serverless integrations with DynamoDB: AWS AppSync for creating GraphQL APIs Amazon API Gateway for creating REST APIs Lambda for serverless compute Amazon Kinesis Data Streams for change data capture (CDC) Importing and exporting data to Amazon S3 Integrating DynamoDB with Amazon S3 enables you to easily export data to an Amazon S3 bucket for analytics and machine learning. DynamoDB supports full table exports and incremental exports to export changed, updated, or deleted data \n                between a specified time period. You can also import data from Amazon S3 into a new DynamoDB table. Zero-ETL integration DynamoDB supports zero-ETL integration with Amazon Redshift and Amazon OpenSearch Service .\n                These integrations enable you to run complex analytics and use advanced search capabilities on your DynamoDB table data. For example, you can perform full-text and vector search, and semantic search on your DynamoDB data.\n                Zero-ETL integrations have no impact on production workloads running on DynamoDB. Caching DynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for DynamoDB. DAX delivers up to 10 times performance improvement \u2013 from milliseconds \n                to microseconds \u2013 even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring you to manage cache invalidation, data population, or cluster management. Security DynamoDB utilizes IAM to help you securely control access to your DynamoDB resources. With IAM, you can centrally \n            manage permissions that control which DynamoDB users can access resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. Because DynamoDB utilizes IAM, there are no user names or passwords for accessing DynamoDB.\n            Because you don't have any complicated password rotation policies to manage, it simplifies your security posture. With IAM, you can also enable fine-grained \n                access control to provide authorization at the attribute level. You can also define resource-based policies with support for IAM Access Analyzer and Block Public Access (BPA) to simplify policy management. By default, DynamoDB encrypts all customer data at rest. Encryption at rest enhances the security of your data by using encryption keys stored in AWS Key Management Service (AWS KMS).\n            With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements. When you access an encrypted table, DynamoDB decrypts the table data transparently. You don't have to change any code or applications to use or manage encrypted tables.\n            DynamoDB continues to deliver the same single-digit millisecond latency that you've come to expect, and all DynamoDB queries work seamlessly on your encrypted data. You can specify whether DynamoDB should use an AWS owned key (default encryption type), AWS managed key, or a Customer managed key to encrypt user data. The default encryption using AWS-owned KMS keys is available at no additional charge. For client-side encryption, you can use the AWS Database Encryption SDK . DynamoDB also adheres to several compliance standards , including HIPAA, PCI DSS, and GDPR, which enables you to meet regulatory requirements. Resilience By default, DynamoDB automatically replicates your data across three Availability Zones to provide high durability and a 99.99% availability SLA. DynamoDB also provides additional capabilities to help you achieve your business \n            continuity and disaster recovery objectives. DynamoDB includes the following features to help support your data resiliency and backup needs: Features Global tables Continuous backups and point-in-time recovery On-demand backup and restore Global tables DynamoDB global tables enable a 99.999% availability SLA and multi-Region resilience. This helps you build resilient applications and optimize them for the lowest recovery time objective (RTO) and recovery point objective (RPO). Global tables also\n                integrates with AWS Fault Injection Service (AWS FIS) to perform fault injection experiments on your global table workloads. For example, pausing global table replication to any replica table. Continuous backups and point-in-time recovery Continuous backups provide you per-second granularity and the ability to initiate a point-in-time recovery. With point-in-time recovery, you can restore a table to any point in time up to the second during the last 35 days. Continuous backups and initiating a point-in-time restore doesn't use provisioned capacity. They also don't have any impact on the performance or availability of your applications. On-demand backup and restore On-demand backup and restore let you create full backups of a table for long-term retention and archival for regulatory compliance needs. Backups don't impact the performance of your table and you can back up tables of any size. With AWS Backup \n                integration , you can use AWS Backup to schedule, copy, tag, and manage the life cycle of your DynamoDB on-demand backups automatically. Using AWS Backup, you can copy on-demand backups across accounts and Regions, and transition older backups to cold storage for cost-optimization. Accessing DynamoDB You can work with DynamoDB using the AWS Management Console , the AWS Command Line Interface , NoSQL Workbench for DynamoDB , or DynamoDB APIs . For more information, see Accessing DynamoDB . DynamoDB pricing DynamoDB charges for reading, writing, and storing data in your tables, along with any optional features you choose to enable. DynamoDB has two capacity modes with their respective billing options for processing reads and writes on your tables: on-demand and provisioned . DynamoDB also offers a free tier that provides 25 GB of storage. The free tier also includes 25 provisioned Write and 25 provisioned Read Capacity Units (WCU, RCU) which is enough to handle 200 M requests per month. For more information, see Amazon DynamoDB pricing . Getting started with DynamoDB If you're a first-time user of DynamoDB, we recommend that you begin by reading the following topics: Getting started with DynamoDB \u2013 Walks you through the process of setting up DynamoDB, creating sample tables, and uploading data. This topic also provides information about performing some basic database operations using the AWS Management Console, AWS CLI, NoSQL Workbench, and DynamoDB APIs. DynamoDB core components \u2013 Describes the basic DynamoDB concepts. Best practices for designing and architecting with\n      DynamoDB \u2013 Provides recommendations about  NoSQL design, DynamoDB Well-Architected Lens, table design and several other DynamoDB features. These best practices help you maximize performance and minimize throughput costs when working with DynamoDB. We also recommend that you review the following tutorials that present complete end-to-end procedures to familiarize yourself with DynamoDB. You can complete these tutorials with the free tier of AWS. Create and Query a NoSQL Table with Amazon DynamoDB Build an Application Using a NoSQL Key-Value Data Store For information about resources, tools, and strategies to migrate to DynamoDB, see Migrating to DynamoDB . To read the latest blogs and whitepapers, see Amazon DynamoDB resources . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Getting started with DynamoDB"}, {"title": "What is Amazon DynamoDB? - Amazon DynamoDB", "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AdditionalResources.PrescriptiveGuidance.html", "content": "What is Amazon DynamoDB? PDF RSS Amazon DynamoDB is a serverless, NoSQL, fully managed database with single-digit millisecond performance at any scale. DynamoDB addresses your needs to overcome scaling and operational complexities of relational databases. DynamoDB is purpose-built and optimized for operational\n        workloads that require consistent performance at any scale. For example, DynamoDB delivers consistent single-digit millisecond performance for a shopping cart \n        use case, whether you've 10 or 100 million users. Launched in 2012 , \n        DynamoDB continues to help you move away from relational databases while reducing cost and improving performance at scale. Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB scales to support tables of virtually any size while providing consistent single-digit millisecond performance and high availability. For events, such as Amazon Prime Day , DynamoDB powers multiple high-traffic Amazon properties and systems,\n        including Alexa , Amazon.com sites, and all Amazon fulfillment centers .\n        For such events, DynamoDB APIs have handled trillions of calls from Amazon properties and systems. DynamoDB continuously serves hundreds of customers with tables that have peak traffic of over half a million requests per second. It \n        also serves hundreds of customers whose table sizes exceed 200 TB, and processes over one billion requests per hour. Topics Characteristics of DynamoDB DynamoDB use cases Capabilities of DynamoDB Service integrations Security Resilience Accessing DynamoDB DynamoDB pricing Getting started with DynamoDB Characteristics of DynamoDB Serverless With DynamoDB, you don't need to provision any servers, or patch, manage, install, maintain, or operate any software. DynamoDB provides zero downtime maintenance. It has no versions (major, minor, or patch), and there are no maintenance windows. DynamoDB's on-demand capacity mode offers pay-as-you-go pricing for read and write requests so you only pay for what you use. With on-demand, DynamoDB instantly scales up or down your tables to adjust for \n                capacity and maintains performance with zero administration. It also scales down to zero so you don't pay for throughput when your table doesn't have traffic and there are no cold starts. NoSQL As a NoSQL database, DynamoDB is purpose-built to deliver improved performance, scalability, manageability, and flexibility compared to traditional relational databases. To support a wide variety of use cases, DynamoDB supports both key-value and document data models. Unlike relational databases, DynamoDB doesn't support a JOIN operator. We recommend that you denormalize your data model to reduce database round trips and processing power needed to answer queries. As a NoSQL database, DynamoDB provides strong read consistency and ACID transactions to build enterprise-grade applications. Fully managed As a fully managed database service, DynamoDB handles the undifferentiated heavy lifting of managing a database so that you can focus on building value for your customers. It handles setup, configurations, maintenance, high availability, hardware provisioning, security, backups,\n                monitoring, and more. This ensures that when you create a DynamoDB table, it's instantly ready for production workloads. DynamoDB constantly improves its availability, reliability, performance, security, and functionality without requiring upgrades or downtime. Single-digit millisecond performance at any scale DynamoDB was purpose-built to improve upon the performance and scalability of relational databases to deliver single-digit millisecond performance at any scale. To achieve this scale and performance, DynamoDB is optimized for high-performance workloads and provides APIs that encourage efficient \n                database usage. It omits features that are inefficient and non-performing at scale, for example, JOIN operations. DynamoDB delivers consistent single-digit millisecond performance for your application, whether you've 100 or 100 million users. DynamoDB use cases Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB is ideal for use cases that require consistent performance at any scale with little to zero operational overhead. The following list presents some use cases where you can use DynamoDB: Financial service applications \u2013 Suppose you're a financial services company building applications, such as live trading and routing, loan management, token generation, and transaction ledgers. With DynamoDB global tables , your applications can respond to events and serve traffic from your chosen AWS Regions with fast, local read and write performance. DynamoDB is suitable for applications with the most stringent availability requirements. It removes the operational burden of manually scaling instances for increased storage or throughput, versioning, and licensing. You can use DynamoDB transactions to achieve atomicity, consistency, isolation, and durability (ACID) across one or more tables with a single request. (ACID) transactions suit workloads that include processing financial transactions or fulfilling orders. DynamoDB instantly accommodates your workloads as they ramp up or down, enabling you to efficiently scale your database for market conditions, such as trading hours. Gaming applications \u2013 As a gaming company, you can use DynamoDB for all parts of game platforms, for example, game state, player data, session history, and leaderboards. Choose DynamoDB for its scale, consistent performance, and the ease of operations provided by its serverless architecture. DynamoDB is well suited for scale-out architectures needed to support successful games. It quickly scales your game\u2019s throughput both in and out (scale to zero with no cold start). This scalability optimizes your architecture's efficiency whether you\u2019re scaling out for peak traffic or scaling back when gameplay usage is low. Streaming applications \u2013 Media and entertainment companies use DynamoDB as a metadata index for content, content management service, or to serve near real-time sports statistics. They also use DynamoDB to run user watchlist and bookmarking services and process billions of daily customer events for generating recommendations. These customers benefit from DynamoDB's scalability, performance, and resiliency. DynamoDB scales to workload changes as they ramp up or down, enabling streaming media use cases that can support any levels of demand. To learn more about how customers from different industries use DynamoDB, see Amazon DynamoDB Customers and This is My Architecture . Capabilities of DynamoDB Multi-active replication with global tables Global tables provide multi-active replication of your data across your chosen AWS Regions with 99.999% availability . Global tables deliver a fully managed solution for deploying a multi-Region, multi-active database, without building and maintaining your own replication solution. With global tables, you can specify the AWS Regions where you want the tables to be available. DynamoDB replicates ongoing data changes to all of these tables. Your globally distributed applications can access data locally in your selected Regions to achieve single-digit millisecond read and write performance. Because global tables are multi-active, you don't need a primary table. This means there are no complicated or delayed fail-overs, or database downtime when failing over an application between Regions. ACID transactions DynamoDB is built for mission-critical workloads. It includes (ACID) transactions support for applications that require complex business logic. DynamoDB provides native, server-side support for transactions, simplifying the developer experience of making coordinated, all-or-nothing changes to multiple items within and across tables. Change data capture for event-driven architectures DynamoDB supports streaming of item-level change data capture (CDC) records in near-real time. It offers two streaming models for CDC: DynamoDB Streams and Kinesis Data Streams for DynamoDB . Whenever an application creates, updates, or deletes items in a table, streams records a time-ordered sequence of every item-level change in near-real time. This makes DynamoDB Streams ideal for applications with event-driven architecture to consume and act upon the changes. Secondary indexes DynamoDB offers the option to create both global and local secondary indexes , which let you query the table data using an alternate key. With these secondary indexes, you can access data with attributes other than the primary key, giving you maximum flexibility in accessing your data. Service integrations DynamoDB broadly integrates with several AWS services to help you get more value from your data, eliminate undifferentiated heavy lifting, and operate your workloads at scale. Some examples are: AWS CloudFormation, Amazon CloudWatch, Amazon S3, AWS Identity and Access Management (IAM), and AWS Auto Scaling. The following sections describe some of the service integrations that you can perform using DynamoDB: Serverless integrations To build end-to-end serverless applications, DynamoDB integrates natively with a number of serverless AWS services. For example, you can integrate DynamoDB with AWS Lambda to create triggers , which are pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build event-driven applications that react to data modifications in DynamoDB tables. \n                For cost optimization, you can filter events that Lambda processes from a DynamoDB stream. The following list presents some examples of serverless integrations with DynamoDB: AWS AppSync for creating GraphQL APIs Amazon API Gateway for creating REST APIs Lambda for serverless compute Amazon Kinesis Data Streams for change data capture (CDC) Importing and exporting data to Amazon S3 Integrating DynamoDB with Amazon S3 enables you to easily export data to an Amazon S3 bucket for analytics and machine learning. DynamoDB supports full table exports and incremental exports to export changed, updated, or deleted data \n                between a specified time period. You can also import data from Amazon S3 into a new DynamoDB table. Zero-ETL integration DynamoDB supports zero-ETL integration with Amazon Redshift and Amazon OpenSearch Service .\n                These integrations enable you to run complex analytics and use advanced search capabilities on your DynamoDB table data. For example, you can perform full-text and vector search, and semantic search on your DynamoDB data.\n                Zero-ETL integrations have no impact on production workloads running on DynamoDB. Caching DynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for DynamoDB. DAX delivers up to 10 times performance improvement \u2013 from milliseconds \n                to microseconds \u2013 even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring you to manage cache invalidation, data population, or cluster management. Security DynamoDB utilizes IAM to help you securely control access to your DynamoDB resources. With IAM, you can centrally \n            manage permissions that control which DynamoDB users can access resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. Because DynamoDB utilizes IAM, there are no user names or passwords for accessing DynamoDB.\n            Because you don't have any complicated password rotation policies to manage, it simplifies your security posture. With IAM, you can also enable fine-grained \n                access control to provide authorization at the attribute level. You can also define resource-based policies with support for IAM Access Analyzer and Block Public Access (BPA) to simplify policy management. By default, DynamoDB encrypts all customer data at rest. Encryption at rest enhances the security of your data by using encryption keys stored in AWS Key Management Service (AWS KMS).\n            With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements. When you access an encrypted table, DynamoDB decrypts the table data transparently. You don't have to change any code or applications to use or manage encrypted tables.\n            DynamoDB continues to deliver the same single-digit millisecond latency that you've come to expect, and all DynamoDB queries work seamlessly on your encrypted data. You can specify whether DynamoDB should use an AWS owned key (default encryption type), AWS managed key, or a Customer managed key to encrypt user data. The default encryption using AWS-owned KMS keys is available at no additional charge. For client-side encryption, you can use the AWS Database Encryption SDK . DynamoDB also adheres to several compliance standards , including HIPAA, PCI DSS, and GDPR, which enables you to meet regulatory requirements. Resilience By default, DynamoDB automatically replicates your data across three Availability Zones to provide high durability and a 99.99% availability SLA. DynamoDB also provides additional capabilities to help you achieve your business \n            continuity and disaster recovery objectives. DynamoDB includes the following features to help support your data resiliency and backup needs: Features Global tables Continuous backups and point-in-time recovery On-demand backup and restore Global tables DynamoDB global tables enable a 99.999% availability SLA and multi-Region resilience. This helps you build resilient applications and optimize them for the lowest recovery time objective (RTO) and recovery point objective (RPO). Global tables also\n                integrates with AWS Fault Injection Service (AWS FIS) to perform fault injection experiments on your global table workloads. For example, pausing global table replication to any replica table. Continuous backups and point-in-time recovery Continuous backups provide you per-second granularity and the ability to initiate a point-in-time recovery. With point-in-time recovery, you can restore a table to any point in time up to the second during the last 35 days. Continuous backups and initiating a point-in-time restore doesn't use provisioned capacity. They also don't have any impact on the performance or availability of your applications. On-demand backup and restore On-demand backup and restore let you create full backups of a table for long-term retention and archival for regulatory compliance needs. Backups don't impact the performance of your table and you can back up tables of any size. With AWS Backup \n                integration , you can use AWS Backup to schedule, copy, tag, and manage the life cycle of your DynamoDB on-demand backups automatically. Using AWS Backup, you can copy on-demand backups across accounts and Regions, and transition older backups to cold storage for cost-optimization. Accessing DynamoDB You can work with DynamoDB using the AWS Management Console , the AWS Command Line Interface , NoSQL Workbench for DynamoDB , or DynamoDB APIs . For more information, see Accessing DynamoDB . DynamoDB pricing DynamoDB charges for reading, writing, and storing data in your tables, along with any optional features you choose to enable. DynamoDB has two capacity modes with their respective billing options for processing reads and writes on your tables: on-demand and provisioned . DynamoDB also offers a free tier that provides 25 GB of storage. The free tier also includes 25 provisioned Write and 25 provisioned Read Capacity Units (WCU, RCU) which is enough to handle 200 M requests per month. For more information, see Amazon DynamoDB pricing . Getting started with DynamoDB If you're a first-time user of DynamoDB, we recommend that you begin by reading the following topics: Getting started with DynamoDB \u2013 Walks you through the process of setting up DynamoDB, creating sample tables, and uploading data. This topic also provides information about performing some basic database operations using the AWS Management Console, AWS CLI, NoSQL Workbench, and DynamoDB APIs. DynamoDB core components \u2013 Describes the basic DynamoDB concepts. Best practices for designing and architecting with\n      DynamoDB \u2013 Provides recommendations about  NoSQL design, DynamoDB Well-Architected Lens, table design and several other DynamoDB features. These best practices help you maximize performance and minimize throughput costs when working with DynamoDB. We also recommend that you review the following tutorials that present complete end-to-end procedures to familiarize yourself with DynamoDB. You can complete these tutorials with the free tier of AWS. Create and Query a NoSQL Table with Amazon DynamoDB Build an Application Using a NoSQL Key-Value Data Store For information about resources, tools, and strategies to migrate to DynamoDB, see Migrating to DynamoDB . To read the latest blogs and whitepapers, see Amazon DynamoDB resources . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Getting started with DynamoDB"}, {"title": "Amazon DynamoDB Documentation", "url": "https://docs.aws.amazon.com/dynamodb/?icmpid=docs_homepage_featuredsvcs#dynamodb-resources", "content": "No main content found."}, {"title": "Amazon DynamoDB Documentation", "url": "https://docs.aws.amazon.com/dynamodb/?icmpid=docs_homepage_featuredsvcs#programming-guides-for-dynamodb", "content": "No main content found."}, {"title": "Amazon DynamoDB Documentation", "url": "https://docs.aws.amazon.com/dynamodb/?icmpid=docs_homepage_featuredsvcs#additional-dynamodb-resources", "content": "No main content found."}, {"title": "Amazon RDS and Aurora Documentation", "url": "https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=RDS&topic_url=https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "What is Amazon Relational Database Service (Amazon RDS)? - Amazon Relational Database Service", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/index.html", "content": "What is Amazon Relational Database Service (Amazon RDS)? PDF RSS Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a\n\t\trelational database in the AWS Cloud. It provides cost-efficient, resizable capacity for an\n\t\tindustry-standard relational database and manages common database administration tasks. Note This guide covers Amazon RDS database engines other than Amazon Aurora. For information about using Amazon Aurora, see the Amazon Aurora User Guide . If you are new to AWS products and services, begin learning more with the following resources: For an overview of all AWS products, see What is cloud\n\t\t\t\t\tcomputing? Amazon Web Services provides a number of database services. To learn more about the variety of database options available on \n\t\t\t\tAWS, see Choosing an\n\t\t\t\t\tAWS database service and Running databases on AWS . Advantages of Amazon RDS Amazon RDS is a managed database service. It's responsible for most management tasks. By\n\t\t\teliminating tedious manual processes, Amazon RDS frees you to focus on your application and\n\t\t\tyour users. Amazon RDS provides the following principal advantages over database deployments that aren't\n\t\t\tfully managed: You can use database engines that you are already familiar with: IBM Db2, MariaDB,\n\t\t\t\t\tMicrosoft SQL Server, MySQL, Oracle Database, and PostgreSQL. Amazon RDS manages backups, software patching, automatic failure detection, and\n\t\t\t\t\trecovery. You can turn on automated backups, or manually create your own backup\n\t\t\t\t\tsnapshots. You can use these backups to restore a database. The Amazon RDS restore\n\t\t\t\t\tprocess works reliably and efficiently. You can get high availability with a primary DB instance and a synchronous secondary\n\t\t\t\t\tDB instance that you can fail over to when problems occur. You can also use read\n\t\t\t\t\treplicas to increase read scaling. In addition to the security in your database package, you can control access by using\n\t\t\t\t\tAWS Identity and Access Management (IAM) to define users and permissions. You can also help protect\n\t\t\t\t\tyour databases by putting them in a virtual private cloud (VPC). Comparison of responsibilities with Amazon EC2\n\t\t\t\tand on-premises deployments We recommend Amazon RDS as your default choice for most relational database deployments.\n\t\t\tThe following alternatives have the disadvantage of making you spend more time managing\n\t\t\tsoftware and hardware: On-premises deployment When you buy an on-premises server, you get CPU, memory, storage, and\n\t\t\t\t\t\tIOPS, all bundled together. You assume full responsibility for the server,\n\t\t\t\t\t\toperating system, and database software. Amazon EC2 Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the AWS Cloud.\n\t\t\t\t\t\tUnlike in an on-premises server, CPU, memory, storage, and IOPS are\n\t\t\t\t\t\tseparated so that you can scale them independently. AWS manages the\n\t\t\t\t\t\thardware layers, which eliminates some of the burden of managing an\n\t\t\t\t\t\ton-premises database server. The disadvantage to running a database on Amazon EC2 is that you're more prone to user errors.\n\t\t\t\t\t\tFor example, when you update the operating system or database software\n\t\t\t\t\t\tmanually, you might accidentally cause application downtime. You might spend\n\t\t\t\t\t\thours checking every change to identify and fix an issue. The following table compares the management models for on-premises databases, Amazon EC2,\n\t\t\tand Amazon RDS. Feature On-premises management Amazon EC2 management Amazon RDS management Application optimization Customer Customer Customer Scaling Customer Customer AWS High availability Customer Customer AWS Database backups Customer Customer AWS Database software patching Customer Customer AWS Database software install Customer Customer AWS Operating system (OS) patching Customer Customer AWS OS installation Customer Customer AWS Server maintenance Customer AWS AWS Hardware lifecycle Customer AWS AWS Power, network, and cooling Customer AWS AWS Amazon RDS shared responsibility\n\t\t\tmodel Amazon RDS is responsible for hosting the software components and infrastructure of DB instances and\n\t\tDB clusters. You are responsible for query tuning, which is the process of adjusting SQL queries\n\t\tto improve performance. Query performance is highly dependent on database design, data size,\n\t\tdata distribution, application workload, and query patterns, which can vary greatly.\n\t\tMonitoring and tuning are highly individualized processes that you own for your RDS\n\t\tdatabases. You can use Amazon RDS Performance Insights and other tools to identify problematic\n\t\tqueries. Amazon RDS DB instances A DB instance is an isolated database environment in the\n\t\tAWS Cloud. The basic building block of Amazon RDS is the DB instance. Your DB instance can contain one or\n\t\tmore user-created databases. The following diagram shows a virtual private cloud (VPC) that\n\t\tcontains two Availability Zones, with each AZ containing two DB instances. You can access your DB instances by using the same tools and applications that you use with a\n\t\tstandalone database instance. You can create and modify a DB instance by using the AWS Command Line Interface\n\t\t(AWS CLI), the Amazon RDS API, or the AWS Management Console. Topics Amazon RDS application\n\t\t\t\tarchitecture: example DB engines DB instance classes DB instance storage DB instances in an Amazon Virtual Private Cloud (Amazon VPC) Amazon RDS application\n\t\t\t\tarchitecture: example The following image shows a typical use case of a dynamic website that uses Amazon RDS\n\t\t\tDB instances for database storage: The primary components of the preceding architecture are as follows: Elastic Load Balancing AWS routes user traffic through Elastic Load Balancing. A load balancer\n\t\t\t\t\t\tdistributes workloads across multiple compute resources, such as virtual\n\t\t\t\t\t\tservers. In this sample use case, the Elastic Load Balancer forwards client\n\t\t\t\t\t\trequests to application servers. Application servers Application servers interact with RDS DB instances. An application server in\n\t\t\t\t\t\tAWS is typically hosted on EC2 instances, which provide scalable computing\n\t\t\t\t\t\tcapacity. The application servers reside in public subnets with different\n\t\t\t\t\t\tAvailability Zones (AZs) within the same Virtual Private Cloud (VPC).\n\t\t\t\t\t\t. RDS DB instances The EC2 application servers interact with RDS DB instances. The DB instances reside in\n\t\t\t\t\t\tprivate subnets within different Availability Zones (AZs) within the same\n\t\t\t\t\t\tVirtual Private Cloud (VPC). Because the subnets are private, no requests\n\t\t\t\t\t\tfrom the internet are permitted. The primary DB instance replicates to another DB instance, called a read\n\t\t\t\t\t\t\treplica . Both DB instances are in private subnets within the VPC,\n\t\t\t\t\t\twhich means that Internet users can't access them directly. DB engines A DB engine is the specific relational database software\n\t\t\tthat runs on your DB instance. Amazon RDS supports the following database engines: IBM Db2 For more information, see Amazon RDS for Db2 . MariaDB For more information, see Amazon RDS for MariaDB . Microsoft SQL Server For more information, see Amazon RDS for Microsoft SQL Server . MySQL For more information, see Amazon RDS for MySQL . Oracle Database For more information, see Amazon RDS for Oracle . PostgreSQL For more information, see Amazon RDS for PostgreSQL . Each DB engine has its own supported features, and each version of a DB engine can include\n\t\t\tspecific features. Support for Amazon RDS features varies across AWS Regions and specific\n\t\t\tversions of each DB engine. To check feature support in different engine versions and\n\t\t\tRegions, see Supported features in Amazon RDS by AWS Region and DB engine . Additionally, each DB engine has a set of parameters in a DB parameter group that control the behavior of the\n\t\t\tdatabases that it manages. For more information about parameter groups, see Parameter groups for Amazon RDS . DB instance classes A DB instance class determines the computation and memory\n\t\t\tcapacity of a DB instance. A DB instance class consists of both the DB instance class type and the size.\n\t\t\tAmazon RDS supports the following instance class types, where the asterisk (*) represents the\n\t\t\tgeneration, optional attribute, and size: General purpose \u2013 db.m* Memory optimized \u2013 db.z*, db.x*, db.r* Compute optimized \u2013 db.c* Burstable performance \u2013 db.t* Each instance class offers different compute, memory, and storage capabilities. For example,\n\t\t\tdb.m7g is a 7th-generation, general-purpose DB instance class type powered by AWS Graviton3\n\t\t\tprocessors. When you create a DB instance, you specify a DB instance class such as\n\t\t\tdb.m7g.2xlarge, where 2xlarge is the size. For more information about the hardware\n\t\t\tspecifications for the different instance classes, see Hardware specifications for DB instance\n                classes . You can select the DB instance class that best meets your requirements. If your requirements\n\t\t\tchange over time, you can change your DB instance class. For example, you might scale up your\n\t\t\tdb.m7g.2xlarge instance to db.m7g.4xlarge. For more information, see DB instance classes . Note For pricing information on DB instance classes, see the Pricing section of the Amazon RDS product page. DB instance storage Amazon EBS provides durable, block-level storage volumes that you can attach to a running\n\t\t\tinstance. DB instance storage comes in the following types: General Purpose (SSD) This cost-effective storage type is ideal for a broad range of workloads\n\t\t\t\t\trunning on medium-sized DB instances. General Purpose storage is best suited for\n\t\t\t\t\tdevelopment and testing environments. Provisioned IOPS (PIOPS) This storage type is designed to meet the needs of I/O-intensive workloads,\n\t\t\t\t\tparticularly database workloads, that require low I/O latency and consistent I/O\n\t\t\t\t\tthroughput. Provisioned IOPS storage is best suited for production\n\t\t\t\t\tenvironments. Magnetic Amazon RDS supports magnetic storage for backward compatibility. We recommend\n\t\t\t\t\tthat you use General Purpose SSD or Provisioned IOPS SSD for any new storage\n\t\t\t\t\tneeds. The storage types differ in performance characteristics and price. You can tailor your\n\t\t\tstorage performance and cost to the requirements of your database. Each DB instance has minimum and maximum storage requirements depending on the storage type and\n\t\t\tthe database engine it supports. It's important to have sufficient storage so that\n\t\t\tyour databases have room to grow. Also, sufficient storage makes sure that features for\n\t\t\tthe DB engine have room to write content or log entries. For more information, see Amazon RDS DB instance storage . DB instances in an Amazon Virtual Private Cloud (Amazon VPC) You can run a DB instance on a virtual private cloud (VPC) using the Amazon Virtual Private Cloud (Amazon VPC) service.\n\t\t\tWhen you use a VPC, you have control over your virtual networking environment. You can\n\t\t\tchoose your own IP address range, create subnets, and configure routing and access\n\t\t\tcontrol lists. The basic functionality of Amazon RDS is the same whether it's running in a VPC or\n\t\t\tnot. Amazon RDS manages backups, software patching, automatic failure detection, and\n\t\t\trecovery. There's no additional cost to run your DB instance in a VPC. For more\n\t\t\tinformation on using Amazon VPC with RDS, see Amazon VPC and Amazon RDS . Amazon RDS uses Network Time Protocol (NTP) to synchronize the time on DB instances. AWS Regions and Availability Zones Amazon cloud computing resources are housed in highly available data center facilities in\n\t\tdifferent areas of the world (for example, North America, Europe, or Asia). Each data center\n\t\tlocation is called an AWS Region. With Amazon RDS, you can create your DB instances in multiple\n\t\tRegions. The following scenario shows an RDS DB instance in one Region that replicates asynchronously to a\n\t\tstandby DB instance in a different Region. If one Region becomes unavailable, the instance in the\n\t\tother Region is still available. Availability Zones Each AWS Region contains multiple distinct locations called Availability Zones, or AZs.\n\t\t\tEach Availability Zone is engineered to be isolated from failures in other Availability\n\t\t\tZones. Each is engineered to provide inexpensive, low-latency network connectivity to\n\t\t\tother Availability Zones in the same AWS Region. By launching DB instances  in separate\n\t\t\tAvailability Zones, you can protect your applications from the failure of a single\n\t\t\tlocation. For more information, see Regions, Availability Zones, and Local Zones . Multi-AZ deployments You can run your DB instance in several Availability Zones, an option called a Multi-AZ deployment. When you\n\t\t\tchoose this option, Amazon automatically provisions and maintains one or more secondary\n\t\t\tstandby DB instances in a different AZ. Your primary DB instance is replicated across Availability\n\t\t\tZones to each secondary DB instance. A Multi-AZ deployment provides the following advantages: Providing data redundancy and failover support Eliminating I/O freezes Minimizing latency spikes during system backups Serving read traffic on secondary DB instances (Multi-AZ DB clusters deployment only) The following diagram depicts a Multi-AZ DB instance deployment, where Amazon RDS automatically\n\t\t\tprovisions and maintains a synchronous standby replica in a different Availability Zone.\n\t\t\tThe replica database doesn't serve read traffic. The following diagram depicts a Multi-AZ DB cluster deployment, which has a writer\n\t\t\tDB instance and two reader DB instances in three separate Availability Zones in the same\n\t\t\tAWS Region. All three DB instances can serve read traffic. For more information, see Configuring and managing a Multi-AZ deployment for Amazon RDS . Access control with security groups A security group controls the access to a DB instance by\n\t\tallowing access to IP address ranges or Amazon EC2 instances that you specify. You can apply a\n\t\tsecurity group to one or more DB instances. A common use of a DB instance in a VPC is to share data with an application server in the same VPC.\n\t\tThe following example uses VPC security group ec2-rds- x to define inbound rules that use the IP addresses of the client\n\t\tapplication as the source. The application server belongs to this security group. A second\n\t\tsecurity group named rds-ec2- x specifies ec2-rds- x as the source and attaches\n\t\tto an RDS DB instance. According to the security group rules, client applications can't directly\n\t\taccess the DB instance, but the EC2 instance can access the DB instance. For more information about security groups, see Security in Amazon RDS . Amazon RDS monitoring Monitoring is an important part of maintaining the reliability, availability, and\n\t\tperformance of Amazon RDS and your other AWS solutions. AWS provides various monitoring\n\t\ttools to watch Amazon RDS, report when something is wrong, and take automatic actions when\n\t\tappropriate. You can track the performance and health of your DB instances using various automated and manual\n\t\ttools: Amazon RDS DB instance status and recommendations View details about the current status of your instance by using the Amazon RDS console, AWS CLI, or RDS API. You can\n\t\t\t\t\talso respond to automated recommendations for database resources, such as\n\t\t\t\t\tDB instances, read replicas, and DB parameter groups. For\n\t\t\t\t\tmore information, see Recommendations from Amazon RDS . Amazon CloudWatch metrics for Amazon RDS You can use the Amazon CloudWatch service to monitor the performance and health of a\n\t\t\t\t\tDB instance. CloudWatch performance charts are shown in the Amazon RDS console. Amazon RDS\n\t\t\t\t\tautomatically sends metrics to CloudWatch every minute for each active database. You\n\t\t\t\t\tdon't get additional charges for Amazon RDS metrics in CloudWatch. Using Amazon CloudWatch alarms, you can watch a single Amazon RDS metric over a specific\n\t\t\t\t\ttime period. You can then perform one or more actions based on the value of the\n\t\t\t\t\tmetric relative to a threshold that you set. For more information, see Monitoring Amazon RDS metrics with Amazon CloudWatch . Amazon RDS Performance Insights and operating-system monitoring Performance Insights assesses the load on your database, and determine when\n\t\t\t\t\tand where to take action. For more information, see Monitoring DB load with Performance Insights on Amazon RDS . Amazon RDS\n\t\t\t\t\tEnhanced Monitoring looks at metrics in real time for the operating system. For\n\t\t\t\t\tmore information, see Monitoring OS metrics with Enhanced Monitoring . Integrated AWS services Amazon RDS is integrated with Amazon EventBridge, Amazon CloudWatch Logs, and Amazon DevOps\u00a0Guru. For more\n\t\t\t\t\tinformation, see Monitoring metrics in an Amazon RDS instance . User interfaces to Amazon RDS You can interact with Amazon RDS in multiple ways. Topics AWS Management Console Command line interface Amazon RDS APIs AWS Management Console The AWS Management Console is a simple web-based user interface. You can manage your DB instances from the console \n\t\t\twith no programming required. To access the Amazon RDS console, sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/ . Command line interface You can use the AWS Command Line Interface (AWS CLI) to access the Amazon RDS API interactively.\n\t\t\tTo install the AWS CLI, see Installing the AWS Command Line Interface .\n\t\t\tTo begin using the AWS CLI for RDS, see AWS Command Line Interface reference for Amazon RDS . Amazon RDS APIs If you are a developer, you can access the Amazon RDS programmatically using APIs. For more\n\t\t\tinformation, see Amazon RDS API reference . For application development, we recommend that you use one of the AWS Software\n\t\t\tDevelopment Kits (SDKs). The AWS SDKs handle low-level details such as\n\t\t\tauthentication, retry logic, and error handling, so that you can focus on your\n\t\t\tapplication logic. AWS SDKs are available for a wide variety of languages. For\n\t\t\tmore information, see Tools for Amazon web\n\t\t\t\tservices . AWS also provides libraries, sample code, tutorials, and other resources to help you get\n\t\t\tstarted more easily. For more information, see Sample code & libraries . How you are charged for Amazon RDS When you use Amazon RDS, you can choose to use on-demand DB instances or reserved DB instances.\n\t\tFor more information, see DB instance billing for Amazon RDS . For Amazon RDS pricing information, see the Amazon RDS product page . What's next? The preceding section introduced you to the basic infrastructure components that RDS offers.\n\t\tWhat should you do next? Getting started Create a DB instance using instructions in Getting started with Amazon RDS . Topics specific to database\n\t\t\t\tengines You can review information specific to a particular DB engine in the following sections: Amazon RDS for Db2 Amazon RDS for MariaDB Amazon RDS for Microsoft SQL Server Amazon RDS for MySQL Amazon RDS for Oracle Amazon RDS for PostgreSQL Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions DB instances"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-ug.pdf", "content": "No main content found."}, {"title": "What is Amazon Aurora? - Amazon Aurora", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/index.html", "content": "What is Amazon Aurora? PDF RSS Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. You already know\n        how MySQL and PostgreSQL combine the speed and reliability of high-end commercial databases with the simplicity and\n        cost-effectiveness of open-source databases. The code, tools, and applications you use today with your existing MySQL and PostgreSQL\n        databases can be used with Aurora. With some workloads, Aurora can deliver up to five times the throughput of MySQL and up to three\n        times the throughput of PostgreSQL without requiring changes to most of your existing applications. Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take\n        advantage of that fast distributed storage. The underlying storage grows automatically as needed. An Aurora cluster volume can grow\n        to a maximum size of 128 tebibytes (TiB). Aurora also automates and standardizes database clustering and replication, which are\n        typically among the most challenging aspects of database configuration and administration. Aurora is part of the managed database service Amazon Relational Database Service (Amazon RDS). Amazon RDS makes it easier to set up, operate, and\n      scale a relational database in the cloud. If you are not already familiar with Amazon RDS, see the Amazon Relational Database Service User Guide . To learn more about the\n      variety of database options available on Amazon Web Services, see Choosing the right \n      database for your organization on AWS . Topics Amazon RDS shared responsibility model How Amazon Aurora works with Amazon RDS Amazon Aurora DB clusters Amazon Aurora versions Regions and\n                Availability Zones Supported features in\n            Amazon Aurora by AWS Region and Aurora DB engine Amazon Aurora endpoint connections Amazon Aurora DB instance classes Amazon Aurora storage Amazon Aurora reliability Amazon Aurora security High availability for Amazon Aurora Replication with Amazon Aurora DB instance billing for Aurora Amazon RDS shared responsibility model Amazon RDS is responsible for hosting the software components and infrastructure of DB instances and DB \n            clusters. You are responsible for query tuning, which is the process of adjusting SQL queries to improve \n            performance. Query performance is highly dependent on database design, data size, data distribution, \n            application workload, and query patterns, which can vary greatly. Monitoring and tuning are highly \n            individualized processes that you own for your RDS databases. You can use Amazon RDS Performance Insights and other tools to \n            identify problematic queries. How Amazon Aurora works with Amazon RDS The following points illustrate how Amazon Aurora relates to the standard MySQL and PostgreSQL engines available in Amazon RDS: You choose Aurora MySQL or Aurora PostgreSQL as the DB engine option when setting up new database servers through\n                    Amazon RDS. Aurora takes advantage of the familiar Amazon Relational Database Service (Amazon RDS) features for management and administration. Aurora uses the\n                    Amazon RDS AWS Management Console interface, AWS CLI commands, and API operations to handle routine database tasks such as provisioning,\n                    patching, backup, recovery, failure detection, and repair. Aurora management operations typically involve entire clusters of database servers that are synchronized through\n                    replication, instead of individual database instances. The automatic clustering, replication, and storage allocation\n                    make it simple and cost-effective to set up, operate, and scale your largest MySQL and PostgreSQL deployments. You can bring data from Amazon RDS for MySQL and Amazon RDS for PostgreSQL into Aurora by creating and restoring snapshots, or by\n                    setting up one-way replication. You can use push-button migration tools to convert your existing RDS for MySQL and\n                    RDS for PostgreSQL applications to Aurora. Before using Amazon Aurora, complete the steps in Setting up your environment for Amazon Aurora ,\n            and then review the concepts and features of Aurora in Amazon Aurora DB clusters . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Aurora DB clusters"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf", "content": "No main content found."}, {"title": "rds \u2014 AWS CLI 1.35.0 Command Reference", "url": "https://docs.aws.amazon.com/cli/latest/reference/rds/", "content": "No main content found."}, {"title": "pi \u2014 AWS CLI 1.35.0 Command Reference", "url": "https://docs.aws.amazon.com/cli/latest/reference/pi/", "content": "No main content found."}, {"title": "rds-data \u2014 AWS CLI 1.35.0 Command Reference", "url": "https://docs.aws.amazon.com/cli/latest/reference/rds-data/", "content": "No main content found."}, {"title": "Welcome - Amazon Relational Database Service", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/index.html", "content": "Welcome PDF Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and \n          scale a relational database in the cloud. It provides cost-efficient, resizeable capacity for an industry-standard relational \n          database and manages common database administration tasks, freeing up developers to focus on what makes their applications \n          and businesses unique. Amazon RDS gives you access to the capabilities of a MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, \n          Oracle, Db2, or Amazon Aurora database server. These capabilities mean that the code, applications, and tools \n          you already use today with your existing databases work with Amazon RDS without modification. Amazon RDS \n          automatically backs up your database and maintains the database software that powers your DB instance. Amazon RDS \n          is flexible: you can scale your DB instance's compute resources and storage capacity to meet your \n          application's demand. As with all Amazon Web Services, there are no up-front investments, and you pay only for \n          the resources you use. This interface reference for Amazon RDS contains documentation for a programming or command line interface \n          you can use to manage Amazon RDS. Amazon RDS is asynchronous, which means that some interfaces might \n          require techniques such as polling or callback functions to determine when a command has been applied. In this \n          reference, the parameter descriptions indicate whether a command is applied immediately, on the next instance reboot, \n          or during the maintenance window. The reference structure is as follows, and we list following some related topics \n          from the user guide. Amazon RDS API Reference For the alphabetical list of API actions, see API Actions . For the alphabetical list of data types, see Data Types . For a list of common query parameters, see Common Parameters . For descriptions of the error codes, see Common Errors . Amazon RDS User Guide For a summary of the Amazon RDS interfaces, see Available RDS Interfaces . For more information about how to use the Query API, see Using the Query API . This document was last published on October 8, 2024. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/rds-api.pdf", "content": "No main content found."}, {"title": "Welcome - Amazon RDS Performance Insights", "url": "https://docs.aws.amazon.com/performance-insights/latest/APIReference/index.html", "content": "Welcome Amazon RDS Performance Insights enables you to monitor and explore different dimensions of database load based on data captured from a running DB instance. The guide\n            provides detailed information about Performance Insights data types, parameters and errors. When Performance Insights is enabled, the Amazon RDS Performance Insights API provides visibility into the performance of your DB instance. Amazon CloudWatch provides the\n            authoritative source for AWS service-vended monitoring metrics. Performance Insights offers a domain-specific view of DB load. DB load is measured as average active sessions. Performance Insights provides the data to API consumers as a two-dimensional time-series dataset. The time dimension\n            provides DB load data for each time point in the queried time range. Each time point decomposes overall load in relation to the requested\n            dimensions, measured at that time point. Examples include SQL, Wait event, User, and Host. To learn more about Performance Insights and Amazon Aurora DB instances, go to the Amazon Aurora User Guide . To learn more about Performance Insights and Amazon RDS DB instances, go to the Amazon RDS User Guide . To learn more about Performance Insights and Amazon DocumentDB clusters, go to the Amazon DocumentDB Developer Guide . This document was last published on October 8, 2024. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/performance-insights/latest/APIReference/performance-insights-api.pdf", "content": "No main content found."}, {"title": "Welcome - RDS Data API", "url": "https://docs.aws.amazon.com/rdsdataservice/latest/APIReference/index.html", "content": "Welcome PDF Amazon RDS provides an HTTP endpoint to run SQL statements on an Amazon Aurora DB cluster. To run these\n            statements, you use the RDS Data API (Data API). Data API is available with the following types of Aurora databases: Aurora PostgreSQL - Serverless v2, provisioned, and Serverless v1 Aurora MySQL - Serverless v2, provisioned, and Serverless v1 For more information about the Data API, see Using RDS Data API in the Amazon Aurora User Guide . This document was last published on October 8, 2024. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/rdsdataservice/latest/APIReference/rdsdataservice-api.pdf", "content": "No main content found."}, {"title": "Release notes for Amazon Relational Database Service (Amazon RDS) for Oracle - Amazon Relational Database Service", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/OracleReleaseNotes/index.html", "content": "Release notes for Amazon Relational Database Service (Amazon RDS) for Oracle PDF RSS The Amazon RDS for Oracle release notes provide details about the Oracle Database versions that are\n        supported by Amazon RDS. Updates to your Amazon RDS for Oracle DB instances keep them current. If you apply updates, you\n        can be confident that your DB instance is running a version of the database software that\n        has been tested by both Oracle and Amazon. We don't support applying one-off patches to\n        individual DB instances. You can specify any currently supported Oracle Database version when creating a new DB\n        instance. You can specify the major version, such as Oracle Database 21c Release 1 (21.0.0.0),\n        and any supported minor version for the specified major version. If no version is specified,\n        Amazon RDS defaults to a supported version, typically the most recent version. If a major version\n        is specified but a minor version isn't, Amazon RDS defaults to a recent release of the major\n        version that you specified. To see a list of supported versions and defaults for newly\n        created DB instances, use the describe-db-engine-versions AWS CLI command. These release notes apply to RDS for Oracle. If you are looking for details about RDS Custom for Oracle, \n        please see RDS Custom . Topics Amazon RDS for Oracle Database 21c (21.0.0.0) Amazon RDS for Oracle Database 19c (19.0.0.0) Amazon RDS for Oracle Database 18c (18.0.0.0) Amazon RDS for Oracle Database 12c Release 2 (12.2.0.1) Amazon RDS for Oracle Database 12c Release 1 (12.1.0.2) Amazon RDS for Oracle Database 11g Release 2 (11.2.0.4) Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions RDS for Oracle Database 21c"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/OracleReleaseNotes/rds-oracle-relnotes.pdf", "content": "No main content found."}, {"title": "Release notes for Amazon Relational Database Service (Amazon RDS) for PostgreSQL - Amazon Relational Database Service", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/PostgreSQLReleaseNotes/index.html", "content": "Release notes for Amazon Relational Database Service (Amazon RDS) for PostgreSQL PDF RSS The Amazon RDS for PostgreSQL release notes provide details about the PostgreSQL versions and \n     extensions that are available for Amazon RDS. Topics Release calendars for\n            Amazon RDS for PostgreSQL Amazon RDS for PostgreSQL updates Amazon RDS Extended Support updates Extension versions for Amazon RDS for PostgreSQL Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions RDS for PostgreSQL release\n            calendar"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/PostgreSQLReleaseNotes/rds-postgresql-relnotes.pdf", "content": "No main content found."}, {"title": "Release notes for Amazon Aurora MySQL-Compatible Edition - Amazon Aurora", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraMySQLReleaseNotes/index.html", "content": "Release notes for Amazon Aurora MySQL-Compatible Edition PDF RSS Amazon Aurora MySQL-Compatible Edition releases updates regularly. Updates are applied to Aurora MySQL DB clusters during system\n  maintenance windows. The timing when updates are applied depends on the AWS Region and\n  maintenance window setting for the DB cluster, and also the type of update. Amazon Aurora MySQL releases are made available to all AWS Regions over the course of multiple days. \n   Some Regions might temporarily show an engine version that isn't available in a different Region yet. Updates are applied to all instances in a DB cluster at the same time. An update requires a database restart on\n  all instances in a DB cluster. So you experience 20 to 30 seconds of downtime, after which you can resume using\n  your DB cluster or clusters. You can view or change your maintenance window settings from the AWS Management Console . Topics Release calendars for Amazon Aurora MySQL Database engine updates for Amazon Aurora MySQL version 3 Database engine updates for Amazon Aurora MySQL version 2 Database engine updates for Amazon Aurora MySQL version 1 (Deprecated) MySQL bugs fixed by Aurora MySQL database\n            engine updates Security vulnerabilities fixed in Aurora MySQL Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Aurora MySQL release calendars"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraMySQLReleaseNotes/aurora-mysql-relnotes.pdf", "content": "No main content found."}, {"title": "Release notes for Amazon Aurora PostgreSQL-Compatible Edition - Amazon Aurora", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraPostgreSQLReleaseNotes/index.html", "content": "Release notes for Amazon Aurora PostgreSQL-Compatible Edition PDF RSS The Amazon Aurora PostgreSQL-Compatible Edition release notes provide details about the Aurora PostgreSQL versions and extensions that are available for Amazon Aurora. Aurora PostgreSQL releases are made available to all AWS Regions over the course of multiple days. \n  Some Regions might temporarily show an engine version that isn't available in a different Region yet. Topics Release calendars for Aurora PostgreSQL Amazon Aurora PostgreSQL updates Babelfish for Aurora PostgreSQL updates Aurora PostgreSQL query plan management updates Extension versions for Amazon Aurora PostgreSQL Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Aurora PostgreSQL release calendars"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraPostgreSQLReleaseNotes/aurora-postgresql-relnotes.pdf", "content": "No main content found."}, {"title": "Amazon RDS and Aurora Documentation", "url": "https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_featuredsvcs#user-guides", "content": "No main content found."}, {"title": "Amazon RDS and Aurora Documentation", "url": "https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_featuredsvcs#references", "content": "No main content found."}, {"title": "Amazon RDS and Aurora Documentation", "url": "https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_featuredsvcs#release-notes", "content": "No main content found."}, {"title": "Amazon RDS and Aurora Documentation", "url": "https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_featuredsvcs#best-practices", "content": "No main content found."}, {"title": "AWS Lambda Documentation", "url": "https://docs.aws.amazon.com/lambda/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=Lambda&topic_url=https://docs.aws.amazon.com/lambda/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "What is AWS Lambda? - AWS Lambda", "url": "https://docs.aws.amazon.com/lambda/latest/dg/index.html", "content": "What is AWS Lambda? PDF RSS You can use AWS Lambda to run code without provisioning or managing servers. Lambda runs your code\n    on a high-availability compute infrastructure and performs all of the administration of the compute resources,\n    including server and operating system maintenance, capacity provisioning and automatic scaling, and\n    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports. You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume\u2014there is no charge when your code is not running. For more information, see AWS Lambda Pricing . Tip To learn how to build serverless solutions , check out the Serverless Developer Guide . When to use Lambda Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to\n      zero when not in demand. For example, you can use Lambda for: File processing: Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload. Stream processing: Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering. Web applications: Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers. IoT backends: Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests. Mobile backends: Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends. When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a\n      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you\n      cannot log in to compute instances or customize the operating system on provided\n        runtimes. Lambda performs operational and administrative activities on your behalf, including managing\n      capacity, monitoring, and logging your Lambda functions. Key features The following key features help you develop Lambda applications that are scalable, secure, and easily\n      extensible: Environment variables Use environment variables to adjust your function's behavior without updating code. Versions Manage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version. Container images Create a container image for a Lambda function by using an AWS provided base image or an alternative base\n            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning. Layers Package libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code. Lambda extensions Augment your Lambda functions with tools for monitoring, observability, security, and governance. Function URLs Add a dedicated HTTP(S) endpoint to your Lambda function. Response streaming Configure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads. Concurrency and scaling controls Apply fine-grained control over the scaling and responsiveness of your production applications. Code signing Verify that only approved developers publish unaltered, trusted code in your Lambda functions Private networking Create a private network for resources such as databases, cache instances, or internal services. File system access Configure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency. Lambda SnapStart for Java Improve startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Create your first function"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-dg.pdf", "content": "No main content found."}, {"title": "Welcome - AWS Lambda", "url": "https://docs.aws.amazon.com/lambda/latest/dg/API_Reference.html", "content": "Welcome PDF This section contains the AWS Lambda API Reference documentation. Instead of making requests\n        to the API directly from your application, we recommend that you use one of the\n        AWS Software Development Kits (SDKs) for your programming language. The\n        AWS SDKs take care of request authentication, serialization, and connection\n        management. If you don't use the AWS SDK, you will need to authenticate your\n        request by providing a signature. AWS Lambda supports signature version 4. For more information,\n        see Signature Version 4 signing process in the Amazon Web Services General Reference . Certificate errors when using an SDK Because AWS SDKs use the CA certificates from your computer, changes to the certificates on the AWS servers\n            can cause connection failures when you attempt to use an SDK. You can prevent these failures by keeping your\n            computer's CA certificates and operating system up-to-date. If you encounter this issue in a corporate\n            environment and do not manage your own computer, you might need to ask an administrator to assist with the\n            update process. The following list shows minimum operating system and Java versions: Microsoft Windows versions that have updates from January 2005 or later installed contain at least one\n                    of the required CAs in their trust list. Mac OS X 10.4 with Java for Mac OS X 10.4 Release 5 (February 2007), Mac OS X 10.5 (October 2007), and\n                    later versions contain at least one of the required CAs in their trust list. Red Hat Enterprise Linux 5 (March 2007), 6, and 7 and CentOS 5, 6, and 7 all contain at least one of the\n                    required CAs in their default trusted CA list. Java 1.4.2_12 (May 2006), 5 Update 2 (March 2005), and all later versions, including Java 6 (December\n                    2006), 7, and 8, contain at least one of the required CAs in their default trusted CA list. When accessing the AWS Lambda management console or AWS Lambda API endpoints, whether through browsers or\n            programmatically, you will need to ensure your client machines support any of the following CAs: Amazon Root CA 1 Starfield Services Root Certificate Authority - G2 Starfield Class 2 Certification Authority Root certificates from the first two authorities are available from Amazon trust services , but keeping your computer\n            up-to-date is the more straightforward solution. To learn more about ACM-provided certificates, see AWS Certificate Manager FAQs. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "Introduction - AWS Lambda", "url": "https://docs.aws.amazon.com/lambda/latest/operatorguide/intro.html", "content": "Introduction AWS Lambda is a flexible service designed for a wide variety of use-cases. Across the millions of AWS customers using Lambda every month, serverless applications generally fall into several common categories: Web applications : serve the front-end code via Amazon S3 and Amazon CloudFront, or automating the entire deployment and hosting with AWS Amplify Console. Web and mobile backends : the front-ends interact with the backend via API Gateway. Integrated authorization and authentication are provided by Amazon Cognito or APN Partners like Auth0. Data processing : event-based processing tasks triggered by data changes in data stores, or streaming data ETL tasks with Amazon Kinesis and Lambda. Parallelized computing tasks : splitting highly complex, long-lived computations to individual tasks across many Lambda function instances to process data more quickly in parallel. Internet of Things (IoT) workloads : processing data generated by physical IoT devices. Additionally, many workloads are hybrid serverless applications, especially where legacy systems are being migrated from either on-premises or instance-based environments. In this case, developers can gradually migrate functionality from a legacy system to a Lambda-based application. This guide is built for developers and operators of Lambda-based applications. It is aimed at operators of typical production serverless applications, looking to understand more clearly how to build, measure, troubleshoot, and optimize their compute processes. This guide covers concepts and best practices for designing Lambda-based applications, together with an approach for ongoing monitoring and troubleshooting. Serverless applications can include a wide variety of different AWS services to manage APIs, messaging, storage, and content distribution. Most of these applications rely upon Lambda for connecting these services and transforming the data throughout an application. This guide focuses on the role of Lambda in these architectures, and how you can fine-tune your functions and their configurations to maximize reliability and maintainability, and reduce cost. Lambda-based applications are event-driven architectures with many of the characteristics of distributed systems. While Lambda handles many of the complex tasks like scaling and infrastructure management, it\u2019s important for operators to understand the scope of knowledge that they need to manage serverless applications successfully. As AWS customers adopt Lambda to solve many of their most challenging workloads, understanding the troubleshooting and monitoring tasks involved is the key to becoming a proficient operator. Both start-ups and enterprises develop Lambda-based applications for green field applications and legacy applications. As these applications develop features and build traffic, many of the same best practices for operations apply. This guide covers many of the most important operational best practices and advice while explaining core topics underpinning how Lambda-based applications work. The first half of this guide provides a deep dive into foundational topics around event-driven architectures, application design, and security. The latter half covers guidance for operators around debugging, monitoring and observability, and performance optimization. The goal is to provide a concrete, actionable approach to operating and troubleshooting Lambda-based applications. These are the topics covered in detail: Event-driven architectures : understanding how events drive serverless applications informs the design of your workload. This chapter explains: how Lambda fits into this paradigm; the benefits and tradeoffs of event-driven architectures; design principles, stateless design, idempotency, and message ordering; retry behaviors; using AWS services; avoiding common anti-patterns. Application design : topics include: understanding quotas; scaling and concurrency; choosing and managing runtimes and SDKs; networking and VPC configurations; comparing synchronous versus asynchronous invocations; controlling traffic flow for non-serverless services. Security : security is the primary concern at AWS, but all developers have a role to play in developing secure applications. This covers: the shared responsibility model; applying the principles of least privilege; handling sensitive data; IAM roles and resource policies; authorization and authentication; code signing; protecting applications with public endpoints. Debugging : the process of identifying errors in software is critical to any production workload. Topics include: standardizing a debugging approach; capturing and replaying events; troubleshooting executions, networking, and deployments; identifying common causes of errors (memory configurations, timeouts, quotas, third-party libraries, and unintended leakage between invocations). Monitoring and observability : serverless applications have parallels with distributed applications for monitoring and observability, presenting challenges to new serverless operators. This chapter explains: instrumentation best practices; CloudWatch Logs (using Insights and AWS Resource Groups); tracing with X-Ray; alerts and automation; code storage optimization. Performance optimization : while Lambda manages running and scaling functions, there are many levers available to developers that influence the performance. Topics include: cold starts and latency, package sizes and dependencies, memory and power settings; performance and cost; maximizing throughput. This guide will be revised regularly to incorporate new Lambda features and AWS services as they are released. If you have any questions or comments about any of the content in this guide, raise an issue in the GitHub repository or contact the author. James Beswick\nAWS Serverless Developer Advocate jbeswick@amazon.com Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Event-driven architectures"}, {"title": "What is serverless development? - Serverless", "url": "https://docs.aws.amazon.com/serverless/latest/devguide/index.html", "content": "What is serverless development? PDF RSS The following topics will guide you through developing a better conceptual understanding of serverless application development,\n  and how various AWS services fit into together to create application patterns that form the core of your cloud applications.\n  These applications can range from microservices that handle discreet business logic as a part of your application back-end, to event-driven workflows that perform\n  data transformations or processing. Understanding serverless development will you help you make critical decisions about which AWS services are best suited for your business need.\n  For example, choosing between Amazon DocumentDB, DynamoDB, and Aurora PostgreSQL for a database depends on various factors, such as what type of data-structure you want to use, or\n  how many concurrent database connections you anticipate as your applications scale. The goal of this serverless developer guide is to give you directed learning paths for the core services you need to implement serverless solutions. Serverless development lets you build applications without managing long-running servers,\n    such as a provisioned Amazon EC2 instance. AWS serverless\n    technologies are pay-as-you-go, can scale up and down as your application needs change,\n    and are built to expand across AWS Regions to ensure resiliency. This guide will highlight what you need to know right away and link to service\n      documentation for more service-specific details. For example, you will learn that the Lambda service creates an execution environment to run compute functions. For more information on how Lambda manages function scaling or reduces start-up time,\n       we will link you to relevant sections of the Lambda developer guide. The topics in this guide will cover the prerequisites for understanding serverless development on AWS, such as account\n        creation and an overview of AWS cloud infrastructure. Then, you will learn how to shift from a traditional development model to\n        a serverless, event-driven architecture with which to develop applications on the cloud. Along the way, this guide will introduce core services,\n        workshops, and tutorials, you can choose to reinforce your\n        learning with hands-on activities. AWS Identity and Access Management \u2014 for securely accessing resources on AWS. AWS Lambda \u2014 for serverless compute functionality. Amazon API Gateway for integrating HTTP and HTTPS requests with services to handle the requests. Amazon DynamoDB for data storage and retrieval Learn serverless techniques in an online workshop Learn by doing in the Serverless Patterns Workshop .\n          The first module introduces a serverless microservice to retrieve data from DynamoDB with Lambda and API Gateway.\n          Additional modules provide practical examples of unit and integration testing, using infrastructure as\n          code to deploy resources, and how to build common architectural patterns used in\n          serverless solutions. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Serverless data processing"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/serverless/latest/devguide/serverless-core.pdf", "content": "No main content found."}, {"title": "AWS Lambda Documentation", "url": "https://docs.aws.amazon.com/lambda/?icmpid=docs_homepage_featuredsvcs#lambda-guides-and-references", "content": "No main content found."}, {"title": "AWS Lambda Documentation", "url": "https://docs.aws.amazon.com/lambda/?icmpid=docs_homepage_featuredsvcs#get-started-with-serverless", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=VPC&topic_url=https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "What is Amazon VPC? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/userguide/index.html", "content": "What is Amazon VPC? With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual \n\t\tnetwork that you've defined. This virtual network closely resembles a traditional network \n\t\tthat you'd operate in your own data center, with the benefits of using the scalable \n\t\tinfrastructure of AWS. The following diagram shows an example VPC. The VPC has one subnet in each of the \n\t\tAvailability Zones in the Region, EC2 instances in each subnet, and an internet gateway\n\t\tto allow communication between the resources in your VPC and the internet. For more information, see Amazon Virtual Private Cloud (Amazon VPC) . Features The following features help you configure a VPC to provide the connectivity\n\t\t\tthat your applications need: Virtual private clouds (VPC) A VPC is a virtual \n\t\t\t\t\tnetwork that closely resembles a traditional network that you'd operate\n\t\t\t\t\tin your own data center. After you create a VPC, you can add subnets. Subnets A subnet is a range of \n\t\t\t\t\tIP addresses in your VPC. A subnet must reside in a single Availability Zone.\n\t\t\t\t\tAfter you add subnets, you can deploy AWS resources in your VPC. IP addressing You can assign IP addresses , \n\t\t\t\t\tboth IPv4 and IPv6, to your VPCs and subnets. You can also bring your public \n\t\t\t\t\tIPv4 addresses and IPv6 GUA addresses to AWS and allocate them to resources in your VPC, \n\t\t\t\t\tsuch as EC2 instances, NAT gateways, and Network Load Balancers. Routing Use route tables to determine \n\t\t\t\t\twhere network traffic from your subnet or gateway is directed. Gateways and endpoints A gateway connects your VPC to \n\t\t\t\t\tanother network. For example, use an internet gateway to connect your VPC to the internet. Use a VPC endpoint to connect to AWS services privately, without \n\t\t\t\t\tthe use of an internet gateway or NAT device. Peering connections Use a VPC peering connection to route traffic between the resources in two VPCs. Traffic Mirroring Copy network traffic from \n\t\t\t\t\tnetwork interfaces and send it to security and monitoring appliances for \n\t\t\t\t\tdeep packet inspection. Transit gateways Use a transit gateway , which \n\t\t\t\t\tacts as a central hub, to route traffic between your VPCs, VPN connections,\n\t\t\t\t    and AWS Direct Connect connections. VPC Flow Logs A flow log captures information \n\t\t\t\t\tabout the IP traffic going to and from network interfaces in your VPC. VPN connections Connect your VPCs to your on-premises networks using AWS Virtual Private Network (AWS VPN) . Getting started with Amazon VPC Your AWS account includes a default VPC in each AWS Region. \n\t\t\tYour default VPCs are configured such that you can immediately start launching and connecting \n\t\t\tto EC2 instances. For more information, see Plan your VPC . You can choose to create additional VPCs with the subnets, IP addresses, gateways\n\t\t\tand routing that you need. For more information, see Create a VPC . Working with Amazon VPC You can create and manage your VPCs using any of the following interfaces: AWS Management Console \u2014 Provides a web interface that you can\n                    use to access your VPCs. AWS Command Line Interface (AWS CLI) \u2014\n\t\t\t\t\tProvides commands for a broad set of AWS services, including Amazon VPC, and is\n\t\t\t\t\tsupported on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provides language-specific\n\t\t\t\t\tAPIs and takes care of many of the connection details, such as calculating\n\t\t\t\t\tsignatures, handling request retries, and error handling. For more information,\n\t\t\t\t\tsee AWS SDKs . Query API \u2014 Provides low-level API actions \n                    that you call using HTTPS requests. Using the Query API is the most direct way to\n                    access Amazon VPC, but it requires that your application handle low-level details such as\n                    generating the hash to sign the request, and error handling. For more information, \n                    see Amazon VPC actions in \n\t\t\t\t\tthe Amazon EC2 API Reference . Pricing for Amazon VPC There's no additional charge for using a VPC. There are, however, charges for some VPC\n\t\t\tcomponents, such as NAT gateways, IP Address Manager, traffic mirroring, Reachability Analyzer, and\n\t\t\tNetwork Access Analyzer. For more information, see Amazon VPC Pricing . Nearly all resources that you launch in your virtual private cloud (VPC) provide you\n\t\t\twith an IP address for connectivity. The vast majority of resources in your VPC use\n\t\t\tprivate IPv4 addresses. Resources that require direct access to the internet over IPv4,\n\t\t\thowever, use public IPv4 addresses. Amazon VPC enables you to launch managed services, such as Elastic Load Balancing, Amazon RDS, and Amazon EMR, without having a VPC set up beforehand. It does this by using the default\n\t\t\t\tVPC in your account if you have\n\t\t\t\tone. Any public IPv4 addresses provisioned to your account by the managed\n\t\t\tservice will be charged. These charges will be associated with Amazon VPC service in\n\t\t\tyour AWS Cost and Usage Report. Pricing for public IPv4 addresses A public IPv4 address is an IPv4 address that is\n\t\t\troutable from the internet. A public IPv4 address is necessary for a resource to be\n\t\t\tdirectly reachable from the internet over IPv4. If you are an existing or new AWS Free Tier customer, you get 750 hours of public IPv4 address usage with the EC2 service at no\n\t\t\tcharge. If you are not using the EC2 service in the AWS Free Tier, Public IPv4\n\t\t\taddresses are charged. For specific pricing information, see the Public IPv4 address tab in Amazon VPC Pricing . Private IPv4 addresses ( RFC\n\t\t\t\t1918 ) are not charged. For more information about how public IPv4 addresses\n\t\t\tare charged for shared VPCs, see Billing and\n\t\t\t\tmetering for the owner and participants . Public IPv4 addresses have the following types: Elastic IP addresses (EIPs) : Static, public IPv4 addresses\n                provided by Amazon that you can associate with an EC2 instance, elastic network\n                interface, or AWS resource. EC2 public IPv4 addresses : Public IPv4 addresses assigned to\n                an EC2 instance by Amazon (if the EC2 instance is launched into a default subnet or\n                if the instance is launched into a subnet that\u2019s been configured to automatically\n                assign a public IPv4 address). BYOIPv4 addresses : Public IPv4 addresses in the IPv4 address\n                range that you\u2019ve brought to AWS using Bring your own IP addresses (BYOIP) . Service-managed IPv4 addresses : Public IPv4 addresses\n                automatically provisioned on AWS resources and managed by an AWS service. For\n                example, public IPv4 addresses on Amazon ECS, Amazon RDS, or Amazon WorkSpaces. The following list shows the most common AWS services that can use public IPv4\n\t\t\taddresses. Amazon AppStream 2.0 AWS Client VPN AWS Database Migration Service Amazon EC2 Amazon Elastic Container Service Amazon EKS Amazon EMR Amazon GameLift AWS Global Accelerator AWS Mainframe Modernization Amazon Managed Streaming for Apache Kafka Amazon MQ Amazon RDS Amazon Redshift AWS Site-to-Site VPN Amazon VPC NAT gateway Amazon WorkSpaces Elastic Load Balancing Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How Amazon VPC works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ug.pdf", "content": "No main content found."}, {"title": "What is VPC peering? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/peering/index.html", "content": "What is VPC peering? PDF A virtual private cloud (VPC) is a virtual network dedicated to your \n        AWS account. It is logically isolated from other virtual networks in the AWS Cloud. \n        You can launch AWS resources, such as Amazon EC2 instances, into your VPC. A VPC peering connection is a networking connection between two VPCs that enables you to\n        route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in\n        either VPC can communicate with each other as if they are within the same network. You can\n        create a VPC peering connection between your own VPCs, or with a VPC in another AWS\n        account. The VPCs can be in different Regions (also known as an inter-Region VPC peering\n        connection). AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is\n        neither a gateway nor a VPN connection, and does not rely on a separate piece of physical\n        hardware. There is no single point of failure for communication or a bandwidth bottleneck. A VPC peering connection helps you to facilitate the transfer of data. For example, if you\n        have more than one AWS account, you can peer the VPCs across those accounts to create a file\n\t\tsharing network. You can also use a VPC peering connection to allow other VPCs to access\n\t\tresources you have in one of your VPCs. When you establish peering relationships between VPCs across different AWS Regions,\n        resources in the VPCs (for example, EC2 instances and Lambda functions) in different AWS\n        Regions can communicate with each other using private IP addresses, without using a gateway,\n        VPN connection, or network appliance. The traffic remains in the private IP address space. All\n        inter-Region traffic is encrypted with no single point of failure, or bandwidth bottleneck.\n        Traffic always stays on the global AWS backbone, and never traverses the public internet,\n        which reduces threats, such as common exploits, and DDoS attacks. Inter-Region VPC peering\n        provides a simple and cost-effective way to share resources between Regions or replicate\n        data for geographic redundancy. Pricing for a VPC peering connection There is no charge to create a VPC peering connection. All data transfer over a VPC\n            Peering connection that stays within an Availability Zone (even if it's between different accounts) is free. Charges apply\n            for data transfer over a VPC Peering connections that cross Availability Zones and\n            Regions. For more information, see Amazon EC2 Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions VPC peering process, lifecycle, and limitations"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/peering/vpc-pg.pdf", "content": "No main content found."}, {"title": "What is Traffic Mirroring? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/mirroring/index.html", "content": "What is Traffic Mirroring? PDF Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network\n    interface of type interface . You can then send the traffic to out-of-band security\n    and monitoring appliances for: Content inspection Threat monitoring Troubleshooting The security and monitoring appliances can be deployed as individual instances, or as a\n    fleet of instances behind either a Network Load Balancer or a Gateway Load Balancer with a UDP listener. Traffic Mirroring supports filters\n    and packet truncation, so that you can extract only the traffic of interest, using the \n    monitoring tools of your choice. Traffic Mirroring concepts The following are the key concepts for Traffic Mirroring: Source \u2014 The network interface to monitor. Filter \u2014 A set of rules that defines the traffic\n          that is mirrored. Target \u2014 The destination for mirrored traffic. Session \u2014 Establishes a relationship between a \n          source, a filter, and a target. Work with Traffic Mirroring You can create, access, and manage your traffic mirror resources using any of the\n      following: AWS Management Console \u2014 Provides a web interface that you\n          can use to access your traffic mirror resources. AWS Command Line Interface (AWS CLI) \u2014 Provides commands for a\n          broad set of AWS services, including Amazon VPC. The AWS CLI is supported on Windows, macOS, and\n          Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provide language-specific APIs.\n          The AWS SDKs take care of many of the connection details, such as calculating\n          signatures, handling request retries, and handling errors. For more information, see AWS SDKs . Query API \u2014 Provides low-level API actions that\n          you call using HTTPS requests. Using the Query API is the most direct way to access Amazon VPC.\n          However, it requires that your application handle low-level details such as generating the\n          hash to sign the request and handling errors. For more information, see Amazon VPC actions in the Amazon EC2 API Reference . Traffic Mirroring benefits Traffic Mirroring offers the following benefits: Simplified operation \u2014 Mirror any range of\n          your VPC traffic without having to manage packet forwarding agents on your EC2\n          instances. Enhanced security \u2014 Capture packets at the\n          elastic network interface, which cannot be disabled or tampered with from a user\n          space. Increased monitoring options \u2014 Send your\n          mirrored traffic to any security device. Regional availability Traffic Mirroring is available in all Regions. Pricing You are charged on an hourly basis for each active traffic mirror session. You'll continue \n      to be charged for Traffic Mirroring until you delete all active traffic mirror sessions . For example, you'll\n      still be charged in the following scenarios: You detached the network interface from the mirror source You stopped or terminated the mirror source You changed the instance type of the mirror source to an unsupported instance type Data transfer charges apply . If your traffic mirroring targets are behind a gateway or network load balancer, data processing for the load balancing services also applies. For information about pricing for Traffic Mirroring, see Network Analysis on\n      the Amazon VPC pricing page. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How Traffic Mirroring works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/mirroring/vpc-tm.pdf", "content": "No main content found."}, {"title": "Amazon VPC actions - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/OperationList-query-vpc.html", "content": "Amazon VPC actions PDF The following API actions are available for Amazon VPC. To learn more about Amazon VPC,\n\t\t\tsee the Amazon VPC product page and the Amazon VPC documentation . DHCP options AssociateDhcpOptions CreateDhcpOptions DeleteDhcpOptions DescribeDhcpOptions Elastic network interfaces AssignIpv6Addresses AssignPrivateIpAddresses AttachNetworkInterface CreateNetworkInterface CreateNetworkInterfacePermission DeleteNetworkInterface DeleteNetworkInterfacePermission DescribeNetworkInterfaceAttribute DescribeNetworkInterfacePermissions DescribeNetworkInterfaces DetachNetworkInterface ModifyNetworkInterfaceAttribute ResetNetworkInterfaceAttribute UnassignIpv6Addresses UnassignPrivateIpAddresses Internet gateways AttachInternetGateway CreateEgressOnlyInternetGateway CreateInternetGateway DeleteEgressOnlyInternetGateway DeleteInternetGateway DescribeEgressOnlyInternetGateways DescribeInternetGateways DetachInternetGateway Managed prefix lists CreateManagedPrefixList DeleteManagedPrefixList DescribeManagedPrefixLists DescribePrefixLists GetManagedPrefixListAssociations GetManagedPrefixListEntries ModifyManagedPrefixList RestoreManagedPrefixListVersion NAT gateways AssignPrivateNatGatewayAddress AssociateNatGatewayAddress CreateNatGateway DeleteNatGateway DescribeNatGateways DisassociateNatGatewayAddress UnassignPrivateNatGatewayAddress Network ACLs CreateNetworkAcl CreateNetworkAclEntry DeleteNetworkAcl DeleteNetworkAclEntry DescribeNetworkAcls ReplaceNetworkAclAssociation ReplaceNetworkAclEntry Route tables AssociateRouteTable CreateRoute CreateRouteTable DeleteRoute DeleteRouteTable DescribeRouteTables DisassociateRouteTable ReplaceRoute ReplaceRouteTableAssociation Security groups AuthorizeSecurityGroupEgress AuthorizeSecurityGroupIngress CreateSecurityGroup DeleteSecurityGroup DescribeSecurityGroupReferences DescribeSecurityGroups DescribeStaleSecurityGroups ModifySecurityGroupRules RevokeSecurityGroupEgress RevokeSecurityGroupIngress UpdateSecurityGroupRuleDescriptionsEgress UpdateSecurityGroupRuleDescriptionsIngress Subnets AssociateSubnetCidrBlock CreateDefaultSubnet CreateSubnet CreateSubnetCidrReservation DeleteSubnet DeleteSubnetCidrReservation DescribeSubnets DisassociateSubnetCidrBlock GetSubnetCidrReservations ModifySubnetAttribute Traffic Mirroring CreateTrafficMirrorFilter CreateTrafficMirrorFilterRule CreateTrafficMirrorSession CreateTrafficMirrorTarget DeleteTrafficMirrorFilter DeleteTrafficMirrorFilterRule DeleteTrafficMirrorSession DeleteTrafficMirrorTarget DescribeTrafficMirrorFilters DescribeTrafficMirrorSessions DescribeTrafficMirrorTargets ModifyTrafficMirrorFilterNetworkServices ModifyTrafficMirrorFilterRule ModifyTrafficMirrorSession VPCs AssociateVpcCidrBlock CreateDefaultVpc CreateVpc DeleteVpc DescribeVpcAttribute DescribeVpcs DisassociateVpcCidrBlock ModifyVpcAttribute ModifyVpcTenancy VPC flow logs CreateFlowLogs DeleteFlowLogs DescribeFlowLogs GetFlowLogsIntegrationTemplate VPC peering AcceptVpcPeeringConnection CreateVpcPeeringConnection DeleteVpcPeeringConnection DescribeVpcPeeringConnections ModifyVpcPeeringConnectionOptions RejectVpcPeeringConnection Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions VM Import/Export actions Amazon VPC IPAM actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api.pdf#OperationList-query-vpc", "content": "No main content found."}, {"title": "What is IPAM? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/ipam/index.html", "content": "What is IPAM? PDF Amazon VPC IP Address Manager (IPAM) is a VPC feature that makes it easier for you to plan, track, and monitor IP addresses for your AWS workloads. \n        You can use IPAM automated workflows to more efficiently manage IP addresses. You can use IPAM to do the following: Organize IP address space into routing and security domains Monitor IP address space that's in use and monitor resources that are using space against\n                business rules View the history of IP address assignments in your organization Automatically allocate CIDRs to VPCs using specific business rules Troubleshoot network connectivity issues Enable cross-region and cross-account sharing of your Bring Your Own IP (BYOIP) addresses Provision Amazon-provided contiguous IPv6 CIDR blocks to pools for VPC\n                creation This guide consists of the following sections: How IPAM works : IPAM concepts and\n                terminology. Getting started with IPAM : Steps to enable\n                company-wide IP address management with AWS Organizations, create an IPAM, and\n                plan IP address usage. Managing IP address space in IPAM : Steps to\n                manage your IPAM, scopes, pools, and allocations. Tracking IP address usage in IPAM : Steps\n                to monitor and track IP address usage with IPAM. Tutorials for Amazon VPC IP Address Manager : Detailed step-by-step\n                tutorials for creating an IPAM and pools, allocating VPC CIDRs, and bringing your\n                own public IP address CIDRs to IPAM. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How IPAM works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/ipam/vpc-ipam.pdf", "content": "No main content found."}, {"title": "Amazon VPC IPAM actions - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/operation-list-ipam.html", "content": "Amazon VPC IPAM actions PDF The following API actions are available for Amazon VPC IP Address Manager (IPAM). For more information,\n\t\t\tsee the IPAM User Guide . AllocateIpamPoolCidr AssociateIpamResourceDiscovery CreateIpam CreateIpamPool CreateIpamScope CreateIpamResourceDiscovery CreatePublicIpv4Pool DeleteIpam DeleteIpamPool DeleteIpamResourceDiscovery DeleteIpamScope DeletePublicIpv4Pool DeprovisionIpamPoolCidr DeprovisionPublicIpv4PoolCidr DescribeIpamPools DescribeIpams DescribeIpamResourceDiscoveries DescribeIpamResourceDiscoveryAssociations DescribeIpamScopes DescribePublicIpv4Pools DisableIpamOrganizationAdminAccount DisassociateIpamResourceDiscovery EnableIpamOrganizationAdminAccount GetIpamAddressHistory GetIpamDiscoveredAccounts GetIpamDiscoveredResourceCidrs GetIpamPoolAllocations GetIpamPoolCidrs GetIpamResourceCidrs ModifyIpam ModifyIpamPool ModifyIpamResourceCidr ModifyIpamResourceDiscovery ModifyIpamScope MoveByoipCidrToIpam ProvisionIpamPoolCidr ProvisionPublicIpv4PoolCidr ReleaseIpamPoolAllocation Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Amazon VPC actions AWS Wavelength actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api.pdf#operation-list-ipam", "content": "No main content found."}, {"title": "What is AWS PrivateLink? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/privatelink/index.html", "content": "What is AWS PrivateLink? PDF RSS AWS PrivateLink is a highly available, scalable technology that you can use to privately\n\t\tconnect your VPC to services as if they were in your VPC. You do not need to use an internet\n\t\tgateway, NAT device, public IP address, AWS Direct Connect connection, or AWS Site-to-Site VPN connection to\n\t\tallow communication with the service from your private subnets. Therefore, you control the\n\t\tspecific API endpoints, sites, and services that are reachable from your VPC. Use cases You can create VPC endpoints to connect resources in your VPC to services that integrate\n\t\t\twith AWS PrivateLink. You can create your own VPC endpoint service and make it available\n\t\t\tto other AWS customers. For more information, see AWS PrivateLink concepts . In the following diagram, the VPC on the left has several EC2 instances in a private subnet \n\t    \tand three interface VPC endpoints. The top-most VPC endpoint connects to an AWS service. \n\t    \tThe middle VPC endpoint connects to a service hosted by another AWS account (a VPC endpoint \n\t    \tservice). The bottom VPC endpoint connects to an AWS Marketplace partner service. Learn more AWS PrivateLink concepts Access AWS services Access SaaS products Access virtual appliances Share your services Work with VPC endpoints You can create, access, and manage VPC endpoints using any of the following: AWS Management Console \u2014 Provides a web interface that you can\n\t\t\t\t\tuse to access your AWS PrivateLink resources. Open the Amazon VPC console and choose Endpoints or Endpoint services . AWS Command Line Interface (AWS CLI) \u2014 Provides commands for a\n\t\t\t\t\tbroad set of AWS services, including AWS PrivateLink. For more information about commands\n\t\t\t\t\tfor AWS PrivateLink, see ec2 in the AWS CLI Command Reference . AWS CloudFormation - Create templates that describe your AWS \n\t\t\t\t\tresources. You use the templates to provision and manage these resources as a single unit.\n\t\t\t\t\tFor more information, see the following AWS PrivateLink resources: AWS::EC2::VPCEndpoint AWS::EC2::VPCEndpointConnectionNotification AWS::EC2::VPCEndpointService AWS::EC2::VPCEndpointServicePermissions AWS::ElasticLoadBalancingV2::LoadBalancer AWS SDKs \u2014 Provide language-specific APIs. The\n\t\t\t\t\tSDKs take care of many of the connection details, such as calculating signatures,\n\t\t\t\t\thandling request retries, and handling errors. For more information, see Tools to Build on AWS . Query API \u2014 Provides low-level API actions that\n\t\t\t\t\tyou call using HTTPS requests. Using the Query API is the most direct way to\n\t\t\t\t\taccess Amazon VPC. However, it requires that your application handle low-level\n\t\t\t\t\tdetails such as generating the hash to sign the request and handling errors. For\n\t\t\t\t\tmore information, see AWS PrivateLink actions in the Amazon EC2 API Reference . Pricing For information about the pricing for VPC endpoints, see AWS PrivateLink Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Concepts"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/privatelink/aws-privatelink.pdf", "content": "No main content found."}, {"title": "AWS PrivateLink actions - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/operation-list-privatelink.html", "content": "AWS PrivateLink actions PDF The following API actions are available for AWS PrivateLink. To learn more about AWS PrivateLink,\n\t\t    see the AWS PrivateLink product page and the AWS PrivateLink User Guide . VPC endpoint services AcceptVpcEndpointConnections CreateVpcEndpointServiceConfiguration DeleteVpcEndpointServiceConfigurations DescribeVpcEndpointServiceConfigurations DescribeVpcEndpointServicePermissions DescribeVpcEndpointServices ModifyVpcEndpointServiceConfiguration ModifyVpcEndpointServicePayerResponsibility ModifyVpcEndpointServicePermissions RejectVpcEndpointConnections StartVpcEndpointServicePrivateDnsVerification VPC endpoints CreateVpcEndpoint CreateVpcEndpointConnectionNotification DeleteVpcEndpointConnectionNotifications DeleteVpcEndpoints DescribeVpcEndpointConnectionNotifications DescribeVpcEndpointConnections DescribeVpcEndpoints ModifyVpcEndpoint ModifyVpcEndpointConnectionNotification Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions AWS Outposts actions Recycle Bin"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api.pdf#operation-list-privatelink", "content": "No main content found."}, {"title": "What is Amazon VPC Transit Gateways? - Amazon VPC", "url": "https://docs.aws.amazon.com/vpc/latest/tgw/index.html", "content": "What is Amazon VPC Transit Gateways? Amazon VPC Transit Gateways is a network transit hub used to interconnect virtual private clouds (VPCs)\n    and on-premises networks. As your cloud infrastructure expands globally, inter-Region peering\n    connects transit gateways together using the AWS Global Infrastructure. All network traffic between\n    AWS data centers is automatically encrypted at the physical layer. Amazon VPC Transit Gateways is a\n    service of Amazon Virtual Private Cloud(VPC) and can be accessed through the Amazon VPC Console at https://console.aws.amazon.com/vpc/home#vpc/ . For more information, see AWS Transit Gateway . Transit gateway concepts The following are the key concepts for transit gateways: Attachments \u2014 You can attach the\n          following: One or more VPCs A Connect SD-WAN/third-party network appliance An AWS Direct Connect gateway A peering connection with another transit gateway A VPN connection to a transit gateway Transit gateway Maximum Transmission Unit (MTU) \u2014\n          The maximum transmission unit (MTU) of a network connection is the size, in bytes, of the\n          largest permissible packet that can be passed over the connection. The larger the MTU of a\n          connection, the more data that can be passed in a single packet. A transit gateway supports an MTU\n          of 8500 bytes for traffic between VPCs, AWS Direct Connect, Transit Gateway Connect, and peering\n          attachments (intra-Region, inter-Region, and Cloud WAN peering attachments). Traffic over\n          VPN connections can have an MTU of 1500 bytes. Transit gateway route table \u2014 A transit gateway has a\n          default route table and can optionally have additional route tables. A route table\n          includes dynamic and static routes that decide the next hop based on the destination IP\n          address of the packet. The target of these routes could be any transit gateway attachment. By\n          default, transit gateway attachments are associated with the default transit gateway route\n          table. Associations \u2014 Each attachment is associated with\n          exactly one route table. Each route table can be associated with zero to many\n          attachments. Route propagation \u2014 A VPC, VPN connection, or\n          Direct Connect gateway can dynamically propagate routes to a transit gateway route table.\n          With a Connect attachment, the routes are propagated to a transit gateway route table by default.\n          With a VPC, you must create static routes to send traffic to the transit gateway. With a VPN\n          connection, routes are propagated from the transit gateway to your on-premises router using Border\n          Gateway Protocol (BGP). With a Direct Connect gateway, allowed prefixes are originated to\n          your on-premises router using BGP. With a peering attachment, you must create a static\n          route in the transit gateway route table to point to the peering attachment. How to get started with transit gateways Use the following resources to help you create and use a transit gateway. How transit gateways work Get started with transit gateways Design best practices Work with transit gateways You can create, access, and manage your transit gateways using any of the following interfaces: AWS Management Console \u2014 Provides a web interface that you\n          can use to access your transit gateways. AWS Command Line Interface (AWS CLI) \u2014 Provides\n          commands for a broad set of AWS services, including Amazon VPC, and is supported on\n                    Windows, macOS, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provides language-specific API\n          operations and takes care of many of the connection details, such as calculating\n          signatures, handling request retries, and handling errors. For more information, see AWS SDKs . Query API \u2014 Provides low-level API actions\n          that you call using HTTPS requests. Using the Query API is the most direct way to access\n          Amazon VPC, but it requires that your application handle low-level details such as generating\n          the hash to sign the request, and handling errors. For more information, see the Amazon EC2 API Reference . Pricing You are charged hourly for each attachment on a transit gateway, and you are charged for\n            the amount of traffic processed on the transit gateway. For more information, see AWS Transit Gateway pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How transit gateways work"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/tgw/vpc-tgw.pdf", "content": "No main content found."}, {"title": "AWS Transit Gateway actions - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/operation-list-tgw.html", "content": "AWS Transit Gateway actions PDF The following API actions are available for AWS Transit Gateway. For more information, see\n\t\t    the Amazon VPC Transit Gateways . Transit gateways AcceptTransitGatewayVpcAttachment CreateTransitGateway CreateTransitGatewayVpcAttachment DeleteTransitGateway DeleteTransitGatewayVpcAttachment DescribeTransitGatewayAttachments DescribeTransitGateways DescribeTransitGatewayVpcAttachments ModifyTransitGateway ModifyTransitGatewayVpcAttachment RejectTransitGatewayVpcAttachment Transit Gateway Connect CreateTransitGatewayConnect CreateTransitGatewayConnectPeer DeleteTransitGatewayConnect DeleteTransitGatewayConnectPeer DescribeTransitGatewayConnects DescribeTransitGatewayConnectPeers Transit Gateway Multicast AcceptTransitGatewayMulticastDomainAssociations AssociateTransitGatewayMulticastDomain CreateTransitGatewayMulticastDomain DeleteTransitGatewayMulticastDomain DeregisterTransitGatewayMulticastGroupMembers DeregisterTransitGatewayMulticastGroupSources DescribeTransitGatewayMulticastDomains DisassociateTransitGatewayMulticastDomain GetTransitGatewayMulticastDomainAssociations RegisterTransitGatewayMulticastGroupMembers RegisterTransitGatewayMulticastGroupSources RejectTransitGatewayMulticastDomainAssociations SearchTransitGatewayMulticastGroups Transit Gateway Peering Attachments AcceptTransitGatewayPeeringAttachment CreateTransitGatewayPeeringAttachment DeleteTransitGatewayPeeringAttachment DescribeTransitGatewayPeeringAttachments RejectTransitGatewayPeeringAttachment Transit gateway policy tables AssociateTransitGatewayPolicyTable CreateTransitGatewayPolicyTable DeleteTransitGatewayPolicyTable DescribeTransitGatewayPolicyTables DisassociateTransitGatewayPolicyTable GetTransitGatewayPolicyTableAssociations GetTransitGatewayPolicyTableEntries Transit gateway route tables AssociateTransitGatewayRouteTable CreateTransitGatewayPrefixListReference CreateTransitGatewayRoute CreateTransitGatewayRouteTable CreateTransitGatewayRouteTableAnnouncement DeleteTransitGatewayPrefixListReference DeleteTransitGatewayRoute DeleteTransitGatewayRouteTable DeleteTransitGatewayRouteTableAnnouncement DescribeTransitGatewayRouteTableAnnouncements DescribeTransitGatewayRouteTables DisableTransitGatewayRouteTablePropagation DisassociateTransitGatewayRouteTable EnableTransitGatewayRouteTablePropagation ExportTransitGatewayRoutes GetTransitGatewayAttachmentPropagations GetTransitGatewayPrefixListReferences GetTransitGatewayRouteTableAssociations GetTransitGatewayRouteTablePropagations ModifyTransitGatewayPrefixListReference ReplaceTransitGatewayRoute SearchTransitGatewayRoutes Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions AWS Site-to-Site VPN actions AWS Verified Access actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api.pdf#operation-list-tgw", "content": "No main content found."}, {"title": "What is AWS Global Networks for Transit Gateways? - AWS Network Manager", "url": "https://docs.aws.amazon.com/network-manager/latest/tgwnm/index.html", "content": "What is AWS Global Networks for Transit Gateways? PDF AWS Global Networks for Transit Gateways enables you to create one or more global networks and then centrally manage those global networks across AWS accounts,\n        Regions, and on-premises locations. Note If you want to create a core network within one of your global networks you'll use AWS Cloud WAN to\n            create, manage, and monitor that core network. For more information on creating a global\n            network with a core network, see the AWS Cloud WAN User\n                    Guide . Global networks concepts The following are the key concepts when using global networks to manage transit gateways. Global network \u2014 A single, private\n                    network that acts as the high-level container for your network objects. A global\n                    network can contain both AWS Transit Gateways and, if you're using AWS Cloud WAN, other Cloud WAN\n                    core networks. You can see these through the AWS Network Manager console. Device \u2014 Represents a physical or a\n                    virtual appliance in an on-premises network, data center,  AWS Cloud, or other\n                    cloud providers. Connection \u2014 Represents connectivity\n                    between two devices. The connection can be between a physical or virtual\n                    appliance and a third-party virtual appliance inside a VPC, or it can be between\n                    physical appliances in an on-premises network. Link \u2014 Represents a single internet\n                    connection from a site. Site \u2014 Represents a physical\n                    on-premises location. It could be a branch, office, store, campus, or a data\n                    center. Home Region The home Region is the AWS Region where data related to your use\n                of your AWS Global Networks for Transit Gateways global network is aggregated and stored. global networks aggregates and\n                stores this information in the home Region to provide you with a\n                central dashboard with visualized insights into your global network. Currently,\n                global networks only supports US West (Oregon) as the home Region. Important Global networks aggregates and stores regional usage data associated with the\n                            transit gateways specified from the AWS Regions you're using to the\n                            US West (Oregon) Region. Gobal networks aggregates and stores regional usage data associated with the\n                            transit gateways from the AWS GovCloud (US) Regions to the\n                            AWS GovCloud (US-West) Region. Once established, you can't change the home\n                            Region. We aggregate and store this regional usage data from the AWS Regions you are\n                using to US West (Oregon) using Amazon Simple Queue Service (SQS) and Amazon Simple Storage Service\n                (S3). This data includes but is not limited to: Topology data for registered transit gateways Event data for transit gateways and VPNs Transit gateway IDs for registering transit gateways into a global network (Optional) Location data related to your device and site\n                        registrations (Optional) Provider and link data related to your link\n                        registrations (Optional) IP address and CIDR ranges used in transit gateway Connect peers All movement and data aggregation occurs over a secure and encrypted channel and\n                stored with encryption at rest. We use a third-party, Mapbox, to create maps of your\n                global network. We send the resource identifiers collected during device and site\n                registrations to Mapbox to generate those maps. Region availability AWS Global Networks for Transit Gateways is available in the following AWS Regions: AWS Region Description af-south-1 Africa (Cape Town) ap-east-1 Asia Pacific (Hong Kong) ap-northeast-1 Asia Pacific (Tokyo) ap-northeast-2 Asia Pacific (Seoul) ap-northeast-3 Asia Pacific (Osaka) ap-south-1 Asia Pacific (Mumbai) ap-south-2 Asia Pacific (Hyderabad) ap-southeast-1 Asia Pacific (Singapore) ap-southeast-2 Asia Pacific (Sydney) ap-southeast-3 Asia Pacific (Jakarta) ap-southeast-4 Asia Pacific (Melbourne) ca-central-1 Canada (Central) ca-west-1 Canada West (Calgary) eu-central-1 Europe (Frankfurt) eu-central-2 Europe (Zurich) eu-north-1 Europe (Stockholm) eu-south-1 Europe (Milan) eu-south-2 Europe (Spain) eu-west-1 Europe (Ireland) eu-west-2 Europe (London) eu-west-3 Europe (Paris) il-central-1 Israel (Tel Aviv) me-central-1 Middle East (UAE) me-south-1 Middle East (Bahrain) sa-east-1 South America (S\u00e3o Paulo) us-east-1 US East (N. Virginia) us-east-2 US East (Ohio) us-west-1 US West (N. California) us-west-2 US West (Oregon) us-gov-east-1 AWS GovCloud (US-East) us-gov-west-1 AWS GovCloud (US-West) How to get started with global networks for transit gateways Use the following resources to help you use global networks: How AWS Global Networks for Transit Gateways works Get started with AWS Global Networks for Transit Gateways Access transit gateway network dashboards using AWS Network Manager Pricing There are no additional fees for using global networks to manage transit gateways networks. You are\n            charged the standard fees for the network resources that you manage in your global\n            network (such as transit gateways). For more information about pricing, see AWS Transit Gateway pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How global networks work"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/network-manager/latest/tgwnm/tgwnm.pdf", "content": "No main content found."}, {"title": "What is AWS Cloud WAN? - AWS Network Manager", "url": "https://docs.aws.amazon.com/network-manager/latest/cloudwan/index.html", "content": "What is AWS Cloud WAN? PDF AWS Cloud WAN is a managed wide-area networking (WAN) service that you can use to build, manage,\n    and monitor a unified global network that connects resources running across your cloud and\n    on-premises environments. It provides a central dashboard from which you can connect on-premises\n    branch offices, data centers, and Amazon Virtual Private Clouds (VPCs) across the AWS global\n    network. You can use simple network policies to centrally configure and automate network\n    management and security tasks, and get a complete view of your global network. For key concepts\n    and terms about global and core networks, see Global and core network key concepts . Note AWS Cloud WAN is designed to work with a core network. You can create a core network at the time\n      you create your global network, or you can create one later on. If you want to create a global\n      network without using a core network, use AWS Global Networks for Transit Gateways. For more\n      information, see the AWS Global\n          Networks for Transit Gateways User Guide . There are a number of ways you can work with AWS Cloud WAN to create and maintain your core network, policies, segments, and attachments: AWS Management console The AWS Management console provides a web interface for you to create your global and\n        core networks, policy versions, segments, and attachments. For more information on using the\n        console to create and maintain your global and core networks, see Quick start: Create a global and core network . AWS Command Line Interface (AWS CLI) Provides command-line support for a broad set of AWS services using the command line.\n        For more information see the Amazon EC2 command line\n          reference , which includes AWS Transit Gateway and Amazon VPC and the AWS Global Networks for Transit Gateways API\n        Reference . AWS SDKs Provides language-specific API operations and takes care of a number of connection\n        details, such as calculating signatures, handling request retries, and handling errors. For\n        more information, see the AWS Global Networks for Transit Gateways API Reference . Query API Provides low-level API actions using HTTPS requests. Using the Query API is the most direct\n        way to access Amazon VPC, but it requires that your application handle low-level details\n        such as generating the hash to sign the request, and handling errors. For more information,\n        see the Amazon EC2\n            API Reference . Global and core network key concepts The following are the key concepts for AWS Cloud WAN: Global network A single, private network that acts as the high-level container for your\n          network objects. A global network can contain both AWS Transit Gateways and other AWS Cloud WAN\n          core networks. These can be seen in the Network Manager console. Core network The part of your global network managed by AWS. This includes Regional\n          connection points and attachments, such as VPNs, VPCs, and Transit Gateway\n          Connects. Your core network operates in the Regions that are defined in your\n          core network policy document. Core network policy A core network policy document is a single document applied to your core\n          network that captures your intent and deploys it for you. The core network\n          policy is a declarative language that defines segments, AWS Region routing,\n          and how attachments should map to segments. With a core network policy, you can\n          describe your intent for access control and traffic routing, and AWS Cloud WAN handles\n          the configuration details. Some examples of advanced architectures that you can\n          create with policy include creating a segment for shared services (for example,\n          service directories or authentication services), providing internet access\n          through a firewall for a segment, automatically assigning VPCs to segments based\n          on tags, and defining which AWS Regions a segment is available in. Over time you might find that you want to make adjustments or additions to\n          your core network policy. With a policy, you can make any changes or additions\n          to your core network and apply those changes through an updated JSON policy. You\n          can do this using either the visual editor on the console, or through an\n          included JSON editor. You can maintain multiple versions of a policy, although\n          only one policy can be in effect. At any time, you can update your core network\n          to use a new policy or revert to a previous version. Attachments Attachments are any connections or resources that you want to add to your core\n          network. Supported attachments include VPCs, VPNs, Transit Gateway route table\n          attachments, and Connect attachments. Core network edge The Regional connection point managed by AWS in each Region, as defined in the core\n          network policy. Every attachment connects to a core network edge. This is also known as an\n          AWS Transit Gateway, and it inherits many of the same properties. In your core network policy document, you define the AWS Region where you want\n          connectivity. At any time, you can add or remove AWS Regions using the policy document.\n          For each AWS Region that you define in the policy document, AWS Cloud WAN then creates a core\n          network edge router in the specified Region.All core network edges in your core network\n          create full-mesh peering with each other to form a highly resilient network. Traffic\n          across the AWS global network uses redundant connections and multiple paths. Network segments Segments are dedicated routing domains, which means that by default, only\n          attachments within the same segment can communicate. You can define segment\n          actions that share routes across segments in the core network policy. In a\n          traditional network, a segment is similar to a globally consistent Virtual\n          Routing and Forwarding (VRF) table, or a Layer 3 IP VPN over an MPLS\n          network. AWS Cloud WAN supports built-in segmentation, which means that you can more easily\n          manage network isolation across your AWS and on-premises locations. Using\n          network segments, you can divide your global network into separate isolated\n          networks. For example, you might want to isolate traffic between different parts\n          of your business, such as between retail sites or IT networks. You can create a segment and define whether resources that ask for access\n          require approval. You can also define explicit route filters to be applied\n          before those routes can be attached to a segment. Each attachment connects to\n          one segment. Each segment will create a dedicated routing domain. You can create\n          multiple network segments within your global network. Resources connected to the\n          same segment can only communicate within the segment. Optionally, resources in\n          the same segment can be isolated from each other, with access only to shared\n          services. With segments, AWS maintains a consistent configuration across\n          AWS Regions for you, instead of you needing to synchronize configuration\n          across every device in your network. Segment actions and attachment policies Segment actions define how routing works between segments. After you create a\n          segment, you can choose to map attachments to the segments either by explicitly\n          mapping a resource to a segment (for example, \" VpcId:\n            \"vpc-2f09a348 ) or by creating and using attachment policies. Instead of\n          manually associating a segment to each attachment, attachments are tagged. Those\n          tags are then associated with the applicable segment. When attachments are\n          mapped to segments, you can choose how routes are shared between segments. For\n          example, you might want to share access to a VPN across multiple segments, or\n          allow access between two types of branch offices. You can also choose to\n          configure centralized internet routing for a segment, or route traffic between\n          segments through a firewall. Core network owner and Attachment\n          owner When creating a core network within a global network, the user that creates\n          the core network automatically becomes the owner of the core network. A core\n          network owner has full control and visibility over all parts of the AWS Cloud WAN\n          network. The core network owner can then share a core network across accounts or\n          across an organization using AWS Resource Access Manager. For more information, see Shared AWS Cloud WAN core network . The account to which the core network is\n          shared becomes an attachment owner. An attachment owner has permission only to\n          create connections, attachments, or tags, but no permission for any core network\n          tasks. A core network owner can also be an attachment owner. A core network owner can: Create, update, restore, delete, or share a Cloud WAN network. Create, update, download, run, delete, or restore core network policy\n              versions. Create, update, or delete core network attachments. Accept or reject core network attachments. Create, update, or remove attachment tags. Visualize network topology and policy change sets. Track network events, routes, and performance. Create sites, links, devices, and other transit gateway associations. An attachment owner can: Create, update, or delete VPC attachments. Add, update, or remove attachment tags. Peering You can interconnect your core network edge and transit gateway in the same AWS Region\n          using a peering connection. You can create one or more route table attachments\n          over a peering connection to peer a transit gateway route table through a Cloud WAN\n          network segment, allowing you to deploy end-to-end network segmentation across\n          your transit gateway and Cloud WAN-based networks. Home Region The home Region is the AWS Region where data related to your use of\n      your AWS Cloud WAN core network is aggregated and stored. Cloud WAN aggregates and stores this\n      information in the home Region to provide you with a central dashboard\n      with visualized insights into your global network. Currently, Cloud WAN only supports\n      US West (Oregon) as the home Region. Important Cloud WAN aggregates and stores Regional usage data associated with the\n            core network edges specified in your core network policy from the\n            AWS Regions you're using to the US West (Oregon) Region. When it has been established, you can't change the home Region. AWS aggregates and stores this Regional usage data from the AWS Regions that you\n      are using to US West (Oregon), using Amazon Simple Queue Service (SQS) and Amazon Simple Storage Service\n      (S3). This data includes but is not limited to: Topology data for registered transit gateways Event data for transit gateways and VPNs Transit gateway IDs for registering transit gateways into a global network (Optional) Location data related to your device and site registrations (Optional) Provider and link data related to your link registrations (Optional) IP address and CIDR ranges used in Cloud WAN and transit gateway Connect\n          peers All movement and data aggregation occurs over a secure and encrypted channel and\n      stored with encryption at rest. We use a third-party, Mapbox, to create maps of your\n      global network. We send the resource identifiers collected during device and site\n      registrations to Mapbox to generate those maps. Region availability AWS Cloud WAN is available in the following AWS Regions: AWS Region Description us-east-1 US East (N. Virginia) us-east-2 US East (Ohio) us-west-1 US West (N. California) us-west-2 US West (Oregon) af-south-1 Africa (Cape Town) ap-northeast-1 Asia Pacific (Tokyo) ap-northeast-2 Asia Pacific (Seoul) ap-northeast-3 Asia Pacific (Osaka) ap-south-1 Asia Pacific (Mumbai) ap-south-2 Asia Pacific (Hyderabad) ap-southeast-1 Asia Pacific (Singapore) ap-southeast-2 Asia Pacific (Sydney) ap-southeast-3 Asia Pacific (Jakarta) ap-southeast-4 Asia Pacific (Melbourne) ca-central-1 Canada (Central) ca-west-1 Canada West (Calgary) eu-central-1 Europe (Frankfurt) eu-central-2 Europe (Zurich) eu-north-1 Europe (Stockholm) eu-west-1 Europe (Ireland) eu-west-2 Europe (London) eu-west-3 Europe (Paris) eu-south-1 Europe (Milan) eu-south-2 Europe (Spain) il-central-1 Israel (Tel Aviv) me-central-1 Middle East (UAE) me-south-1 Middle East (Bahrain) Cloud WAN pricing Information about Cloud WAN pricing can be found here, AWS Cloud WAN Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Quick start: Create a global and core network"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/network-manager/latest/cloudwan/cloudwan.pdf", "content": "No main content found."}, {"title": "Welcome - AWS Global Networks for Transit Gateways", "url": "https://docs.aws.amazon.com/networkmanager/latest/APIReference/index.html", "content": "Welcome PDF Amazon Web Services enables you to centrally manage your AWS Cloud WAN core network and your Transit Gateway network across AWS accounts, Regions, and on-premises locations. This document was last published on October 8, 2024. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/networkmanager/latest/APIReference/network-manager-api.pdf", "content": "No main content found."}, {"title": "What is Reachability Analyzer? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/reachability/index.html", "content": "What is Reachability Analyzer? Reachability Analyzer is a configuration analysis tool that enables you to perform connectivity testing\n    between a source resource and a destination resource in your virtual private clouds (VPCs). When\n    the destination is reachable, Reachability Analyzer produces hop-by-hop details of the virtual network path\n    between the source and the destination. When the destination is not reachable, Reachability Analyzer identifies\n    the blocking component. For example, paths can be blocked by configuration issues in a security\n    group, network ACL, route table, or load balancer. For more information, see How Reachability Analyzer works . Use cases You can use Reachability Analyzer to do the following: Troubleshoot connectivity issues caused by network misconfiguration. Verify that your network configuration matches your intended connectivity. Automate the verification of your connectivity intent as your network configuration\n          changes. Working with Reachability Analyzer You can use any of the following interfaces to work with Reachability Analyzer: AWS Management Console \u2014 A web interface for AWS services,\n          including Reachability Analyzer. AWS Command Line Interface (AWS CLI) \u2014 Provides commands for AWS\n          services, including Reachability Analyzer. The AWS CLI is supported on Windows, macOS, and Linux. For more\n          information, see the AWS Command Line Interface User Guide . AWS CloudFormation \u2014 Enables you to create templates that\n          describe your AWS resources. You use a template to provision and manage AWS resources   \n          as a single unit. For more information, see the following resources: AWS::EC2::NetworkInsightsAnalysis and AWS::EC2::NetworkInsightsPath . AWS SDKs \u2014 Provide language-specific APIs and\n          take care of many of the connection details, such as calculating signatures, handling\n          request retries, and handling errors. For more information, see AWS SDKs . Query API \u2014 Provides low-level API actions that\n          you call using HTTPS requests. Using the Query API is the most direct way to access Reachability Analyzer.\n          However, the Query API requires that your application handle low-level details such as\n          generating the hash to sign the request, and handling errors. For more information, see\n          the Amazon EC2 API Reference . Pricing You are charged per analysis run between a source and destination. For more information,\n      see Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How Reachability Analyzer works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/reachability/reachability-ug.pdf", "content": "No main content found."}, {"title": "What is Infrastructure Performance for AWS Network Manager? - AWS Network Manager", "url": "https://docs.aws.amazon.com/network-manager/latest/infrastructure-performance/index.html", "content": "What is Infrastructure Performance for AWS Network Manager? PDF RSS Infrastructure Performance allows you to obtain near real-time and historical network latency across\n    AWS Regions and across or within Availability Zones for a specified time period. However,\n    network performance data is not available prior to Oct 26 2022 00:00:00 GMT+000. Performance\n    metrics are aggregated according to a time period that you choose. Network performance results\n    are returned for that time period, showing whether the performance was healthy or if there was\n    any performance degradation. This can help  you to more easily evaluate whether network\n    performance might affect your applications. Infrastructure Performance also supports publishing metrics as\n    subscriptions to Amazon CloudWatch, allowing you to view performance metrics in CloudWatch. There is no cost\n    associated with using Infrastructure Performance. However, you might incur charges when using subscriptions to\n    view performance in CloudWatch. For more information about CloudWatch pricing guidelines, see Amazon CloudWatch pricing . Note Infrastructure Performance is designed to give you an overview showing whether network performance is\n   normal or degraded. It does not provide details about the specific causes of degradation, such as\n   a hardware failure. Infrastructure Performance works in the following geographical areas: Availability zones \u2014 Generate network performance metrics between two\n    Availability Zones or within a single Availability Zone. Regions \u2014 Generate network performance metrics to see\n    performance between two Regions. You can use the Region metrics to evaluate a Region\n        expansion strategy. Infrastructure Performance can help you gather information needed to understand\n        network performance between Regions where you currently have a presence, and\n        Regions that you're exploring expanding into. For example, you might have a\n        presence in us-west-2 and are looking to expand into eu-central- 1.\n        Use Infrastructure Performance to check the latency between these two Regions to help you\n        make a more informed choice for Region expansion. For a list of the\n        Regions that support Infrastructure Performance, see Region availability . Note Infrastructure Performance does not incorporate performance metrics for paths through VPC networking resources,\n   such as transit gateways, NAT gateways, VPC endpoints, Elastic Load Balancing, or Amazon EC2 network interfaces. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How Infrastructure Performance works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/network-manager/latest/infrastructure-performance/infrastructure-performance.pdf", "content": "No main content found."}, {"title": "What is Network Access Analyzer? - Amazon Virtual Private Cloud", "url": "https://docs.aws.amazon.com/vpc/latest/network-access-analyzer/index.html", "content": "What is Network Access Analyzer? PDF RSS Network Access Analyzer is a feature that identifies unintended network access to your resources on AWS. You\n    can use Network Access Analyzer to specify your network access requirements and to identify potential network\n    paths that do not meet your specified requirements. You can use Network Access Analyzer to: Understand, verify, and improve your network security\n          posture \u2013 Network Access Analyzer helps you identify unintended network access relative to\n        your security and compliance requirements, enabling you to take steps to improve your\n        network security. Demonstrate compliance \u2013 Network Access Analyzer helps you\n        demonstrate that your network on AWS meets your compliance requirements. Network Access Analyzer can help you verify the following example requirements: Network segmentation \u2013 Verify that your\n        production environment VPCs and development environment VPCs are isolated from one another.\n        Likewise, you can verify that a separate logical network is used for systems that process\n        credit card information, and that it's isolated from the rest of your environment. Internet accessibility \u2013 Identify resources in\n        your environment that can be accessed from internet gateways, and verify that they are\n        limited to only those resources that have a legitimate need to be accessible from the\n        internet. Trusted network paths \u2013 Verify that you have\n        appropriate network controls such as network firewalls and NAT gateways on all network paths\n        between your resources and internet gateways. Trusted network access \u2013 Verify that your\n        resources have network access only from a trusted IP address range, over specific ports and\n        protocols. You can specify your network access requirements in terms of: Resource IDs (for example, vpc-1234567890abcdef0 ) Resource types (for example, AWS::EC2::InternetGateway ) Resource tags IP address ranges, port ranges, and traffic protocols Network Access Analyzer concepts The following are the key concepts for Network Access Analyzer: Network Access Scopes You can specify your network access requirements as Network Access Scopes, which determine\n            the types of findings that the analysis produces. You add entries to MatchPaths to specify the types of network paths to identify. You add entries to ExcludePaths to specify the types of network paths to exclude. MatchPaths \u2013 Specifies the types of network paths\n                that an analysis produces. Typically, you specify network paths that you consider to\n                be a violation of your security or compliance requirements. For example, if you\n                don't want to allow network paths that start in VPC A and end in VPC B, specify VPC\n                A as a source and VPC B as a destination. When you analyze this Network Access\n                Scope, you would see any findings that indicate any potential network paths that\n                start in VPC A and end in VPC B. ExcludePaths \u2013 Prevents certain network paths from appearing \n                in your findings. Typically, you specify network paths that you consider to be a legitimate \n                exception to your network security or compliance requirements. For example, to identify all \n                network interfaces that are reachable from an internet gateway except for your web servers, \n                specify the relevant paths using MatchPaths , and then exclude any path \n                with your web servers as a destination using ExcludePaths . When you analyze \n                this Network Access Scope, you would see any network paths that originate from an internet \n                gateway and end at a network interface, except for any paths that end at your web servers. Findings Findings are potential paths in your network that match any of the MatchPaths entries in your Network Access Scope,\n            but do not match any of the ExcludePaths entries in \n            your Network Access Scope. Access Network Access Analyzer You can use any of the following interfaces to access and work with Network Access Analyzer: AWS Management Console \u2013 Provides a web interface that you can\n          use to create and manage Network Access Analyzer resources. AWS Command Line Interface (AWS CLI) \u2013 Provides commands for \n          AWS services including Network Access Analyzer. The AWS CLI is supported on Windows, macOS, and Linux. \n          For more information, see the AWS Command Line Interface User Guide . AWS CloudFormation \u2013 Create templates to provision and manage \n          AWS resources as a single unit. For more information, see AWS::EC2::NetworkInsightsAccessScope and AWS::EC2::NetworkInsightsAccessScopeAnalysis . AWS SDKs \u2013 Provides language-specific APIs and\n          takes care of many of the connection details, such as calculating signatures, and handling\n          request retries and errors. For more information, see Tools to build on AWS . Query API \u2013 Provides low-level API actions that\n          you call using HTTPS requests. Using the Query API is the most direct way to access Network Access Analyzer.\n          However, the Query API requires your application to handle low-level details such as\n          generating the hash to sign the request and handling errors. For more information, see Amazon VPC actions in the Amazon EC2 API Reference . Pricing When you run a Network Access Analyzer analysis, you are charged based on the number of network interfaces\n      that are analyzed. For more information, see Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How Network Access Analyzer works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc/latest/network-access-analyzer/network-access-analyzer.pdf", "content": "No main content found."}, {"title": "AWS Network Manager actions - Amazon Elastic Compute Cloud", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/OperationList-query-networkmanager.html", "content": "AWS Network Manager actions PDF The following API actions are available for AWS Network Manager. Infrastructure Performance DescribeAwsNetworkPerformanceMetricSubscriptions DisableAwsNetworkPerformanceMetricSubscription EnableAwsNetworkPerformanceMetricSubscription GetAwsNetworkPerformanceData Network Access Analyzer CreateNetworkInsightsAccessScope DeleteNetworkInsightsAccessScope DeleteNetworkInsightsAccessScopeAnalysis DescribeNetworkInsightsAccessScopeAnalyses DescribeNetworkInsightsAccessScopes GetNetworkInsightsAccessScopeAnalysisFindings GetNetworkInsightsAccessScopeContent StartNetworkInsightsAccessScopeAnalysis Reachability Analyzer CreateNetworkInsightsPath DeleteNetworkInsightsAnalysis DeleteNetworkInsightsPath DescribeNetworkInsightsAnalyses DescribeNetworkInsightsPaths EnableReachabilityAnalyzerOrganizationSharing StartNetworkInsightsAnalysis Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Amazon EC2 actions AWS Nitro Enclaves"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/AWSEC2/latest/APIReference/ec2-api.pdf#OperationList-query-networkmanager", "content": "No main content found."}, {"title": "What is Amazon VPC Lattice? - Amazon VPC Lattice", "url": "https://docs.aws.amazon.com/vpc-lattice/latest/ug/index.html", "content": "What is Amazon VPC Lattice? PDF RSS Amazon VPC Lattice is a fully managed application networking service that you use to connect,\n  secure, and monitor the services for your application. You can use VPC Lattice with a single virtual\n  private cloud (VPC) or across multiple VPCs from one or more accounts. Modern applications can consist of multiple small and modular services, which are often\n  called microservices . While modernization has its advantages,\n  it can also introduce networking complexities and challenges when you connect these microservices. \n  For example, if the developers are spread across different teams, they might build and deploy \n  microservices across multiple accounts or VPCs. In VPC Lattice, we refer to a microservice as a service .\n  This is the wording that you see in the VPC Lattice documentation. Contents Key components Roles and responsibilities Features How VPC Lattice works Accessing VPC Lattice Pricing Key components To use Amazon VPC Lattice, you should be familiar with its key components. Service An independently deployable unit of software that delivers a specific task or function. \n      A service can run on EC2 instances or ECS containers, or as Lambda functions, within an account \n      or a virtual private cloud (VPC). A VPC Lattice service has the following components: target groups, \n      listeners, and rules. Target group A collection of resources, also known as targets, that run your application or service.\n      Targets can be EC2 instances, IP addresses, Lambda functions, Application Load Balancers, or Kubernetes Pods . These are\n       similar to the target groups provided by Elastic Load Balancing, but they are not interchangeable. Listener A process that checks for connection requests, and routes them to targets in a target\n      group. You configure a listener with a protocol and a port number. Rule A default component of a listener that forwards requests to the targets in a VPC Lattice \n      target group. Each rule consists of a priority, one or more actions, and one or more conditions. \n      Rules determines how the listener routes client requests. Service network A logical boundary for a collection of services. A client is any resource deployed in a\n      VPC that is associated with the service network. Clients and services that are associated with\n      the same service network can communicate with each other if they are authorized to do\n      so. In the following figure, the clients can communicate with both services, because the VPC\n      and services are associated with the same service network. Service directory A central registry of all VPC Lattice services that you own or are shared with your\n      account through AWS Resource Access Manager (AWS RAM). Auth policies Fine-grained authorization policies that can be used to define access to services. You can\n      attach separate auth policies to individual services or to the service network. For example, you\n      can create a policy for how a payment service running on an auto scaling group of EC2 instances\n      should interact with a billing service running in AWS Lambda. Roles and responsibilities A role determines who is responsible for the setup and flow of information within Amazon VPC Lattice.\n   There are typically two roles, service network owner and service owner, and their\n   responsibilities can overlap. Service network owner \u2013 The service network owner is\n   usually the network administrator or the cloud administrator in an organization. Service network\n   owners create, share, and provision the service network. They also manage who can access the\n   service network or services within VPC Lattice. The service network owner can define coarse-grained\n   access settings for the services associated with the service network. These controls are used to\n   manage communication between clients and services using authentication and authorization\n   policies. The service network owner can also associate a service with the service network, if the\n   service is shared with the service network owner's account. Service owner \u2013 The service owner is usually a software\n   developer in an organization. Service owners create services within VPC Lattice, define routing\n   rules, and also associate services with the service network. They can also define fine-grained\n   access settings, which can restrict access to only authenticated and authorized services and\n   clients. Features The following are the core features that VPC Lattice provides. Service discovery All clients and services in VPCs associated with the service network can communicate with\n      other services within the same service network. DNS directs client-to-service and\n      service-to-service traffic through the VPC Lattice endpoint. When a client wants to send a\n      request to a service, it uses the service\u2019s DNS name. The Route\u00a053 Resolver sends the traffic to\n      VPC Lattice, which then identifies the destination service. Connectivity Client-to-service connectivity is established using the VPC Lattice data plane within the\n      AWS network infrastructure. When you associate a VPC with the service network, any client\n      within the VPC can connect with services in the service network, if they have the required\n      access. Observability VPC Lattice generates metrics and logs for each request and response traversing the service\n      network, to help you monitor and troubleshoot applications. By default, VPC Lattice publishes\n      metrics in the service owner account, and gives you the option to turn on logging. If the\n      clients are also associated with the same service network, the service network owner receives\n      logs for all services associated with the service network. The service owner receives logs for\n      all clients making requests to their service. VPC Lattice works with the following tools to help you monitor and troubleshoot your services:\n      CloudWatch log groups, Firehose delivery streams, and S3 buckets. Security VPC Lattice provides a framework that you can use to implement a defense strategy at multiple\n      layers of the network. The first layer is the service and VPC association. Without a VPC and\n      service association, clients can't access the service. The second layer enables users to\n      attach security groups to the association between the VPC and the service network. The third and\n      fourth layers are auth policies that can be applied individually at the service network level and\n      the service level. Accessing VPC Lattice You can create, access, and manage VPC Lattice using any of the following interfaces: AWS Management Console \u2013 Provides a web interface that you can\n     use to access VPC Lattice. AWS Command Line Interface (AWS CLI) \u2013 Provides commands for a\n        broad set of AWS services, including VPC Lattice. The AWS CLI is supported on Windows, MacOS, \n        and Linux. For more information about the CLI, see AWS Command Line Interface . \n       For more information about the APIs, see Amazon VPC Lattice API Reference . VPC Lattice Controller for Kubernetes \u2013\n      Manages VPC Lattice resources for a Kubernetes cluster. For more information about using VPC Lattice \n      with Kubernetes, see the AWS \n       Gateway API Controller User Guide . AWS CloudFormation \u2013 Helps you to model and set up your AWS\n       resources. For more information, see the Amazon VPC \n        Lattice resource type reference . Pricing With VPC Lattice you pay for the time that a service is provisioned, the amount of data transferred\n   through each service, and the number of requests. For more information, see Amazon VPC Lattice Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How VPC Lattice works"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc-lattice/latest/ug/vpc-lattice.pdf", "content": "No main content found."}, {"title": "Welcome - Amazon VPC Lattice", "url": "https://docs.aws.amazon.com/vpc-lattice/latest/APIReference/index.html", "content": "Welcome PDF Amazon VPC Lattice is a fully managed application networking service that you use to connect, secure,\n   and monitor all of your services across multiple accounts and virtual private clouds (VPCs).\n   Amazon VPC Lattice interconnects your microservices and legacy services within a logical boundary, so that\n   you can discover and manage them more efficiently. For more information, see the Amazon VPC Lattice User Guide This document was last published on October 8, 2024. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Actions"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/vpc-lattice/latest/APIReference/vpc-lattice-api.pdf", "content": "No main content found."}, {"title": "vpc-lattice \u2014 AWS CLI 1.35.0 Command Reference", "url": "https://docs.aws.amazon.com/cli/latest/reference/vpc-lattice/", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs#amazon-vpc", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs#amazon-vpc-ip-address-manager", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs#aws-privatelink", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs#aws-transit-gateway", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs#aws-network-manager", "content": "No main content found."}, {"title": "Amazon Virtual Private Cloud Documentation", "url": "https://docs.aws.amazon.com/vpc/?icmpid=docs_homepage_featuredsvcs#amazon-vpc-lattice", "content": "No main content found."}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/index.html", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=a0d18259-7974-4e8f-8382-0a4be53f4374&topic_url=https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Document history - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/doc-history.html", "content": "Document history PDF RSS The following table describes the important changes to this decision guide. \n  For notifications about updates to this guide, you can subscribe to an RSS feed. Change Description Date Updated content Updated Amazon Bedrock feature names, including Amazon Bedrock Agents, Amazon Bedrock Guardrails, Amazon Bedrock Knowledge Bases, and Amazon Bedrock Custom Model Import. August 28, 2024 Updated content Minor updates to improve readability. August 16, 2024 Updated content Updates for newly released features of Amazon Q,\n     Amazon SageMaker, and Amazon Bedrock. July 18, 2024 Initial release Initial release of the decision guide. July 9, 2024 Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Decision Guide"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/pdfs/decision-guides/latest/generative-ai-on-aws-how-to-choose/generative-ai-on-aws-how-to-choose.pdf#guide", "content": "No main content found."}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#introduction", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#understand", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#consider", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#choose", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#use", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#explore", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing a generative AI service - Choosing a generative AI service", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html?icmpid=docs_homepage_featuredsvcs#resources", "content": "Choosing a generative AI service PDF RSS Taking the first step Purpose Determine which AWS generative AI services are the best\n            fit for your organization. Last updated August 28, 2024 Covered services Amazon Bedrock Amazon Bedrock Studio Amazon Q Business Amazon Q Developer Amazon SageMaker Amazon Titan foundation models Public foundation models Introduction Generative AI is a set of artificial intelligence (AI) systems and models designed to generate\n    content such as code, text, images, music, or other forms of data. These systems can produce new\n    content based on patterns and knowledge learned from existing data. Increasingly, organizations\n    and businesses are using generative AI to: Automate creative workflows \u2014 Use generative AI\n        services to automate the workflows of time-consuming creative processes such as writing,\n        image or video creation, and graphic design. Customize and personalize content \u2014 Generate\n        targeted content, product recommendations, and customized offerings for an audience-specific\n        context. Augment data \u2014 Synthesize large training datasets\n        for other ML models to unlock scenarios where human-labeled data is\n        scarce. Reduce cost \u2014 Potentially lower costs by using\n        synthesized data, content, and digital assets. Faster experimentation \u2014 Test and iterate on more\n        content variations and creative concepts than would be possible manually. This guide helps you select the AWS generative AI services and tools that are the best\n    fit for your needs and your organization. A twelve-minute video about building generative AI applications on AWS. Understand Amazon offers a range of generative AI services, applications, tools, and supporting\n    infrastructure. Which of these you use depends a lot on the following factors: What you\u2019re trying to do How much choice you need in the foundation models that you use The degree of customization you need in your generative AI applications The expertise within your organization Amazon Q \u2014 Get pre-defined applications for your use case At the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use\n    large language models (LLMs) and foundation models. However, they don\u2019t require that you\n    explicitly choose a model. Each of these applications is aimed at a different use case and all\n    are powered by Amazon Bedrock . Learn more about the primary Amazon Q generative AI\u2013powered assistants currently available: Amazon Q Business Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Q Developer With Amazon Q Developer , you can understand, build, extend, and operate AWS applications.\n          The supported use cases include tasks that range from coding, testing, and upgrading\n          applications, to diagnosing errors, performing security scanning and fixes, and optimizing\n          AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer\n          are aimed at reducing the work involved in common tasks (such as performing Java version\n          upgrades). These capabilities can also help implement new features generated from\n          developer requests. Amazon Q Developer is also available as a feature in several other\n            AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC\n          Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly\n          from the AWS Management Console. Using natural language prompts to interact with your AWS account,\n          you can get specific resource details and ask about relationships between\n          resources. Amazon Q in QuickSight Amazon Q in\n            QuickSight is aimed at meeting the needs of a specific use case: getting\n          actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business\n          intelligence (BI) service. You can use it to build visualizations of your data, summarize\n          insights, answer data questions, and build data stories using natural language. Amazon Q in Connect Amazon Q\n            in Connect can automatically detect customer issues. It provides your customer\n          service agents with contextual customer information along with suggested responses and\n          actions for faster resolution of issues. It combines the capabilities of the Amazon\n            Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your\n          real-time conversations with your customers, along with relevant company content, to\n          recommend what to say or what actions an agent should take to assist customers. anchor anchor anchor anchor Amazon Q Business Amazon Q Developer Amazon Q in QuickSight Amazon Q in Connect Amazon Q Business can answer questions, provide summaries, generate content, and\n          securely complete tasks based on the data in your enterprise systems. It supports the\n          general use case of using generative AI to start making the most of the information in\n          your enterprise. With Amazon Q Business, you can make English-language queries about that\n          information. It provides responses in a manner appropriate to your team\u2019s needs. In\n          addition, you can create lightweight, purpose-built Amazon Q Apps within\n          your Amazon Q Business Pro subscription. Amazon Bedrock \u2014 Choose your foundation models If you're developing custom AI applications, need access to multiple foundation models, and\n    want more control over the AI models and outputs, then Amazon Bedrock could be the service that\n    meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular\n    foundation models, including Anthropic\n      Claude , Cohere Command\n      & Embed , AI21 Labs Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon Titan . In addition, Amazon Bedrock provides what you need to build generative AI applications with\n    security, privacy, and responsible AI\u2014regardless of the foundation model you choose. It also\n    offers model-independent, single API access and the flexibility to use different foundation\n    models and upgrade to the latest model versions, with minimal code changes. Learn more about the key features of Amazon Bedrock: Model customization Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Agents Amazon Bedrock Agents helps you plan and create multistep tasks using company systems and\n          data sources\u2014from answering customer questions about your product availability to taking\n          their orders. You can create an agent by first selecting an FM and then providing it\n          access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your\n          APIs securely. An agent analyzes the user request, and a Lambda function or your\n          application can automatically call the necessary APIs and data sources to fulfill the\n          request. Agents can retain memory across multiple interactions to remember where you last left\n          off and provide better recommendations based on prior interactions. Agents can also\n          interpret code to tackle complex data-driven use cases, such as data analysis, data\n          visualization, text processing, solving equations, and optimization problems. Guardrails Amazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific\n          policies, and provides an additional layer of safeguards, regardless of the underlying FM.\n          Using a short natural language description, you can use Amazon Bedrock Guardrails to define\n          a set of topics to avoid within the context of your application. Guardrails detects and\n          blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks , to detect hallucinations in model responses for\n          applications using Retrieval Augmented Generation (RAG) and summarization applications.\n          Contextual grounding checks add to the safety protection in Guardrails to make sure the\n          LLM response is based on the right enterprise source data, and evaluates the LLM response\n          to confirm that it\u2019s relevant to the user\u2019s query or instruction. Contextual grounding\n          checks can detect and filter over 75% hallucinated responses for RAG and summarization\n          workloads. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed capability that you can use to\n          implement the entire Retrieval Augmented Generation (RAG) workflow\u2014from ingestion\n          to retrieval and prompt augmentation\u2014without having to build custom integrations to\n          data sources, and manage data flows. Session context management is built in, so your\n          application can support multi-turn conversations. You can use the Retrieve API to fetch\n          relevant results for a user query from knowledge bases. With RAG, you can provide a model with new knowledge or up-to-date info from multiple\n          sources, including document repositories, databases, and APIs. For example, the model\n          might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service.\n          Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data\n          sources, including Amazon Aurora , Amazon OpenSearch\n            Serverless , MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes\n          connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more\n          business data to customize models for your specific needs. Converse API Use the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an\n          Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over\n          many turns and uses a persona or tone customization that is unique to your needs, such as\n          a helpful technical support assistant. Tool use (function calling) Tool use\n            (function calling) gives a model access to tools that can help it generate\n          responses for messages that you send to the model. For example, you might have a chat\n          application that lets users find out the most popular song played on a radio station. To\n          answer a request for the most popular song, a model needs a tool that can query and return\n          the song information. Amazon Bedrock Studio Explore Amazon Bedrock Studio (in\n          preview), an SSO-enabled web interface that provides a way for developers across your\n          organization to experiment with LLMs and other FMs, collaborate on projects, and iterate\n          on generative AI applications. It offers a rapid prototyping environment and streamlines\n          access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports\n          Amazon Bedrock Knowledge Bases\n and Amazon Bedrock Guardrails. Prompt management Use Amazon Bedrock to create and save your own prompts using Prompt management , so that\n          you can save time by applying the same prompt to different workflows. When you create a\n          prompt, you can select a model to run inference on it and modify the inference parameters\n          to use. You can include variables in the prompt so that you can adjust the prompt for\n          different use case. Prompt flows Prompt\n            flows for Amazon Bedrock offers the ability for you to use supported FMs to build\n          workflows by linking prompts, foundational models, and other AWS services to create\n          comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a\n          visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and\n          other AWS services such as AWS Lambda by transferring data between them. You can also\n          deploy immutable workflows to move from testing to production in few clicks. anchor anchor anchor anchor anchor anchor anchor anchor anchor Model customization Agents Guardrails Knowledge Bases Converse API Tool use (function calling) Amazon Bedrock Studio Prompt management Prompt flows Model\n            customization can deliver differentiated and personalized user experiences. To\n          customize models for specific tasks, you can privately fine-tune FMs using your own\n          labeled datasets. Custom models include capabilities such as fine-tuning and continued\n          pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports\n          fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express,\n          Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n          (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. In addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them\n          within Amazon Bedrock. Amazon SageMaker \u2014 Build custom models and control the full ML\n      lifecycle, from data preparation to model deployment and monitoring With Amazon SageMaker , you\n    can build, train, and deploy machine learning models, including FMs, at scale. Consider this\n    option when you have use cases that can benefit from extensive training, fine-tuning, and\n    customization of foundation models. It also streamlines the sometimes-challenging task of\n    evaluating which FM is the best fit for your use case. Amazon SageMaker also provides infrastructure and purpose-built tools for use throughout the ML\n    lifecycle, including integrated development environments (IDEs), distributed training\n    infrastructure, governance tools, machine learning operations (MLOps) tools, inference options\n    and recommendations, and model evaluation. Explore key features of Amazon SageMaker that may help you determine when to use it: SageMaker JumpStart Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. SageMaker Clarify Amazon SageMaker\n            Clarify addresses the all-important decision of which foundation model to use.\n          Use SageMaker Clarify to create model evaluation jobs. A model evaluation job evaluates and\n          compares model quality and responsibility metrics for text-based foundation models from\n          JumpStart. Model evaluation jobs also support the use of JumpStart models that have\n          already been deployed to an endpoint. SageMaker Canvas With Amazon SageMaker\n            Canvas , you can use machine learning to generate predictions without writing any\n          code. You can also use Amazon SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n          deploy language models. This blog post describes how you can use them to optimize customer interaction\n          by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker\n          JumpStart. The following diagram, from this blog post, demonstrates how SageMaker Canvas and\n          Amazon Bedrock can be used together to fine-tune and deploy language models. SageMaker Studio Amazon SageMaker Studio is a web-based experience for running ML workflows. Studio offers a\n          suite of integrated development environments (IDEs). These include Code Editor, based on\n          Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and\n          Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker\n            Studio . The web-based UI in Studio provides access to all SageMaker resources, including jobs and\n          endpoints, in one interface. ML practitioners can also choose their preferred IDE to\n          accelerate ML development. A data scientist can use JupyterLab to explore data and tune\n          models. In addition, a machine learning operations (MLOps) engineer can use Code Editor\n          with the pipelines tool in Studio to deploy and monitor models in production. SageMaker Studio includes generative AI assistance, powered by Amazon Q Developer right within your\n          JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access\n          expert guidance on SageMaker features, code generation, and troubleshooting. anchor anchor anchor anchor SageMaker JumpStart SageMaker Clarify SageMaker Canvas SageMaker Studio Amazon SageMaker\n            JumpStart is an ML hub that provides access to publicly available foundation\n          models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be\n          customized with advanced fine-tuning and deployment techniques such as Parameter Efficient\n          Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker JumpStart within\n          the AWS Management Console. Infrastructure for FM training and inference AWS offers specialized, accelerated hardware for high performance ML training and\n    inference. Amazon EC2 P5 instances are equipped\n        with NVIDIA H100 Tensor Core GPUs, which are well-suited for both training and inference\n        tasks in machine learning. Amazon EC2 G5 instances feature up to\n        8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range\n        of graphics-intensive and machine learning use cases. AWS Trainium is the\n        second-generation ML accelerator that AWS has purpose-built for deep learning (DL)\n        training of 100B+ parameter models. AWS Inferentia2-based Amazon EC2 Inf2\n          instances are designed to deliver high performance at the lowest cost in Amazon EC2 for\n        your DL and generative AI inference applications. Consider After you've decided on a generative AI service, choose the foundation model (FM) that gives\n    you the best results for your use case. Amazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and\n    selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluaion is now generally available on the AWS News Blog. Here are some critical factors to consider when choosing an appropriate FM for your use\n    case: Modality Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Model size Model Size What it is: This criterion refers to the number\n          of\u00a0parameters\u00a0in a model. A parameter is a\n          configuration variable that is internal to the model. Its values can be estimated\n          (trained) during the training phase from the given training data. Parameters are crucial\n          as they directly define the model's capability to learn from data. Large models often have\n          more than 50 billion parameters. Why it matters: The number of parameters is a key\n          indicator of the model's complexity. More parameters mean that the model can capture more\n          intricate patterns and nuances in the data, which generally leads to better performance.\n          However, these models are not only expensive to train, but also require more computational\n          resources to operate. Inference latency Inference latency What it is: Inference\n            speed , or latency , is the time it takes\n          for a model to process input (often measured in tokens) and return an output.\n          This\u00a0processing time\u00a0is crucial when the model's responses are part of an\u00a0interactive\n          system, like an AWS Chatbot. Why it matters: Quick response times are essential\n          for\u00a0real-time applications\u00a0such as interactive\u00a0chatbots\u00a0or instant\n            translation services. These applications depend on the model's ability to\n          process and respond to prompts rapidly to maintain a smooth user experience. Although\n          larger FMs typically offer more detailed and accurate responses, their complex\n          architectures can lead to slower inference speeds. This slower processing might frustrate\n          users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even\n          if it means compromising somewhat on the responses' depth or accuracy. Context window Maximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model\n          can consider at any one time when generating responses. Why it matters: Larger context windows enable the\n          model to\u00a0remember and process\u00a0more information in a\u00a0single run. This ability is\n          particularly valuable in complex tasks such as understanding long documents, engaging in\n          detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a\u00a0larger\u00a0context window can\n          remember\u00a0more\u00a0of the earlier dialogue, and provide responses that are more relevant to the\n          entire conversation. This leads to a more natural and satisfying user experience, as the\n          model can maintain the thread of discussion without losing context. Pricing Pricing considerations What it is: The cost of using an FM is influenced by\n          the model's complexity and the model provider\u2019s\u00a0pricing structure. Why it matters: Deploying high-performance models\n          often comes with high costs due to increased computational needs. While these models\n          provide advanced capabilities, their operational expenses can be high, particularly for\n          startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without\n          significantly compromising performance. Weigh the model\u2019s cost against its benefits to\n          ensure it fits within your project's financial constraints and gets you the best value for\n          your investment. Fine-tuning Fine-tuning and continuous pre-training capability What it is: Fine-tuning\u00a0is a specialized training\n          process in which a\u00a0pre-trained model\u00a0that has been trained on a large, generic dataset\n          is\u00a0further trained\u00a0(or fine-tuned) on a\u00a0smaller, specific dataset. This process adapts the\n          model to particularities of the new data, improving its performance on related\n          tasks.\u00a0Continuous pre-training, on the other hand, involves extending the initial\n          pre-training phase with additional training on new, emerging data that wasn't part of the\n          original training set, helping the\u00a0model stay relevant as data evolves. You can also use Retrieval\n            Augmented Generation (RAG) to retrieve data from outside an FM  and augment your\n          prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase\n          model accuracy by providing your own task-specific\u00a0labeled training\u00a0dataset and further\n          specialize your FMs. With continued pre-training, you can train models using your\n          own\u00a0unlabeled data\u00a0in a secure and managed environment. Continuous pre-training helps\n          models become more domain-specific by accumulating more robust knowledge and adaptability\n          beyond their original training. Data quality Data quality Data quality is a critical factor in the success of a generative AI application.\n          Consider the following quality factors: Relevance: Ensure that the data you use for\n              training your generative AI model is relevant to your application. Irrelevant or noisy\n              data can lead to poor model performance. Accuracy: The data should be accurate and free\n              from errors. Inaccurate data can mislead your model and result in incorrect outputs. Consistency: Maintain consistency in your data.\n              Inconsistencies in the data can confuse the model and hinder its ability to learn\n              patterns. Bias and fairness: Be aware of biases in your\n              data, as they can lead to biased model outputs. Take steps to mitigate bias and help\n              ensure fairness in your generative AI system. Annotation and labeling: If your application\n              requires labeled data, verify that the annotations or labels are of high quality and\n              created by experts. Data preprocessing: Prepare your data by cleaning\n              and preprocessing it. This might involve text tokenization, image resizing, or other\n              data-specific transformations to make it suitable for training. Data quantity Data quantity Quantity along with quality goes hand in hand. Consider the following quantity\n          factors: Sufficient data: In most cases, more data is\n              better. Larger datasets allow your model to learn a wider range of patterns and\n              generalize better. However, the required amount of data can vary depending on the\n              complexity of your application. Data augmentation: If you have limitations on the\n              quantity of available data, consider data augmentation techniques. These techniques\n              involve generating additional training examples by applying transformations to\n              existing data. For example, you can rotate, crop, or flip images or paraphrase text to\n              create more training samples. Balancing data: Ensure that your dataset is\n              balanced, especially if your generative AI application is expected to produce outputs\n              with equal representation across different categories or classes. Imbalanced datasets\n              can lead to biased model outputs. Transfer learning: For certain applications, you\n              can use pre-trained models. With transfer learning, you can use models that were\n              trained on massive datasets and fine-tune them with your specific data, often\n              requiring less data for fine-tuning. It's also important to continuously monitor and update your dataset as your generative\n          AI applications evolve and as new data becomes available. Quality of response Quality of response What it is: The most essential criterion is\n          the\u00a0quality of response. This is where you evaluate the output of a model based on several\n          quality metrics, including\u00a0accuracy,\u00a0relevance,\u00a0toxicity,\u00a0fairness, and\u00a0robustness against\n          adversarial attacks. Accuracy measures how often the model's responses are correct\n              (and you would typically measure this against a pre-configured standard or\n              baseline). Relevance assesses how appropriate the responses are to the\n              context or question posed. Toxicity checks for harmful biases or inappropriate content\n              in the model's outputs. Fairness evaluates whether the model's responses are unbiased\n              across different groups. Robustness indicates how well the model can handle\n              intentionally misleading or malicious inputs designed to confuse it. Why it matters: The reliability and safety of model\n          outputs are paramount, especially in applications that interact directly with users or\n          make automated decisions that can affect people's lives.\u00a0High-quality\n          responses\u00a0ensure\u00a0user trust\u00a0and\u00a0satisfaction, reducing the risk of miscommunication and\n          enhancing the overall user experience, thus earning the trust of your customers. anchor anchor anchor anchor anchor anchor anchor anchor anchor Modality Model size Inference latency Context window Pricing Fine-tuning Data quality Data quantity Quality of response Identify use cases/modality What it is: Modality refers to the type of data the\n          model processes: text,\u00a0images (vision), or\u00a0embeddings. Why it matters: The choice of modality should align\n          with the data that you're working with. For example, if your project involves processing\n          natural language, a\u00a0text-based\u00a0model like Claude, Llama 3.1, or Titan Text G1 is suitable.\n          If you want to create\u00a0embeddings, then you might use a model like Titan\n            Embeddings G1 . Similarly, for\u00a0image-related\u00a0tasks, models such as Stable\n          Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also\n          involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business . Choose Generative AI category What is it optimized for? Generative AI services Amazon Q Generating\n              code and providing\n              responses\n              to questions across business\n              data by\n              connecting to enterprise data repositories to summarize the data logically, analyze\n              trends, and engage in dialogue about the data. Amazon Q Business Amazon Q Developer Amazon Bedrock Offering\n              a choice of foundation\n              models,\n              customizing\n              them with your own data, and\n              building\n              generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Building, training, and deploying machine learning models,\n            including foundation models, at scale. Amazon SageMaker Amazon FMs Providing\n              models\n              that\n              support a variety of multi-modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering\n              services\n              that\n              maximize the price performance benefits in FM training and inference. AWS Trainium AWS Inferentia Use Now that we've covered the criteria you need to apply in choosing an AWS generative AI\n    service, you can select which services are optimized for your needs and explore how you might\n    get started using each of them. Amazon Q Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Amazon Q Business What is Amazon Q Business? Get an overview of Amazon Q Business, with explanations of what it is, how it works, and how\n              to get started using it. Explore the guide Create a sample Amazon Q Business application Learn how to create your first Amazon Q Business application in either the AWS Management Console or using\n              the command line interface (CLI). Explore the guide Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps Build private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer What is Amazon Q Developer? Get an overview of Amazon Q Developer, with explanations of what it is, how it works, and\n              how to get started using it. Explore the guide Get started with Amazon Q Developer Read this blog post to explore some key tasks that you can accomplish with\n              Amazon Q Developer. Read the blog post Working with Amazon Q Developer Use the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts,\n              videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock What is Amazon Bedrock? Learn how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock, including how to use\n              agents, security considerations, details on Amazon Bedrock software development kits (SDKs),\n              Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing\n              works. Read the FAQs Guidance for generating product descriptions with Amazon Bedrock Learn how to use Amazon Bedrock as part of a solution to automate your product review and\n              approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock Studio? Learn how you can use this web application to prototype apps that use Amazon Bedrock models\n              and features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio\n                (preview) This blog explains how you can build applications using a wide array of top-performing\n              models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock\n              Studio. Read the blog post Building an app with Amazon Bedrock Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Learn how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Learn how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and RStudio\n              on SageMaker. Explore the guide Get started with Amazon SageMaker JumpStart Explore SageMaker JumpStart solution templates that set up infrastructure for common use\n              cases, and executable example notebooks for machine learning with SageMaker. Explore the guide Amazon Titan Amazon Titan in Amazon Bedrock overview Get an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide Cost-effective document classification using the Amazon Titan Multimodal\n                Embeddings Model Learn how you can use this model to categorize and extract insights from high volumes\n              of documents of different formats. This blog explores how you can use it to help\n              determine the next set of actions to take, depending on the type of document. Read the blog post Build generative AI applications with Amazon Titan Text Premier,\n                Amazon Bedrock, and AWS CDK Explore building and deploying two sample applications powered by Amazon Titan Text\n              Premier in this blog post. Read the blog post AWS Trainium Overview of AWS Trainium Learn about AWS Trainium, the second-generation machine learning (ML) accelerator that\n              AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2\n              Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance,\n              low-cost solution for deep learning (DL) training in the cloud. Explore the guide Recommended Trainium Instances Explore how AWS Trainium instances are designed to provide high performance and cost\n              efficiency for deep learning model inference workloads. Explore the guide Scaling distributed training with AWS Trainium and Amazon EKS If you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn\n              how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by\n              AWS Trainium\u2014a purpose-built ML accelerator optimized to provide a high-performance,\n              cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post AWS Inferentia Overview of AWS Inferentia Understand\u00a0how AWS designs accelerators to deliver high performance at the lowest\n              cost for your deep learning (DL) inference applications. Explore the guide AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and\n                10x lower latency Understand what AWS Inferentia2 is optimized for and how it was designed to deliver higher\n              performance, while lowering the cost of LLMs and generative AI inference. Read the blog post Machine learning inference using AWS Inferentia Learn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and\n              optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia\n              chips, which are custom built by AWS to provide high-performance and low-cost\n              inference in the cloud. Explore the guide anchor anchor anchor anchor anchor anchor anchor anchor anchor Amazon Q Amazon Q Business Amazon Q Developer Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Amazon Titan AWS Trainium AWS Inferentia Get started with Amazon Q Review your options for getting started with Amazon Q Business and Amazon Q Developer in either the\n              AWS Management Console or the IDE. Explore the guide Work with Amazon Q Use the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API\n              Reference, to learn how you can tailor Amazon Q to your business needs. Learn how\n              Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate\n              applications and workloads on AWS. Explore the guides Learn Amazon Q Take this short, introductory AWS Skill Builder course to get a high-level overview\n              of Amazon Q (requires registration). Start the course Explore Architecture diagrams These reference architecture diagrams show examples of AWS AI and ML\n    services in use. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices\n    in choosing and using AI and ML services. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use\n    cases for AI and ML services. Explore solutions Resources Public foundation models Supported foundation models are updated on a regular basis, and currently include: Anthropic Claude Cohere Command & Embed AI21 Labs Jurassic Meta Llama Mistral AI Stable Diffusion XL Amazon Titan Use Amazon Bedrock and Amazon SageMaker to experiment with a variety of foundation models, and privately\n    customize them with your data. To explore generative AI quickly, you also have the option of\n    using PartyRock, an Amazon Bedrock Playground . PartyRock\n    is a generative AI app building playground with which you can experiment hands-on with prompt\n    engineering. Associated blog posts Build private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center Amazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience Chat about your AWS account resources with Amazon Q Developer Build enterprise-grade applications with natural language using AWS App Studio (preview) Amazon Bedrock model evaluation is now generally available Build generative AI applications with Amazon Bedrock Studio (preview) Fine-tune and deploy language models with Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/generative-ai-on-aws-how-to-choose.rss", "content": "No main content found."}, {"title": "What is Amazon Bedrock? - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "content": "What is Amazon Bedrock? PDF RSS Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from\n  leading AI companies and Amazon available for your use through a unified API. You can choose from a\n  wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also\n  offers a broad set of capabilities to build generative AI applications with security, privacy, and\n  responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top foundation models for\n  your use cases, privately customize them with your data using techniques such as fine-tuning and\n  Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise\n  systems and data sources. With Amazon Bedrock's serverless experience, you can get started quickly, privately customize\n  foundation models with your own data, and easily and securely integrate and deploy them into your\n  applications using AWS tools without having to manage any infrastructure. Topics What can I do with Amazon Bedrock? How do I get started with Amazon Bedrock? Amazon Bedrock pricing Supported AWS Regions in Amazon Bedrock Key terminology What can I do with Amazon Bedrock? You can use Amazon Bedrock to do the following: Experiment with prompts and configurations \u2013 Submit prompts and generate responses with model inference by sending prompts using different\n     configurations and foundation models to generate responses. You can use the API or the text,\n     image, and chat playgrounds in the console to experiment in a graphical interface. When you're\n     ready, set up your application to make requests to the InvokeModel APIs. Augment response generation with information from your data\n      sources \u2013 Create knowledge bases by\n     uploading data sources to be queried in order to augment a foundation model's generation of\n     responses. Create applications that reason through how to help a\n      customer \u2013 Build agents that use foundation\n     models, make API calls, and (optionally) query knowledge bases in order to reason through and\n     carry out tasks for your customers. Adapt models to specific tasks and domains with training\n      data \u2013 Customize an Amazon Bedrock foundation\n      model by providing training data for fine-tuning or continued-pretraining in order to\n     adjust a model's parameters and improve its performance on specific tasks or in certain\n     domains. Improve your FM-based application's efficiency and output \u2013 Purchase Provisioned Throughput for a foundation\n     model in order to run inference on models more efficiently and at discounted rates. Determine the best model for your use case \u2013 Evaluate outputs of different models with built-in or custom\n     prompt datasets to determine the model that is best suited for your application. Prevent inappropriate or unwanted content \u2013 Use guardrails to implement safeguards for your generative AI\n     applications. To see feature limitations by AWS Region, see Model support by AWS Region . How do I get started with Amazon Bedrock? We recommend that you start with Amazon Bedrock by  doing the following: Familiarize yourself\n     with the terms and concepts that Amazon Bedrock uses. Understand how AWS charges you for using Amazon Bedrock. Try the Getting started with Amazon Bedrock tutorials. In the tutorials, you learn how to use the playgrounds in Amazon Bedrock console . You also learn and how to use the AWS SDK to call Amazon Bedrock API operations. Read the documentation for the features that you want to include\n     in your application. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Amazon Bedrock pricing"}, {"title": "What is Amazon Bedrock Studio? - Amazon Bedrock Studio", "url": "https://docs.aws.amazon.com/bedrock/latest/studio-ug/what-is-bedrock-studio.html", "content": "Amazon Bedrock Studio is in preview release and is subject to change. Amazon Bedrock Studio is in preview release and is subject to change. What is Amazon Bedrock Studio? PDF RSS Amazon Bedrock Studio is a web app that lets you easily protoype apps that use Amazon Bedrock models and\n  features, without having to set up and use a developer environment. For example, you can use\n  Amazon Bedrock to try a prompt with an Anthropic Claude model without having to write any code.\n  Later, you can use Bedrock Studio to create a prototype app that uses an Amazon Bedrock model and features, such as a\n  Knowledge Base or a Guardrail, again without having to write any code. To use Amazon Bedrock Studio, you must be a member of a workspace. Your organization will provide you with login details.\n  If you don't have login details, contact your administrator. Note If you are administrator and need information about managing an Amazon Bedrock Studio workspace, see Manage Amazon Bedrock Studio in the Amazon Bedrock user\n  guide. Workspaces A workspace in Amazon Bedrock Studio is where you experiment with Amazon Bedrock\n   models and where you can build Amazon Bedrock enabled apps. As an Amazon Bedrock Studio user, you can use a\n   workspace in two types of user mode, Explore or Build . Explore mode Explore mode provides a playground that\n   lets you easily try a model by sending prompts to the model and viewing the responses.\n  For more information, see Explore Amazon Bedrock Studio . Build mode Build mode is where you can create apps that use Amazon Bedrock models. You can create two types\n    of apps, a chat app and a Prompt Flows app . A chat app allows users to communicate with an Amazon Bedrock model\n    through a conversational interface, typically by sending text messages and receiving responses.\n    A Prompt Flows app allows you to link prompts, supported\n    foundational models (FMS), and other units of work, such as a Knowledge Base, together and\n    create generative AI workflows for end-to-end solutions. Apps that you create with Bedrock Studio can integrate the following Amazon Bedrock features. Data sources \u2014 Enrich apps by including context that is received from querying a Knowledge Base or a\n      document. Guardrails \u2014\n      Lets you implement safeguards for your Bedrock Studio app based on your use cases and responsible\n      AI policies. Functions \u2014 Lets\n      a model call a function to access a specific capability when handling a prompt. Prompt Management \u2014 Reusable prompts that you can use in a Prompt Flows app. Within Build mode, you use a project to organize the apps, prompts,\n    and components that you use for a solution. A component is an Amazon Bedrock Knowledge Base,\n    Guardrail, or Function. When you open a project, you can access the App\n     builder and Project details by opening the side navigation menu. The App builder is one way you can access the chat app or Prompt Flows app that you are\n    currently working on. Project details is where you create and manage the apps, components, and\n    prompts that the apps in your project use. For more information, see Organize your work with projects in Amazon Bedrock Studio . If you work on a team, you can collaborate by sharing a project with other team members.\n    For more information, see Share an Amazon Bedrock Studio project . Are you a first-time Amazon Bedrock Studio user? If you're a first-time user of Bedrock Studio, we recommend that you read the following\n   sections in order: Explore Amazon Bedrock Studio \u2013 In this section, you access your\n     Amazon Bedrock Studio workspace for the first time and use Explore mode to experiment with an Amazon Bedrock model. Build a chat app with Amazon Bedrock Studio \u2013 In this section,\n     you use Build mode to create a project in your workspace and create a simple Amazon Bedrock Studio app. Build a Prompt Flows app with Amazon Bedrock Studio \u2013 In this section,\n     you learn how to create a Prompt Flows app. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Explore Amazon Bedrock Studio"}, {"title": "What is Amazon Q Business? - Amazon Q Business", "url": "https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/what-is.html", "content": "What is Amazon Q Business? PDF RSS Powered by Amazon Bedrock: AWS implements automated\n              abuse detection . Because Amazon Q is built on Amazon Bedrock, users can take full\n            advantage of the controls implemented in Amazon Bedrock to enforce safety, security, and the\n            responsible use of artificial intelligence (AI). Amazon Q Business is a fully managed, generative-AI powered assistant that you can\n    configure to answer questions, provide summaries, generate content, and complete tasks based on\n    your enterprise data. It allows end users to receive immediate, permissions-aware responses from\n    enterprise data sources with citations, for use cases such as IT, HR, and benefits help\n    desks. Amazon Q Business also helps streamline tasks and accelerate problem solving. You can\n    use Amazon Q Business to create and share task automation applications, or perform routine\n    actions like submitting time-off requests and sending meeting invites. Amazon Q Business integrates with services like Amazon Kendra and other\n      supported data sources such as Amazon S3 , Microsoft SharePoint , and Salesforce . Topics Benefits of Amazon Q Business Pricing and availability Accessing Amazon Q Business Related services Are you a first-time Amazon Q Business user? Benefits of Amazon Q Business Some of the benefits of Amazon Q Business include: Accurate and comprehensive answers Amazon Q Business generates comprehensive responses to natural language\n            queries from users by analyzing information across all enterprise content that it has\n            access to. It can avoid incorrect statements by confining its generated responses to\n            existing enterprise data, and provides citations to the sources that it used to generate\n            its response. Simple to deploy and manage Amazon Q Business takes care of the complex task of developing and managing\n            machine learning infrastructure and models so that you can build your chat solution\n            quickly. Amazon Q Business connects to your data and ingests it for processing\n            using its pre-built connectors, document retrievers, document upload\n            capabilities. Configurable and customizable Amazon Q Business provides you with the flexibility of choosing what sources\n            should be used to respond to user queries. You can control whether the responses should\n            only use your enterprise data, or use both enterprise data and model knowledge. Data and application security Amazon Q Business supports access control for your data so that the right\n            users can access the right content. Its responses to questions are based on the content\n            that your end user has permissions to access. You can use AWS IAM Identity Center or AWS Identity and Access Management to\n            manage end user access for Amazon Q Business. Broad connectivity Amazon Q Business offers out-of-the-box connections to multiple supported data sources. Additionally, you can connect Amazon Q to any\n            third-party application using plugins to perform actions and query application\n            data. Pricing and availability Amazon Q Business charges you both for user subscriptions to applications, and for\n      index capacity. For information about what's included in the tiers of user subscriptions and\n      index capacity, see Subscription and index pricing . For pricing information, including examples of charges for index capacity, subscribing and\n      unsubscribing users to Amazon Q Business tiers, upgrading and downgrading Amazon Q Business tiers, and more, see Amazon Q Business Pricing . For a list of regions where Amazon Q Business is currently available, see Supported\n        regions . Accessing Amazon Q Business You can access Amazon Q Business in the following ways in the AWS Regions that\n      it's available in: AWS Management Console You can use the AWS Management Console\u2014a browser-based interface to interact with\n            AWS services\u2014to access the Amazon Q Business console and resources. You\n            can perform most Amazon Q Business tasks using the Amazon Q Business\n            console. Amazon Q Business API To access Amazon Q Business programmatically, you can use the Amazon Q API. For more information, see the Amazon Q Business API\n              Reference . AWS Command Line Interface The AWS Command Line Interface (AWS CLI) is an open source tool. You can use the AWS CLI to interact with\n            AWS services using commands in your command line shell. If you want to build\n            task-based scripts, using the command line can be faster and more convenient than using\n            the console. SDKs AWS SDKs provide language APIs for AWS services to use programmatically. Related services The following are some of the other AWS services that Amazon Q Business integrates\n      with: Amazon Kendra Amazon Kendra is an intelligent search service that uses natural language\n            processing and machine learning algorithms to return specific answers from your data for\n            end user queries. If you're already an Amazon Kendra user, you can use Amazon Kendra as a data retriever for your Amazon Q Business web application. Amazon S3 Amazon S3 is an object storage service. If you're an Amazon S3 user,\n            you can use Amazon S3 as a data source for your Amazon Q Business\n            application. Are you a first-time Amazon Q Business user? If you're a first-time user of Amazon Q Business, we recommend that you read the\n      following sections in order: How it\n            works Introduces Amazon Q Business components and describes how they work to create\n            your Retrieval Augmented Generation (RAG) solution. Key\n            concepts Explains key concepts and important Amazon Q Business terminology. Setting\n            up Explains key concepts and important Amazon Q Business terminology and outlines\n            how to set up Amazon Q Business so that you can begin creating your Amazon Q Business application and web experience. Creating a\n            sample application Explains how to create the Amazon Q Business application integrated with\n            IAM Identity Center. Connecting Amazon Q Business data source\n                connectors Configuration information for specific connectors to use with your Amazon Q Business web experience. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Getting started"}, {"title": "What is Amazon Q Developer? - Amazon Q Developer", "url": "https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/what-is.html", "content": "What is Amazon Q Developer? PDF RSS Note Powered by Amazon Bedrock: AWS implements automated\n            abuse detection . Because Amazon Q Developer is built on\n         Amazon Bedrock, users can take full advantage of the controls implemented in Amazon Bedrock to\n         enforce safety, security, and the responsible use of artificial intelligence (AI). Amazon Q Developer is a generative artificial intelligence (AI) powered conversational assistant\n      that can help you understand, build, extend, and operate AWS applications. You can ask\n      questions about AWS architecture, your AWS resources, best practices, documentation,\n      support, and more. Amazon Q is constantly updating its capabilities so your questions get the\n      most contextually relevant and actionable answers. When used in an integrated development environment (IDE), Amazon Q provides software\n      development assistance. Amazon Q can chat about code, provide inline code completions,\n      generate net new code, scan your code for security vulnerabilities, and make code upgrades and\n      improvements, such as language updates, debugging, and optimizations. Amazon Q is powered by Amazon Bedrock , a fully managed service that makes foundation models (FMs) available through\n      an API. The model that powers Amazon Q has been augmented with high quality AWS content to get\n      you more complete, actionable, and referenced answers to accelerate your building on\n      AWS. Note This is the documentation for Amazon Q Developer. If you are looking for\n         documentation for Amazon Q Business, see the Amazon Q Business User\n            Guide . Get started with Amazon Q Developer To quickly get started using Amazon Q, you can access it in the following ways: AWS apps and websites Add the necessary permissions to your\n                  IAM identity, and then choose the Amazon Q icon to start chatting in the AWS Management Console,\n                  AWS Documentation website, AWS website, or AWS Console Mobile Application. For more information, see Using Amazon Q Developer on AWS apps and websites . IDEs Download the Amazon Q extension and use your AWS Builder ID (no AWS account\n                  required) to sign in for free. Download Amazon Q in Visual Studio Code Download Amazon Q in JetBrains IDEs Download Amazon Q in the AWS Toolkit for Visual Studio From the Amazon Q extension, choose Open Chat Panel to start\n                  chatting or initiate a development workflow. For more information, see Installing the Amazon Q Developer extension or plugin in your IDE . Command line Download Amazon Q for the macOS command line . For more information, see Using Amazon Q Developer on the command line . AWS Chatbot for Microsoft Teams and Slack Add the AmazonQFullAccess managed policy to your IAM identity and channel\n                  guardrails for Microsoft Teams or Slack channels configured with\n                  AWS Chatbot. For more information, see Chatting with Amazon Q Developer in AWS Chatbot . Amazon Q Developer pricing Amazon Q Developer is available through a Free Tier and the Amazon Q Developer Pro subscription. For\n         more information, see Amazon Q Developer pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Features"}, {"title": "What is Amazon SageMaker? - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html.html", "content": "What is Amazon SageMaker? PDF RSS Amazon SageMaker is a fully managed machine learning (ML) service. With SageMaker, data scientists and\n    developers can quickly and confidently build, train, and deploy ML models into a\n    production-ready hosted environment. It provides a UI experience for running ML workflows that\n    makes SageMaker ML tools available across multiple integrated development environments (IDEs). With SageMaker, you can store and share your data without having to build and manage your own\n    servers. This gives you or your organizations more time to collaboratively build and develop\n    your ML workflow, and do it sooner. SageMaker provides managed ML algorithms to run efficiently\n    against extremely large data in a distributed environment. With built-in support for\n    bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that\n    adjust to your specific workflows. Within a few steps, you can deploy a model into a secure and\n    scalable environment from the SageMaker console. Topics Pricing for Amazon SageMaker Recommendations for a first-time user of Amazon SageMaker Overview of machine learning with Amazon SageMaker Amazon SageMaker Features Pricing for Amazon SageMaker For information about AWS Free Tier limits\n      and the cost of using SageMaker, see Amazon SageMaker\n        Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Recommendations for a first-time user of Amazon SageMaker"}, {"title": "Overview of Amazon Titan models - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/titan-models.html", "content": "Overview of Amazon Titan models PDF RSS Amazon Titan foundation models (FMs) are a family of FMs pretrained by AWS on large datasets, making them powerful, \n  general-purpose models built to support a variety of use cases. Use them as-is or privately customize them \n  with your own data. Amazon Titan supports the following models for Amazon Bedrock. Amazon Titan Text Amazon Titan Text Embeddings V2 Amazon Titan Multimodal Embeddings G1 Amazon Titan Image Generator G1 V1 Topics Amazon Titan Text models Amazon Titan Text Embeddings models Amazon Titan Multimodal Embeddings G1 model Amazon Titan Image Generator G1 models Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Tagging Amazon Bedrock resources Amazon Titan Text"}, {"title": "Creating purpose-built Amazon Q Apps - Amazon Q Business", "url": "https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/purpose-built-qapps.html", "content": "Creating purpose-built Amazon Q\n                Apps PDF RSS Important As of July 1, 2024, Amazon Q Apps are available only to Amazon Q Business Pro users . Amazon Q Business Lite users will no longer be able to\n            create, run, or view Q Apps. To access, Q Apps, Lite users must upgrade to Amazon Q Business Pro. As of August 30, 2024, all  Amazon Q Apps created by Lite\n            users who did not upgrade their account to Amazon Q Business Pro have been deleted. You and your web experience users can create lightweight, purpose-built Amazon Q\n            Apps within your broader Amazon Q Business application environment. Using your\n        enterprise data, users can create a generative AI-powered app that streamlines their tasks.\n        These Q Apps can be easily created by anyone at the click of a button, transforming their\n        conversations with an Amazon Q Business assistant into reusable and shareable\n        Amazon Q Apps. For example, if your Amazon Q Business assistant generates useful content for all\n        company employees, your marketing team could then create their own Amazon Q App for task\n        automation. Let\u2019s say a marketing team member finds a useful response to their question. The\n        marketing team member can use that response or conversation and further build onto it to\n        generate marketing content that adheres to the company's branding guidelines already known\n        to Amazon Q Business. Amazon Q Apps is enabled by default when you create a new Amazon Q Business\n        application environment using IAM Identity Center or IAM\n            Federation in the Amazon Q Business console. Amazon Q Apps can be accessed\n        through the web experience. You can also create and manage Q Apps programmatically. For an more information, see Amazon Q Apps in the Amazon Q Business. API\n        Reference . Topics Prerequisites for Amazon Q\n                Apps Managing Amazon Q Apps Using the web experience to create\n                and run Amazon Q Apps Understanding and managing Verified\n                Amazon Q Apps Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Managing admin controls and guardrails Prerequisites for Amazon Q\n                Apps"}, {"title": "What is Amazon Q Developer? - Amazon Q Developer", "url": "https://docs.aws.amazon.com/en_us/amazonq/latest/qdeveloper-ug/q-and-aws-services.html", "content": "What is Amazon Q Developer? PDF RSS Note Powered by Amazon Bedrock: AWS implements automated\n            abuse detection . Because Amazon Q Developer is built on\n         Amazon Bedrock, users can take full advantage of the controls implemented in Amazon Bedrock to\n         enforce safety, security, and the responsible use of artificial intelligence (AI). Amazon Q Developer is a generative artificial intelligence (AI) powered conversational assistant\n      that can help you understand, build, extend, and operate AWS applications. You can ask\n      questions about AWS architecture, your AWS resources, best practices, documentation,\n      support, and more. Amazon Q is constantly updating its capabilities so your questions get the\n      most contextually relevant and actionable answers. When used in an integrated development environment (IDE), Amazon Q provides software\n      development assistance. Amazon Q can chat about code, provide inline code completions,\n      generate net new code, scan your code for security vulnerabilities, and make code upgrades and\n      improvements, such as language updates, debugging, and optimizations. Amazon Q is powered by Amazon Bedrock , a fully managed service that makes foundation models (FMs) available through\n      an API. The model that powers Amazon Q has been augmented with high quality AWS content to get\n      you more complete, actionable, and referenced answers to accelerate your building on\n      AWS. Note This is the documentation for Amazon Q Developer. If you are looking for\n         documentation for Amazon Q Business, see the Amazon Q Business User\n            Guide . Get started with Amazon Q Developer To quickly get started using Amazon Q, you can access it in the following ways: AWS apps and websites Add the necessary permissions to your\n                  IAM identity, and then choose the Amazon Q icon to start chatting in the AWS Management Console,\n                  AWS Documentation website, AWS website, or AWS Console Mobile Application. For more information, see Using Amazon Q Developer on AWS apps and websites . IDEs Download the Amazon Q extension and use your AWS Builder ID (no AWS account\n                  required) to sign in for free. Download Amazon Q in Visual Studio Code Download Amazon Q in JetBrains IDEs Download Amazon Q in the AWS Toolkit for Visual Studio From the Amazon Q extension, choose Open Chat Panel to start\n                  chatting or initiate a development workflow. For more information, see Installing the Amazon Q Developer extension or plugin in your IDE . Command line Download Amazon Q for the macOS command line . For more information, see Using Amazon Q Developer on the command line . AWS Chatbot for Microsoft Teams and Slack Add the AmazonQFullAccess managed policy to your IAM identity and channel\n                  guardrails for Microsoft Teams or Slack channels configured with\n                  AWS Chatbot. For more information, see Chatting with Amazon Q Developer in AWS Chatbot . Amazon Q Developer pricing Amazon Q Developer is available through a Free Tier and the Amazon Q Developer Pro subscription. For\n         more information, see Amazon Q Developer pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Features"}, {"title": "Using Generative BI with Amazon Q in QuickSight - Amazon QuickSight", "url": "https://docs.aws.amazon.com/quicksight/latest/user/quicksight-gen-bi.html", "content": "Using Generative BI with Amazon Q in QuickSight PDF RSS Note Powered by Amazon Bedrock: AWS implements automated abuse\n            detection . Because  is built on Amazon Bedrock, users can take full advantage of the controls implemented in Amazon Bedrock to enforce safety,\n            security, and the responsible use of artificial intelligence (AI). Amazon Q integrates with Amazon QuickSight to give QuickSight users access to a suite of new Generative BI capabilities. With Amazon Q in QuickSight, you can utilize the Generative BI authoring experience, create executive summaries of your data, ask and answer questions of data, and generate data stories. To access all QuickSight Generative BI features that are relevant to your task, choose the Q icon at the top right of any QuickSight page. In the Q pane that opens, Amazon Q displays all content that is available based on the context of the task that you are performing. For example, if you're working in an Analysis, you can build a calculation, edit visuals, set up Q&A, or ask questions about your data. If you're working in a Dashboard, you can build a data story, generate an executive summary, or ask questions about the dashboard. The image below shows the Q icon that opens the Q pane. Note Amazon Q in QuickSight Generative BI features are not available in all AWS regions. To see a list of regions that Generative BI features are available in, see Supported AWS Regions for Amazon Q in QuickSight Use the following topics to learn more about Generative BI with Amazon Q in QuickSight. Topics Get started with Generative BI The Generative BI authoring experience Creating executive summaries with Amazon Q in QuickSight Authoring\n                Q&A Asking and answering questions of data with Amazon Q in QuickSight Opting out of Amazon Q in QuickSight Working with data stories in Amazon QuickSight Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Unsubscribe from Q Get started"}, {"title": "Use Amazon Q in Connect for generative AI\u2013powered agent assistance in real-time - Amazon Connect", "url": "https://docs.aws.amazon.com/connect/latest/adminguide/amazon-q-connect.html", "content": "Use Amazon Q in Connect for generative AI\u2013powered agent\n            assistance in real-time PDF RSS Note Powered by Amazon Bedrock : AWS implements automated abuse\n                detection . Because Amazon Q in Connect is built on Amazon Bedrock, users can take full\n                advantage of the controls implemented in Amazon Bedrock to enforce safety, security, and the\n                responsible use of artificial intelligence (AI). Amazon Q in Connect is a generative AI customer service assistant. It is an LLM-enhanced evolution of Amazon Connect Wisdom that delivers real-time\n            recommendations to help contact center agents resolve customer issues quickly and\n            accurately. Amazon Q in Connect automatically detects customer intent during calls and chats\n            using conversational analytics and natural language understanding (NLU). It then provides\n            agents with immediate, real-time generative responses and suggested actions. It also provides \n            links to relevant documents and articles. In addition to receiving automatic\n            recommendations, agents can also query Amazon Q in Connect directly using natural language or\n            keywords to answer customer requests. Amazon Q in Connect works right within the Amazon Connect agent workspace. Beyond its core functionality, Amazon Q in Connect supports: Integration with step-by-step guides to\n                    help agents arrive at solutions faster . Configuration of the Gen AI system that powers Amazon Q in Connect so that its behavior\n                can be customized to fit the unique needs of your contact center. Amazon Q in Connect is available by API to be used in an existing agent workspace. For more information, see the Amazon Q in Connect API Reference Guide . Note To use Amazon Q in Connect with calls, you must enable Amazon Connect Contact Lens. Contact Lens is not required to use\n            Amazon Q in Connect with chats. Amazon Q in Connect can be used in compliance with GDPR and is HIPAA eligible. The following image shows how an Amazon Q in Connect article may appear in the agent application\n        when the agent is on a call. The Amazon Q in Connect toggle button on the top-right can be used to toggle the expansion of the Amazon Q in Connect widget. Amazon Q in Connect proactively generates clickable intents to assist the customer service agent handling the contact. Upon click an intent, Amazon Q in Connect generates a solution using an appropriate source from the Knowledge Base configured for it. Amazon Q in Connect provides a solution with citations. Sources associated with the citations and other related articles are also displayed and can be clicked to dive into the Knowledge Base material. The agent can ask natural-language questions to receive responses on demand for Amazon Q in Connect using the input at the bottom of the widget. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Third-party application SSO Federation\n                    setup Enable Amazon Q in Connect"}, {"title": "What is Amazon Connect? - Amazon Connect", "url": "https://docs.aws.amazon.com/connect/latest/adminguide/what-is-amazon-connect.html", "content": "What is Amazon Connect? PDF RSS Amazon Connect is an AI-powered application that provides one seamless experience for your contact\n        center customers and users. It's comprised of a full suite of features across communication\n        channels. Using an intuitive web application\u2014the Amazon Connect admin website\u2014you can set up a contact center in a few steps, add\n        agents who are located anywhere, and start engaging with your customers. You can innovate\n        and make changes in minutes, not months. No coding required. If you're using Amazon Connect, you're likely one of these users: Customers reach out to your contact center because they are having trouble with\n                some issue they can't resolve for themselves, or resolve easily. They want the\n                ability to contact your contact center using any method they choose. Agents are responsible for helping customers solve general problems, and come to a\n                quick resolution whenever possible. They spend most of their time interacting with\n                customers, whether through voice, chat, SMS, or other channels, and then documenting\n                the interaction. Contact center managers and supervisors spend most of their day monitoring their\n                team's metrics and readjusting their configuration to be optimal for their business.\n                They onboard most new agents, and provide coaching to help their team members grow. Administrators handle the entire Amazon Connect configuration. They provision phone\n                numbers and integrate Amazon Connect with other products. Along with contact center managers,\n                they define queues and routing profiles, implement flows, and create rules to set up\n                alerts and notifications. Get more information in the Amazon Connect feature overview . How to get started If you are a first-time user of Amazon Connect, we recommend that you do the following: Check out the Introduction to Amazon Connect workshop. Explore Amazon Connect with our tutorials . Read the architectural\n                    guidance . Set up your contact center in Amazon Connect . Pricing With Amazon Connect, you pay only for what you use. For more information, see Amazon Connect pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Feature overview"}, {"title": "Customize your model to improve its performance for your use case - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html", "content": "Customize your model to improve its performance for your use case PDF RSS Model customization is the process of providing training data to a model in order to improve its performance for specific use-cases. You can customize Amazon Bedrock foundation models in order to improve their performance and create a better customer experience. Amazon Bedrock currently provides the following customization methods. Continued Pre-training Provide unlabeled data to pre-train a foundation model by familiarizing it with certain types of inputs. You can provide data from specific topics in order to expose a model to those areas. The Continued Pre-training process will tweak the model parameters to accommodate the input data and improve its domain knowledge. For example, you can train a model with private data, such as business documents, that are not publicly available for training large language models. Additionally, you can continue to improve the model by retraining the model with more unlabeled data as it becomes available. Fine-tuning Provide labeled data in order to train a model to improve performance on specific tasks. By providing a training dataset of labeled examples, the model learns to associate what types of outputs should be generated for certain types of inputs. The model parameters are adjusted in the process and the model's performance is improved for the tasks represented by the training dataset. For information about model customization quotas, see Amazon Bedrock endpoints and quotas in the AWS General Reference. Note You are charged for model training based on the number of\n            tokens processed by the model (number of tokens in training data corpus \u00d7 number of\n            epochs) and model storage charged per month per model. For more information, see Amazon Bedrock pricing . You carry out the following steps in model customization. Create a training and, if applicable, a validation dataset for your customization task. If you plan to use a new custom IAM role, set up IAM permissions to access the S3 buckets for your data. You can also use an existing role or let the console automatically create a role with the proper permissions. (Optional) Configure KMS keys and/or VPC for extra security. Create a Fine-tuning or Continued Pre-training job , controlling the training process by adjusting the hyperparameter values. Analyze the results by looking at the training or validation metrics or by using model evaluation. Purchase Provisioned Throughput for your newly created custom model. Use your custom model as you would a base model in Amazon Bedrock tasks, such as model inference. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Run code samples Supported regions and models"}, {"title": "Import a customized model to Amazon Bedrock - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html", "content": "Import a customized model to Amazon Bedrock PDF RSS Custom Model Import is in preview\n                        release for Amazon Bedrock and is subject to change. You can create a custom model in Amazon Bedrock by using the Custom Model Import feature to\n        import Foundation Models that you have customized in other environments, such as Amazon SageMaker.\n        For example, you might have a model that you have created in Amazon SageMaker that has proprietary\n        model weights. You can now import that model into Amazon Bedrock and then leverage Amazon Bedrock features to\n        make inference calls to the model. You can use a model that you import with on demand throughput. Use the InvokeModel or InvokeModelWithResponseStream operations to make inference calls to the model. For more information,\n        see Submit a single prompt with the InvokeModel API operations . Note For the preview release, Custom Model Import is available in the US East (N. Virginia) and US West (Oregon)\n            AWS Regions only. You can't use Custom Model Import with the following Amazon Bedrock\n            features. Amazon Bedrock Agents Amazon Bedrock Knowledge Bases Amazon Bedrock Guardrails Batch inference AWS CloudFormation Before you can use Custom Model Import, you must first request a quota increase for\n            the Imported models per account quota. For more information, see Requesting a quota\n                increase . With Custom Model Import you can create a custom model that supports the following\n        patterns. Fine-tuned or Continued Pre-training model \u2014 You can customize the model weights using proprietary data, but retain the\n                configuration of the base model. Adaptation You can customize the model\n                to your domain for use cases where the model doesn't generalize well. Domain\n                adaptation modifies a model to generalize for a target domain and\n                deal with discrepancies across domains, such as a financial\n                industry wanting to create a model which generalizes well on pricing. Another example is\n                language adaptation. For example you could customize a model to generate responses\n                in Portuguese or Tamil. Most often, this involves changes to the\n                vocabulary of the model that you are using. Pretrained from scratch \u2014 In addition to\n                customizing the weights and vocabulary of the model, you can also change model\n                configuration parameters such as the number of attention heads, hidden layers, or\n                context length. Topics Supported architectures Import source Importing a model Supported architectures The model you import must be in one of the following architectures. Mistral \u2014 A decoder-only Transformer\n                    based architecture with Sliding Window Attention (SWA) and options for Grouped\n                    Query Attention (GQA). For more information, see Mistral in the Hugging Face documentation. Flan \u2014 An\n                enhanced version of the T5 architecture, an encoder-decoder based transformer model.\n                For more information, see Flan T5 in the Hugging Face documentation. Llama 2 and Llama3 \u2014 An improved version of Llama\n                with Grouped Query Attention (GQA). For more information, see Llama 2 and Llama 3 in the\n                Hugging Face documentation. Import source You import a model into Amazon Bedrock by creating a model import job in the Amazon Bedrock console.\n        In the job you specify the Amazon S3 URI for the source of the model files. Alternatively, if you\n        created the model in Amazon SageMaker, you can specify the SageMaker model. During model training, the\n        import job automatically detects your model's architecture. If you import from an Amazon S3 bucket, you need to supply the model files in the\n                Hugging Face weights format. You can create the files by using the\n            Hugging Face transformer library. To create model files for a Llama\n            model, see convert_llama_weights_to_hf.py . To create the files for a Mistral AI model,\n            see convert_mistral_weights_to_hf.py . To import the model from Amazon S3, you minimally need the following files that the Hugging Face transformer library creates. .safetensor \u2014 the model weights in Safetensor format. Safetensors is a format created by\n                Hugging Face that stores a model weights as tensors. You must store the tensors\n                for your model in a file with the extension .safetensors . For more\n                information, see Safetensors . For information about converting model weights to\n                Safetensor format, see Convert\n                    weights to safetensors . Note Currently, Amazon Bedrock only supports model weights with FP32, FP16, and\n                            BF16 precision. Amazon Bedrock will reject model weights if you supply them\n                            with any other precision.  Internally Amazon Bedrock will convert\n                            FP32 models to BF16 precision. Amazon Bedrock doesn't support the import of quantized models. config.json \u2014 For examples, \n            see LlamaConfig and MistralConfig . tokenizer_config.json \u2014 For an example,\n                see LlamaTokenizer . tokenizer.json tokenizer.model Importing a model The following procedure shows you how to create a custom model by importing a model that you \n            have already customized.\n            The model import job can take several minutes. During the  import job, Amazon Bedrock validates that the model uses a compatible model architecture. To submit a model import job, carry out the following steps. Request a quota increase for the Imported models per account quota. For more\n                information, see Requesting\n                        a quota increase . If you are importing your model files from Amazon S3, convert the model to\n                    the Hugging Face format. If your model is a Mistral AI model, use convert_mistral_weights_to_hf.py . If your model is a Llama model, see convert_llama_weights_to_hf.py . Upload the model files to an Amazon S3 bucket in your\n                        AWS account. For more information, see Upload an\n                            object to your bucket . Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions , and open the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/ . Choose Imported models under Foundation models from the left navigation pane. Choose the Models tab. Choose Import model . In the Imported tab, choose Import\n                        model to open the Import model page. In the Model details section, do the following: In Model name enter a name for the model. (Optional) To associate tags with the\n                            model, expand the Tags section and select Add new tag . In the Import job name section, do the following: In Job name enter a name for the model import\n                            job. (Optional) To associate tags with the\n                            custom model, expand the Tags section and\n                            select Add new tag . In Model import settings , select the import options you want to use. If you are importing your model files from an Amazon S3 bucket, choose Amazon S3 bucket and enter the Amazon S3 location in S3 location . Optionally, you can choose Browse\n                                    S3 to choose the file location. If you are importing your model from Amazon SageMaker, choose Amazon SageMaker model and then choose the SageMaker model\n                            that you want to import in SageMaker models . In the Service access section, select one of the\n                    following: Create and use a new service role \u2013\n                            Enter a name for the service role. Use an existing service role \u2013 Select a\n                            service role from the drop-down list. To see the permissions that your\n                            existing service role needs, choose View permission\n                                details . For more information on setting up a service role with the appropriate\n                            permissions, see Create a service role for model import . Choose Import . On the Custom models page, choose Imported . In the Jobs section, check the status of the import job.\n                    The model name you chose identifies the model import job. The job is complete if\n                    the value of Status for the model is Complete . Get the model ID for your model by doing the following. On the Imported models page, choose the Models tab. Copy the ARN for the model that you want to use from the ARN column. Use your model for inference calls. For more information, see Submit a single prompt with the InvokeModel API operations . You can use the model\n                    with on demand throughput. You can also use your model in the Amazon Bedrock text playground . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Delete a custom model Share a model for another account to use"}, {"title": "Automate tasks in your application using conversational agents - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html", "content": "Automate tasks in your application using conversational agents PDF RSS Amazon Bedrock Agents offers you the ability to build and configure autonomous agents in your\n        application. An agent helps your end-users complete actions based on organization data and\n        user input. Agents orchestrate interactions between foundation models (FMs), data sources,\n        software applications, and user conversations. In addition, agents automatically call APIs\n        to take actions and invoke knowledge bases to supplement information for these actions.\n        Developers can save weeks of development effort by integrating agents to accelerate the\n        delivery of generative artificial intelligence (generative AI) applications . With agents, you can automate tasks for your customers and answer questions for them. For\n        example, you can create an agent that helps customers process insurance claims or an agent\n        that helps customers make travel reservations. You don't have to provision capacity, manage\n        infrastructure, or write custom code. Amazon Bedrock manages prompt engineering, memory, monitoring,\n        encryption, user permissions, and API invocation. Agents perform the following tasks: Extend foundation models to understand user requests and break down the tasks that\n                the agent must perform into smaller steps. Collect additional information from a user through natural conversation. Take actions to fulfill a customer's request by making API calls to your company\n                systems. Augment performance and accuracy by querying data sources. To use an agent, you perform the following steps: (Optional) Create a knowledge base to store your private data in that database. For more information, see Retrieve data and generate AI responses with knowledge bases . Configure an agent for your use case and add at least one of the following components: At least one action group that the agent can perform. To\n                        learn how to define the action group and how it's handled by the agent, see Use action groups to define actions for your agent to perform . Associate a knowledge base with the agent to augment the agent's performance. For\n                        more information, see Augment response generation for your agent with knowledge base . (Optional) To customize the agent's behavior to your specific use-case, modify\n                prompt templates for the pre-processing, orchestration, knowledge base response\n                generation, and post-processing steps that the agent performs. For more information,\n                see Enhance agent's accuracy using advanced prompt templates in Amazon Bedrock . Test your agent in the Amazon Bedrock console or through API calls to the TSTALIASID . Modify the configurations as necessary. Use traces to\n                examine your agent's reasoning process at each step of its orchestration. For more\n                information, see Test and troubleshoot agent behavior and Track agent's step-by-step reasoning process using trace . When you have sufficiently modified your agent and it's ready to be deployed to\n                your application, create an alias to point to a version of your agent. For more\n                information, see Deploy and integrate an Amazon Bedrock agent into your application . Set up your application to make API calls to your agent alias. Iterate on your agent and create more versions and aliases as necessary. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Deploy your knowledge base for your application How Amazon Bedrock Agents work"}, {"title": "Stop harmful content in models using Amazon Bedrock Guardrails - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html", "content": "Stop harmful content in models using Amazon Bedrock Guardrails PDF RSS Amazon Bedrock Guardrails enables you to implement safeguards for your generative AI applications based on your\n        use cases and responsible AI policies. You can create multiple guardrails tailored to\n        different use cases and apply them across multiple foundation models (FM), providing a consistent\n        user experience and standardizing safety and privacy controls across generative AI\n        applications. You can use guardrails with text-based user inputs and model\n        responses. Guardrails can be used in multiple ways to safeguard generative AI applications. For\n        example: A chatbot application can use guardrails to filter harmful user inputs and \n            toxic model responses. A banking application can use guardrails to block user queries or model \n            responses associated with seeking or providing investment advice. A call center application to summarize conversation transcripts between \n            users and agents can use guardrails to redact users\u2019 personally identifiable information \n            (PII) to protect user privacy. You can configure the following policies in a guardrail to avoid undesirable and harmful content \n        and remove sensitive information for privacy protection. Content filters \u2013 Adjust filter strengths\n                to block input prompts or model responses containing harmful content. Denied topics \u2013 Define a set of topics that\n                are undesirable in the context of your application. These topics will be blocked if\n                detected in user queries or model responses. Word filters \u2013 Configure filters to block\n                undesirable words, phrases, and profanity. Such words can include offensive terms, \n                competitor names etc. Sensitive information filters \u2013 Block or\n                mask sensitive information such as personally identifiable information (PII) or custom \n                regex in user inputs and model responses. Contextual grounding check \u2013 Detect and filter \n                hallucinations in model responses based on grounding in a source and \n                relevance to the user query. In addition to the above policies, you can also configure the messages to be returned to \n        the user if a user input or model response is in violation of the policies defined in the guardrail. You can create multiple guardrail versions for your guardrail. When you create a\n        guardrail, a working draft is automatically available for you to iteratively modify.\n        Experiment with different configurations and use the built-in test window to see whether\n        they are appropriate for your use-case. If you are satisfied with a set of configurations,\n        you can create a version of the guardrail and use it with supported foundation models. Guardrails can be used directly with FMs during the inference API invocation by specifying\n        the guardrail ID and the version. If a guardrail is used, it will evaluate the input prompts\n        and the FM completions against the defined policies. For retrieval augmented generation (RAG) or conversational applications, you may need to\n        evaluate only the user input in the input prompt while discarding system instructions,\n        search results, conversation history, or few short examples. To selectively evaluate a\n        section of the input prompt, see Apply tags to user input to filter content . Important Amazon Bedrock Guardrails supports English-only. Evaluating text content in other \n        languages can result in unreliable results. Topics How Amazon Bedrock Guardrails works Supported regions and models for Amazon Bedrock Guardrails Components of a guardrail Prerequisites for using guardrails with your AWS account Create a guardrail Set up permissions to use guardrails for content filtering Test a guardrail View information about your guardrails Modify a guardrail Delete a guardrail Deploy your guardrail Use guardrails for your use case Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Run code samples Next"}, {"title": "Retrieve data and generate AI responses with knowledge bases - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html", "content": "Retrieve data and generate AI responses with knowledge bases PDF RSS You can integrate proprietary information into your generative-AI applications with Amazon Bedrock knowledge bases. \n        Using the Retrieval Augment Generation (RAG) technique , a knowledge base searches your data to \n        find the most useful information and then uses it to answer natural language questions. After you set up a knowledge base, you can then take advantage of the knowledge base in \n        the following ways: Configure your RAG application to use the RetrieveAndGenerate API to query your knowledge base and generate \n                responses from the information it retrieves. You can also call the Retrieve API to query your knowledge base with information retrieved \n                directly from the knowledge base. Associate your knowledge base with an agent (for more information, see Automate tasks in your application using conversational agents ) to add RAG capability to the agent by helping it reason through the steps it can take to help end users. A knowledge base can be used not only to answer user queries, and analyze documents, but also to \n        augment prompts provided to foundation models by providing context to the prompt. When answering user \n        queries, the knowledge base retains conversation context. The knowledge base also grounds answers in \n        citations so that users can find further information by looking up the exact \n        text that a response is based on and also check that the response makes sense and is factually \n        correct. You take the following steps to set up and use your knowledge base. Gather source documents to add to your knowledge base. Store your source documents in a supported data source . (Optional if using Amazon S3 to store your source documents) Create a metadata \n                file for each source document to allow for filtering of results during knowledge base \n                query. (Optional) Set up your own supported vector store to index the vector embeddings representation of your data. You can use \n                the Amazon Bedrock console to create an Amazon OpenSearch Serverless vector store for you. Create and configure your knowledge base. You must enable model access to use a model \n                that's supported for knowledge bases. If you use the Amazon Bedrock API, take note of your model Amazon Resource Name (ARN) that's required \n                for converting your data into vector embeddings and for knowledge base retrieval and generation . Copy the model ID for your chosen model for \n                            knowledge bases and construct the model ARN using the model (resource) ID, following the \n                provided ARN examples for your model resource type. If you use the Amazon Bedrock console, you are not required to construct a model ARN, as you can select an available model as \n                part of the steps for creating a knowledge base. Set up your application or agent to query the knowledge base and return augmented responses. Topics How Amazon Bedrock knowledge bases work Supported regions and models for Amazon Bedrock knowledge bases Chat with your document without a knowledge base configured Build and manage knowledge bases for retrieval and responses Connect to your data repository for your knowledge base Test your knowledge base with queries and responses Deploy your knowledge base for your AI application Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Manage a work team for human evaluations How knowledge bases work"}, {"title": "Carry out a conversation with the Converse API operations - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html", "content": "Carry out a conversation with the Converse API operations PDF RSS You can use the Amazon Bedrock Converse API to create conversational applications that send\n        and receive messages to and from an Amazon Bedrock model. For example, you can\n        create a chat bot that maintains a conversation over many turns and uses a persona or tone\n        customization that is unique to your needs, such as a helpful technical support assistant. To use the Converse API, you use the Converse or ConverseStream (for streaming responses) operations to send messages to a\n        model. It is possible to use the existing base inference operations ( InvokeModel or InvokeModelWithResponseStream ) for conversation applications. However, we\n        recommend using the Converse API as it provides consistent API, that works with all\n        Amazon Bedrock models that support messages. This means you can write code once and use it with\n        different models. Should a model have unique inference parameters, the Converse API also\n        allows you to pass those unique parameters in a model specific structure. You can use the Converse API to implement tool use and guardrails in your applications. Note With Mistral AI and Meta models, the Converse API embeds your input in a\n            model-specific prompt template that enables conversations. For code examples, see the following: Python examples for this topic \u2013 Converse API examples Various languages and models \u2013 Code examples for Amazon Bedrock Runtime using AWS SDKs Java tutorial \u2013 A Java developer's guide to Bedrock's new Converse API JavaScript tutorial \u2013 A developer's guide to Bedrock's new Converse API Topics Supported models and model features Using the Converse API Converse API examples Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Submit a single prompt with InvokeModel Supported models and model features"}, {"title": "Use a tool to complete an Amazon Bedrock model response - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html", "content": "Use a tool to complete an Amazon Bedrock model response PDF RSS You can use the Amazon Bedrock API to give a model access to tools that can help it generate\n        responses for messages that you send to the model. For example, you might have a chat\n        application that lets users find out the most popular song played on a radio station. To\n        answer a request for the most popular song, a model needs a tool that can query and return\n        the song information. Note Tool use with models is also known as Function calling . In Amazon Bedrock, the model doesn't directly call the tool. Rather, when you send a message to\n        a model, you also supply a definition for one or more tools that could potentially help the\n        model generate a response. In this example, you would supply a definition for a tool that\n        returns the most popular song for a specified radio station. If the model determines that it\n        needs the tool to generate a response for the message, the model responds with a request for\n        you to call the tool. It also includes the input parameters (the required radio station) to\n        pass to the tool. In your code, you call the tool on the model's behalf. In this scenario, assume the tool\n        implementation is an API. The tool could just as easily be a database, Lambda function, or\n        some other software. You decide how you want to implement the tool. You then continue the\n        conversation with the model by supplying a message with the result from the tool. Finally\n        the model generates a response for the orginal message that includes the tool results that\n        you sent to the model. To use tools with a model you can use the Converse API ( Converse or ConverseStream ). The example\n        code in this topic uses the Converse API to show how to use a tool that gets the most\n        popular song for a radio station. For general information about calling the Converse API,\n        see Carry out a conversation with the Converse API operations . It is possible to use tools with the base inference operations ( InvokeModel or InvokeModelWithResponseStream ). To find the inference parameters that you pass\n        in the request body, see the inference parameters for the model that you want to use. We recommend using the Converse API as it provides\n        a consistent API, that works with all Amazon Bedrock models that support tool use. For information about models that support tool calling, see Supported models and model features . Topics Call a tool with the Converse API Converse API tool use examples Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Converse API examples Call a tool with the Converse API"}, {"title": "Construct and store reusable prompts with Prompt management in Amazon Bedrock - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html", "content": "Construct and store reusable prompts with Prompt management in Amazon Bedrock PDF RSS Note Prompt management is in preview and is subject to change. Amazon Bedrock provides you the ability to create and save your own prompts using Prompt management so that you can save time by applying the same prompt to different workflows. When you create your a prompt, you can select a model to run inference on it and modify the inference parameters to use. You can include variables in the prompt so that you can adjust the prompt for different use case. When you test your prompt, you have the option of comparing different variants of the prompt and choosing the variant that yields outputs that are best-suited for your use case. While iterating on your prompt, you can save versions of it. You integrate a prompt into your application with the help of Amazon Bedrock Prompt flows . The following is the general workflow for using Prompt management: Create a prompt in Prompt management that you want to reuse across different use cases. Include variables to provide flexibility in the model prompt. Choose a model to run inference on the prompt and modify the inference configurations as necessary. Fill in test values for the variables and run the prompt. You can create variants of your prompt and compare the outputs of different variants to choose the best one for your use case. Topics Key definitions Supported Regions and models for Prompt management Prerequisites for prompt management Create a prompt using Prompt management View information about prompts using Prompt management Modify a prompt using Prompt management Test a prompt using Prompt management Deploy a prompt to your application using versions in Prompt management Delete a prompt in Prompt management Run Prompt management code samples Key definitions The following list introduces you to the basic concepts of Prompt management: Prompt \u2013 An input provided to a model to guide it to generate an appropriate response or output. Variable \u2013 A placeholder that you can include in the prompt. You can include values for each variable when testing the prompt or when you at runtime. Prompt variant \u2013 An alternative configuration of the prompt, including its message or the model or inference configurations used. You can create different variants of a prompt, test them, and save the variant that you want to keep. Prompt builder \u2013 A tool in the Amazon Bedrock console that lets you create, edit, and test prompts and their variants in a visual interface. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Prompt templates and examples for\n    Amazon Bedrock text models Supported Regions and models"}, {"title": "Build an end-to-end generative AI workflow with Amazon Bedrock Prompt flows - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/flows.html", "content": "Build an end-to-end generative AI workflow with Amazon Bedrock Prompt flows PDF RSS Note Amazon Bedrock Prompt flows is in preview and is subject to change. Amazon Bedrock Prompt flows offers the ability for you to use supported foundation models (FMs) to build workflows by \n        linking prompts, foundational models, and other AWS services to create end-to-end solutions. With prompt flows, you can quickly build complex generative AI workflows using a visual builder, easily integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and other AWS services such as AWS Lambda \n            by transferring data between them, and deploying immutable workflows to move from testing to production in few clicks. Refer to the following resources for more information about Amazon Bedrock Prompt flowss: Pricing for Amazon Bedrock Prompt flows is dependent on the resources that you use. For example, if you invoke a flow with a prompt node that uses an Amazon Titan model, you'll be charged for invoking that model. For more information, see Amazon Bedrock pricing . To see quotas for prompt flows, see Amazon Bedrock endpoints and quotas in the AWS General Reference. The following are some example tasks that you can build a prompt flow for in Amazon Bedrock: Create and send an email invite \u2013 Create a prompt flow connecting a prompt node, knowledge base node, and Lambda function node. Provide the following prompt to generate an email body: Send invite to John Smith\u2019s extended team for in-person documentation read for an hour at 2PM EST next Tuesday . After processing the prompt, the prompt flow queries a knowledge base to look up the email addresses of John Smith's extended team, and then sends the input to a Lambda function to send the invite to all the team members in the list. Troubleshoot using the error message and the ID of the resource that is causing the error \u2013 The prompt flow looks up the possible causes of the error from a documentation knowledge base, pulls system logs and other relevant information about the resource, and updates the faulty configurations and values for the resource. Generate reports \u2013 Build a prompt flow to generate metrics for top products. The prompt flow looks for the sales metrics in a\n                database, aggregates the metrics, generates a summary report for top product\n                purchases, and publishes the report on the specified portal. Ingest data from a specified dataset \u2013 Provide a prompt such as the following: Start ingesting new datasets added after 3/31 and report\n                    failures . The prompt flow starts preparing data for ingestion and keeps\n                reporting on the status. After the data preparation is complete, the prompt flow starts the\n                ingestion process filtering the failed data. After data ingestion is complete, the\n                prompt flow summarizes the failures and publishes a failure report. Flows for Amazon Bedrock makes it easy for you link foundation models (FMs), prompts, and other AWS services to quickly create, test, and run your prompt flows.  You can manage prompt flows \n        using the visual builder in the Amazon Bedrock console or through the APIs. The general steps for creating, testing, and deploying a prompt flow are as follows: Create the prompt flow: Specify a prompt flow name, description, and appropriate IAM permissions. Design your prompt flow by deciding the nodes you want to use. Create or define all the resources you require for each node. For example, if you are planning to use an AWS Lambda function, define the functions you need for the node to complete its task. Add nodes to your prompt flow, configure them, and create connections between the nodes by linking the output of a node to the input of another node in the prompt flow. Test the prompt flow: Prepare the prompt flow, so that the latest changes apply to the working draft of the prompt flow, a version of the prompt flow that you can use to iteratively test and update your prompt flow Test the prompt flow by invoking it with sample inputs to see the outputs it yields. When you're satisfied with a prompt flow's configuration, you can create a snapshot of it by publishing a version . The version preserves prompt flow definition as it exists at the time of the creation. Versions are immutable because they act as a snapshot of the prompt flow at the time it was created. Deploy the prompt flow Create an alias that points to the version of your prompt flow that you want to use in your application. Set up your application to make InvokeFlow requests to the alias. If you need to revert to an older version or upgrade to a newer one, you can change the routing configuration of the alias. Topics How Amazon Bedrock Prompt flows works Supported regions and models for prompt flows Prerequisites for Amazon Bedrock Prompt flows Create a prompt flow in Amazon Bedrock View information about prompt flows in Amazon Bedrock Modify a prompt flow in Amazon Bedrock Test a prompt flow in Amazon Bedrock Deploy a prompt flow to your application using versions and aliases Delete a prompt flow in Amazon Bedrock Run Amazon Bedrock Prompt flows code samples Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Delete an alias of an agent in Amazon Bedrock How it works"}, {"title": "What is Amazon SageMaker? - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html", "content": "What is Amazon SageMaker? PDF RSS Amazon SageMaker is a fully managed machine learning (ML) service. With SageMaker, data scientists and\n    developers can quickly and confidently build, train, and deploy ML models into a\n    production-ready hosted environment. It provides a UI experience for running ML workflows that\n    makes SageMaker ML tools available across multiple integrated development environments (IDEs). With SageMaker, you can store and share your data without having to build and manage your own\n    servers. This gives you or your organizations more time to collaboratively build and develop\n    your ML workflow, and do it sooner. SageMaker provides managed ML algorithms to run efficiently\n    against extremely large data in a distributed environment. With built-in support for\n    bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that\n    adjust to your specific workflows. Within a few steps, you can deploy a model into a secure and\n    scalable environment from the SageMaker console. Topics Pricing for Amazon SageMaker Recommendations for a first-time user of Amazon SageMaker Overview of machine learning with Amazon SageMaker Amazon SageMaker Features Pricing for Amazon SageMaker For information about AWS Free Tier limits\n      and the cost of using SageMaker, see Amazon SageMaker\n        Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Recommendations for a first-time user of Amazon SageMaker"}, {"title": "SageMaker JumpStart pretrained models - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html", "content": "SageMaker JumpStart pretrained models PDF RSS SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to\n    help you get started with machine learning. You can incrementally train and tune these models\n    before deployment. JumpStart also provides solution templates that set up infrastructure for\n    common use cases, and executable example notebooks for machine learning with SageMaker. You can deploy, fine-tune, and evaluate pretrained models from popular models hubs through\n    the JumpStart landing page in the updated Studio experience. You can also access pretrained models, solution templates, and examples through the JumpStart\n    landing page in Amazon SageMaker Studio Classic. The following steps show how to access JumpStart models using Amazon SageMaker Studio and Amazon SageMaker Studio Classic. You can also access JumpStart models using the SageMaker Python SDK. For information about how to\n    use JumpStart models programmatically, see Use SageMaker JumpStart Algorithms with Pretrained Models . Open and use JumpStart in Studio The following sections give information on how to open, use, and manage JumpStart from the\n      Studio UI. Important As of November 30, 2023, the previous Amazon SageMaker Studio experience is now named\n            Amazon SageMaker Studio Classic. The following section is specific to using the updated Studio\n            experience. For information about using the Studio Classic application, see Amazon SageMaker Studio Classic . Open JumpStart in Studio In Amazon SageMaker Studio, open the JumpStart landing page either through the Home page or the Home menu on the left-side\n        panel. This opens the SageMaker JumpStart landing page where you can explore\n        model hubs and search for models. From the Home page, choose JumpStart in the Prebuilt and automated solutions pane. From the Home menu in the left panel, navigate to the SageMaker JumpStart node. For more information on getting started with Amazon SageMaker Studio, see Amazon SageMaker Studio . Important Before downloading or using third-party content: You are responsible for reviewing and\n          complying with any applicable license terms and making sure that they are acceptable for\n          your use case. Use JumpStart in Studio From the SageMaker JumpStart landing page in Studio, you can explore\n        model hubs from providers of both proprietary and publicly available models. You can find specific hubs or models using the search bar. Within each model hub, you\n        can search directly for models, sort by provided attributes, or filter based on a list of\n        provided model tasks. Manage JumpStart in Studio Choose a model to see its model detail card. In the upper right-hand corner of the model\n        detail card, choose Fine-tune , Deploy , or Evaluate to start working through the fine-tuning, deployment, or\n        evaluation workflows, respectively. Note that not all models are available for fine-tuning\n        or evaluation. For more information on each of these options, see Use foundation\n                    models in Studio . Open and use JumpStart in Studio Classic The following sections give information on how to open, use, and manage JumpStart from the\n      Amazon SageMaker Studio Classic UI. Important As of November 30, 2023, the previous Amazon SageMaker Studio experience is now named\n      Amazon SageMaker Studio Classic. The following section is specific to using the Studio Classic application. For\n      information about using the updated Studio experience, see Amazon SageMaker Studio . Open JumpStart in Studio Classic In Amazon SageMaker Studio Classic, open the JumpStart landing page either through the Home page or the Home menu on the left-side\n        panel. From the Home page you can either: Choose JumpStart in the Prebuilt and automated\n                  solutions pane. This opens the SageMaker JumpStart landing\n                page. Choose a model directly in the SageMaker JumpStart landing page, or\n                choose the Explore All option to see available solutions or\n                models of a specific type. From the Home menu in the left panel you can either: Navigate to the SageMaker JumpStart node, then choose Models, notebooks, solutions . This opens the SageMaker\n                  JumpStart landing page. Navigate to the JumpStart node, then choose Launched\n                  JumpStart assets . The Launched JumpStart assets page lists your currently launched\n                solutions, deployed model endpoints, and training jobs created with JumpStart. You can\n                access the JumpStart landing page from this tab by clicking on the Browse\n                  JumpStart button at the top right of the tab. The JumpStart landing page lists available end-to-end machine learning solutions, pretrained\n        models, and example notebooks. From any individual solution or model page, you can choose\n        the Browse JumpStart button ( ) at the top right of the tab to return to the SageMaker\n          JumpStart page. Important Before downloading or using third-party content: You are responsible for reviewing and\n          complying with any applicable license terms and making sure that they are acceptable for\n          your use case. Use JumpStart in Studio Classic From the SageMaker JumpStart landing page, you can browse for solutions,\n        models, notebooks, and other resources. You can find JumpStart resources by using the search bar, or by browsing each category.\n        Use the tabs to filter the available solutions by categories: Solutions \u2013 In one step, launch comprehensive\n            machine learning solutions that tie SageMaker to other AWS services. Select Explore All Solutions to view all available solutions. Resources \u2013 Use example notebooks, blogs, and\n            video tutorials to learn and head start your problem types. Blogs \u2013 Read details and solutions from\n                machine learning experts. Video tutorials \u2013 Watch video tutorials for\n                SageMaker features and machine learning use cases from machine learning experts. Example notebooks \u2013 Run example notebooks\n                that use SageMaker features like Spot Instance training and experiments over a large\n                variety of model types and use cases. Data types \u2013 Find a model by data type (e.g.,\n            Vision, Text, Tabular, Audio, Text Generation). Select Explore All\n              Models to view all available models. ML tasks \u2013 Find a model by problem type (e.g.,\n            Image Classification, Image Embedding, Object Detection, Text Generation). Select Explore All Models to view all available models. Notebooks \u2013 Find example notebooks that use SageMaker\n            features across multiple model types and use cases. Select Explore All\n              Notebooks to view all available example notebooks. Frameworks \u2013 Find a model by framework (e.g.,\n            PyTorch, TensorFlow, Hugging Face). Manage JumpStart in Studio Classic From the Home menu in the left panel, navigate to SageMaker\n          JumpStart , then choose Launched JumpStart assets to list your\n        currently launched solutions, deployed model endpoints, and training jobs created with\n        JumpStart. Topics Amazon SageMaker JumpStart Foundation Models Private curated hubs for foundation model access control in JumpStart Amazon SageMaker JumpStart in Studio Classic Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions API reference Foundation models"}, {"title": "Understand options for evaluating large language models with SageMaker Clarify - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate.html", "content": "Understand options for evaluating large\n            language models with SageMaker Clarify PDF RSS Important In order to use SageMaker Clarify Foundation Model Evaluations, you must upgrade to the new Studio experience.\n            As of November 30, 2023, the previous Amazon SageMaker Studio experience is now named\n            Amazon SageMaker Studio Classic. The foundation evaluation feature can only be used in the updated\n            experience. For information about how to update Studio, see Migration from Amazon SageMaker Studio Classic . For\n            information about using the Studio Classic application, see Amazon SageMaker Studio Classic . Using Amazon SageMaker Clarify you can evaluate large language models (LLMs) by creating model evaluation jobs. A model evaluation job allows you to evaluate and compare model quality and responsibility metrics for text-based foundation models from JumpStart. Model evaluation jobs also support the use of JumpStart models that have already been deployed to an endpoint. You can create a model evaluation job using three different approaches. Create an automated model evaluation jobs in Studio \u2013 Automatic model evaluation jobs allow you to quickly evaluate a model's ability to perform a task. You can either provide your own custom prompt dataset that you've tailored to a specific use case, or you can use an available built-in dataset. Create a model evaluation jobs that use human workers in  Studio \u2013 Model evaluation jobs that use human workers allow you to bring human input to the model evaluation process. They can be employees of your company or a group of subject-matter experts from your industry. Create an automated model evaluation job using the fmeval library \u2013 Creating a job using the fmeval give you the most fine-grained control over your model evaluation jobs. It also supports the use LLMs outside of AWS or non-JumpStart based models from other services. Model evaluation jobs support common use cases for LLMs such as text generation, text classification, question and answering, and text summarization. Open-ended generation \u2013 The production of\n                natural human responses to text that does not have a pre-defined structure. Text summarization \u2013 The generation of a\n                concise and condensed summary while retaining the meaning and key information that's\n                contained in larger text. Question answering \u2013 The generation of a\n                relevant and accurate response to a prompt. Classification \u2013 Assigning a category,\n                such as a label or score to text, based on its content. The following topics describe the available model evaluation tasks, and the kinds of metrics you can use. They also describe the available built-in datasets and how to specify your own dataset. Topics What are foundation model evaluations? Get started with model\n            evaluations Using prompt datasets and available evaluation dimensions in model evaluation jobs Create a model evaluation job that uses human workers Automatic model evaluation Understand the results of your model\n\t\t\tevaluation job Customize your workflow using the fmeval library Model evaluation notebook\n            tutorials Resolve errors when\n            creating a model evaluation job in Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Evaluate, explain, and detect bias in models Model evaluations"}, {"title": "Amazon SageMaker Canvas - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/canvas.html", "content": "Amazon SageMaker Canvas PDF RSS Amazon SageMaker Canvas gives you the ability to use machine learning to generate predictions without\n        needing to write any code. The following are some use cases where you can use SageMaker Canvas: Predict customer churn Plan inventory efficiently Optimize price and revenue Improve on-time deliveries Classify text or images based on custom categories Identify objects and text in images Extract information from documents With Canvas, you can chat with popular large language models (LLMs), access\n        Ready-to-use models, or build a custom model trained on your data. Canvas chat is a functionality that leverages open-source and Amazon LLMs to help you\n        boost your productivity. You can prompt the models to get assistance with tasks such as\n        generating content, summarizing or categorizing documents, and answering questions. To learn\n        more, see Generative AI foundation models in SageMaker Canvas . The Ready-to-use models in Canvas\n        can extract insights from your data for a variety of use cases. You don\u2019t have to build a\n        model to use Ready-to-use models because they are powered by Amazon AI services, including Amazon Rekognition , Amazon Textract ,\n        and Amazon Comprehend . You\n        only have to import your data and start using a solution to generate predictions. If you want a model that is customized to your use case and trained with your data, you\n        can build a model . You can get predictions\n        customized to your data by doing the following: Import your data from one or more data sources. Build a predictive model. Evaluate the model's performance. Generate predictions with the model. Canvas supports the following types of custom models: Numeric prediction (also known as regression ) Categorical prediction for 2 and 3+ categories (also known as binary and multi-class\n                    classification ) Time series forecasting Single-label image prediction (also known as image\n                    classification ) Multi-category text prediction (also known as multi-class\n                    text classification ) You can also bring\n            your own models into Canvas from Amazon SageMaker Studio Classic. To learn more about pricing, see the SageMaker Canvas pricing page . You can also see Billing and cost in SageMaker Canvas for more\n        information. SageMaker Canvas is currently available in the following Regions: US East (Ohio) US East (N. Virginia) US West (N. California) US West (Oregon) Asia Pacific (Mumbai) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Paris) Europe (Stockholm) South America (S\u00e3o Paulo) Topics Are you a first-time SageMaker Canvas user? Getting started with using Amazon SageMaker Canvas Tutorial: Build an end-to-end machine\n            learning workflow in SageMaker Canvas Amazon SageMaker Canvas setup and permissions management (for IT\n      administrators) Data import Data preparation Generative AI foundation models in SageMaker Canvas Ready-to-use models Custom models Logging out of Amazon SageMaker Canvas Limitations and troubleshooting Billing and cost in SageMaker Canvas Are you a first-time SageMaker Canvas user? If you are a first-time user of SageMaker Canvas, we recommend that you begin by reading the\n            following sections: For IT administrators \u2013 Amazon SageMaker Canvas setup and permissions management (for IT\n      administrators) For analysts and individual users \u2013 Getting started with using Amazon SageMaker Canvas For an example of an end to end workflow \u2013 Tutorial: Build an end-to-end machine\n            learning workflow in SageMaker Canvas Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Troubleshooting Getting started"}, {"title": "Applications supported in Amazon SageMaker Studio - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-apps.html", "content": "Applications supported in Amazon SageMaker Studio PDF RSS Important As of November 30, 2023, the previous Amazon SageMaker Studio experience is now named\n            Amazon SageMaker Studio Classic. The following section is specific to using the updated Studio\n            experience. For information about using the Studio Classic application, see Amazon SageMaker Studio Classic . Amazon SageMaker Studio supports the following applications: Code Editor, based on Code-OSS, Visual Studio Code - Open Source \u2013 Code Editor offers a lightweight and\n                powerful integrated development environment (IDE) with familiar shortcuts, terminal,\n                and advanced debugging capabilities and refactoring tools. It is a fully managed,\n                browser-based application in Studio. For more information, see Code Editor in Amazon SageMaker Studio . Amazon SageMaker Studio Classic \u2013 Amazon SageMaker Studio Classic is a web-based\n                IDE for machine learning. With Studio Classic, you can build, train, debug, deploy, and\n                monitor your machine learning models. For more information, see Amazon SageMaker Studio Classic . JupyterLab \u2013JupyterLab offers a set of\n                capabilities that augment the fully managed notebook offering. It includes kernels\n                that start in seconds, a pre-configured runtime with popular data science, machine\n                learning frameworks, and high performance block storage. For more information, see SageMaker JupyterLab . Amazon SageMaker Canvas \u2013 With SageMaker Canvas, you can use machine\n                learning to generate predictions without writing code.\u00a0With Canvas, you can chat\n                with popular large language models (LLMs), access ready-to-use models, or build a\n                custom model that's trained on your data. For more information, see Amazon SageMaker Canvas . RStudio \u2013 RStudio is an integrated development\n                environment for R. It includes a console and syntax-highlighting editor that\n                supports running code directly. It also includes tools for plotting, history,\n                debugging, and workspace management. For more information, see RStudio on Amazon SageMaker . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Modify your idle shutdown time limit Amazon SageMaker Studio spaces"}, {"title": "Amazon Titan Text Embeddings models - Amazon Bedrock", "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html", "content": "Amazon Titan Text Embeddings models PDF RSS Amazon Titan Embeddings text models include Amazon Titan Text Embeddings v2 and Titan Text Embeddings G1 model. Text embeddings represent meaningful vector representations of unstructured text such as documents, paragraphs, and sentences. You input a body of text \n   and the output is a (1 x n) vector. You can use embedding vectors for a wide variety of applications. The Amazon Titan Text Embedding v2 model ( amazon.titan-embed-text-v2:0 ) can intake \n   up to 8,192 tokens and outputs a vector of 1,024 dimensions. The model also works in 100+ different languages. The model is optimized \n   for text retrieval tasks, but can also perform additional tasks, such as semantic similarity and clustering. Amazon Titan Embeddings text v2\n   also supports long documents, however, for retrieval tasks it is recommended to segment documents into logical segments \n   , such as paragraphs or sections. Amazon Titan Embeddings models generate meaningful semantic representation of documents,\n   paragraphs and sentences. Amazon Titan Text Embeddings takes as input a body of text and\n   generates a n-dimensional vector. Amazon Titan Text Embeddings is offered via latency-optimized\n   endpoint invocation for faster search (recommended during the retrieval step) as well as\n   throughput optimized batch jobs for faster indexing. The Amazon Titan Embedding Text v2 model supports the following languages: English,  German, French, Spanish, Japanese, Chinese, Hindi, Arabic, Italian, \n   Portuguese, Swedish, Korean, Hebrew, Czech, Turkish, Tagalog, Russian, Dutch, Polish, Tamil, Marathi, Malayalam, Telugu, Kannada, Vietnamese, Indonesian, Persian, \n   Hungarian, Modern Greek, Romanian, Danish, Thai, Finnish, Slovak, Ukrainian, Norwegian, Bulgarian, Catalan, Serbian, Croatian, Lithuanian, Slovenian, Estonian, \n   Latin, Bengali, Latvian, Malay, Bosnian, Albanian, Azerbaijani, Galician, Icelandic, Georgian, Macedonian, Basque, Armenian, Nepali, \n   Urdu, Kazakh, Mongolian, Belarusian, Uzbek, Khmer, Norwegian Nynorsk, Gujarati, Burmese, Welsh, Esperanto, Sinhala, Tatar, Swahili, Afrikaans, Irish, \n   Panjabi, Kurdish, Kirghiz, Tajik, Oriya, Lao, Faroese, Maltese, Somali, Luxembourgish, Amharic, Occitan, Javanese, Hausa, Pushto, Sanskrit, \n   Western Frisian, Malagasy, Assamese, Bashkir, Breton, Waray (Philippines), Turkmen, Corsican, Dhivehi, Cebuano, Kinyarwanda, Haitian, Yiddish, Sindhi, Zulu, \n   Scottish Gaelic, Tibetan, Uighur, Maori, Romansh, Xhosa, Sundanese, Yoruba. Note Amazon Titan Text Embeddings v2 model and Titan Text Embeddings v1 model do not supports inference parameters such as maxTokenCount or topP . Amazon Titan Text Embeddings V2 model Model ID \u2013 amazon.titan-embed-text-v2:0 Max input text tokens \u2013 8,192 Languages \u2013 English (100+ languages in preview) Max input image size \u2013 5 MB Output vector size \u2013 1,024 (default), 384, 256 Inference types \u2013 On-Demand, Provisioned Throughput Supported use cases \u2013 RAG, document search, reranking, classification, etc. Note Titan Text Embeddings V2 takes as input a non-empty string with up to 8,192 tokens. The characters to \n    token ratio in English is 4.7 characters per token. While Titan Text Embeddings V1 and Titan Text Embeddings V2 are able to accommodate up to 8,192 tokens, it is recommended \n    to segment documents into logical segments (such as paragraphs or sections). To use the text or image embeddings models, use the Invoke Model API operation with amazon.titan-embed-text-v1 or amazon.titan-embed-image-v1 as the model Id and retrieve the embedding object in the response. To see Jupyter notebook examples: Sign in to the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/home. From the left-side menu, choose Base models . Scroll down and select the Amazon Titan Embeddings G1 - Text model In the Amazon Titan Embeddings G1 - Text tab (depending on which model you chose), \n     select View example notebook to see example notebooks for embeddings. For more information on preparing your dataset for multimodal training, see Preparing your dataset . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Amazon Titan Text Amazon Titan Multimodal Embeddings G1"}, {"title": "Connecting Amazon Q Business data source connectors - Amazon Q Business", "url": "https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/supported-connectors.html", "content": "Connecting Amazon Q Business data source\n                connectors PDF RSS To connect a data source to your Amazon Q Business application environment, you can use\n            the AWS Management Console or the CreateDataSource API operation. By using the CreateDataSource API operation, you can configure tags, sync\n            run schedules, and configure Amazon VPC settings. Then, you can use the configuration parameter to provide all other configuration information\n            specific to your data source connector. Note This procedure is available if you chose the Use native retriever option to configure your\n                application environment. This section contains an overview of data source connector features, recommended best\n            practices for configuration, and configuration information specific to your data source\n            connector. Topics Data source connector concepts What is a document? Best practices for data source connector\n                    configuration in Amazon Q Business Supported connectors Understanding Amazon Q Business\n                    User Store Using Amazon VPC with Amazon Q Business connectors Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Uploading files Concepts"}, {"title": "Retrieval Augmented Generation - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html", "content": "Retrieval Augmented\n                    Generation PDF RSS Foundation models are usually trained offline, making the model agnostic to any\n                data that is created after the model was trained. Additionally, foundation models\n                are trained on very general domain corpora, making them less effective for\n                domain-specific tasks. You can use Retrieval Augmented Generation (RAG) to retrieve\n                data from outside a foundation model and augment your prompts by adding the relevant\n                retrieved data in context. For more information about RAG model architectures, see Retrieval-Augmented Generation for\n                    Knowledge-Intensive NLP Tasks . With RAG, the external data used to augment your prompts can come from multiple\n                data sources, such as a document repositories, databases, or APIs. The first step is\n                to convert your documents and any user queries into a compatible format to perform\n                relevancy search. To make the formats compatible, a document collection, or\n                knowledge library, and user-submitted queries are converted to numerical\n                representations using embedding language models. Embedding is the process by which text is given numerical\n                representation in a vector space. RAG model architectures compare the embeddings of\n                user queries within the vector of the knowledge library. The original user prompt is\n                then appended with relevant context from similar documents within the knowledge\n                library. This augmented prompt is then sent to the foundation model. You can update\n                knowledge libraries and their relevant embeddings asynchronously. The retrieved document should be large enough to contain useful context to help\n                augment the prompt, but small enough to fit into the maximum sequence length of the\n                prompt. You can use task-specific JumpStart models, such as the General Text Embeddings\n                (GTE) model from Hugging Face, to provide the embeddings for your\n                prompts and knowledge library documents. After comparing the prompt and document\n                embeddings to find the most relevant documents, construct a new prompt with the\n                supplemental context. Then, pass the augmented prompt to a text generation model of\n                your choosing. Example\n                        notebooks For more information on RAG foundation model solutions, see the following\n                    example notebooks: Retrieval-Augmented Generation: Question Answering using LangChain\n                                and Cohere\u2019s Generate and Embedding Models from SageMaker\n                                JumpStart Retrieval-Augmented Generation: Question Answering using LLama-2,\n                                Pinecone and Custom Dataset Retrieval-Augmented Generation: Question Answering based on Custom\n                                Dataset with Open-sourced LangChain Library Retrieval-Augmented Generation: Question Answering based on Custom\n                                Dataset Retrieval-Augmented Generation: Question Answering using Llama-2\n                                and Text Embedding Models Amazon SageMaker JumpStart - Text Embedding and Sentence Similarity You can clone the Amazon SageMaker examples repository to run the available JumpStart foundation\n                    model examples in the Jupyter environment of your choice within Studio. For\n                    more information on applications that you can use to create and access Jupyter in\n                    SageMaker, see Applications supported in Amazon SageMaker Studio . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Fine-tune a model with prompt instructions Evaluate a Model"}, {"title": "Guide to getting set up with Amazon SageMaker - Amazon SageMaker", "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html", "content": "Guide to getting set up with Amazon SageMaker PDF RSS To use the features in Amazon SageMaker, you must have access to Amazon SageMaker. To set up Amazon SageMaker and\n        its features, use one of the following options. Use quick setup : Fastest setup for\n                individual users with default settings. Use custom setup : Advanced setup for\n                enterprise Machine Learning (ML) administrators. Ideal option for ML administrators\n                setting up SageMaker for many users or an organization. Note You do not need to set up SageMaker if: An email is sent to you inviting you to create a password to use the IAM Identity Center\n                    authentication. The email also contains the AWS access portal URL you use to sign\n                    in. For more information about signing in to the AWS access portal, see Sign in to the AWS access portal . You intend to use the Amazon SageMaker Studio Lab ML environment. Studio Lab does not require you\n                    to have an AWS account. For information about Studio Lab, see Amazon SageMaker Studio Lab . If you are using the AWS CLI, SageMaker APIs, or SageMaker SDKs You do not need to set up SageMaker if any of the prior situations apply. You can skip the\n            rest of this Guide to getting set up with Amazon SageMaker chapter and navigate to the\n            following: Automated ML, no-code, or low-code Machine learning environments offered by Amazon SageMaker APIs, CLI, and SDKs Topics Complete Amazon SageMaker prerequisites Use quick setup for Amazon SageMaker Use custom setup for Amazon SageMaker Amazon SageMaker domain overview Supported Regions and Quotas Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions SageMaker Features Complete Amazon SageMaker prerequisites"}, {"title": "Recommended Trainium Instances - AWS Deep Learning AMIs", "url": "https://docs.aws.amazon.com/dlami/latest/devguide/trainium.html", "content": "Recommended Trainium Instances PDF RSS AWS Trainium instances are designed to provide high performance and cost efficiency for deep learning model inference workloads. \n\t\t\tSpecifically, Trn1 instance types use AWS Trainium chips and the AWS Neuron SDK , which is integrated with popular machine learning frameworks \n\t\t\tsuch as TensorFlow and PyTorch. Customers can use Trn1 instances to run large scale machine learning inference applications such as search, recommendation engines, \n\t\t\tcomputer vision, speech recognition, natural language processing, personalization, and fraud detection, at the lowest cost in the cloud. Note The size of your model should be a factor in choosing an instance. If your model exceeds an\n\t\t\tinstance's available RAM, choose a different instance type with enough memory for\n\t\t\tyour application. Amazon EC2 Trn1 Instances have up to up to 16 AWS Trainium chips and 100 Gbps of networking throughput. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Inferentia Setting up"}, {"title": "Deploy ML inference workloads with AWSInferentia on Amazon EKS - Amazon EKS", "url": "https://docs.aws.amazon.com/eks/latest/userguide/inferentia-support.html", "content": "Help improve this page Want to contribute to this user guide? Scroll to the bottom of this page\n   and select Edit this page on GitHub . Your contributions will help make our\n   user guide better for everyone. Help improve this page Want to contribute to this user guide? Scroll to the bottom of this page\n   and select Edit this page on GitHub . Your contributions will help make our\n   user guide better for everyone. Deploy ML inference workloads with AWSInferentia on Amazon EKS PDF RSS This topic describes how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and (optionally) deploy a\n        sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia chips, which are custom\n        built by AWS to provide high performance and lowest cost inference in the cloud. Machine\n        learning models are deployed to containers using AWS Neuron , a specialized software development\n        kit (SDK) consisting of a compiler, runtime, and profiling tools that optimize the machine\n        learning inference performance of Inferentia chips. AWS Neuron supports popular machine\n        learning frameworks such as TensorFlow, PyTorch, and MXNet. Note Neuron device logical IDs must be contiguous. If a Pod requesting multiple Neuron devices\n        is scheduled on an inf1.6xlarge or inf1.24xlarge instance type\n        (which have more than one Neuron device), that Pod will fail to start if the Kubernetes\n        scheduler selects non-contiguous device IDs. For more information, see Device logical IDs must be\n            contiguous on GitHub. Prerequisites Have eksctl installed on your computer. If you don't have it installed, see Installation in the eksctl documentation. Have kubectl installed on your computer. For more information,\n                    see Set up kubectl and eksctl . (Optional) Have python3 installed on your computer. If you don't\n                    have it installed, then see Python downloads for installation instructions. Create a cluster To create a cluster with Inf1 Amazon EC2 instance nodes Create a cluster with Inf1 Amazon EC2 instance nodes. You can replace inf1.2xlarge with any Inf1 instance type .\n                    The eksctl utility detects that you are launching a node group with an Inf1 instance type and will start your nodes using one of the\n                    Amazon EKS optimized accelerated Amazon Linux AMIs. Note You can't use IAM roles for\n                            service accounts with TensorFlow Serving. eksctl create cluster \\\n    --name inferentia \\\n    --region region-code \\\n    --nodegroup-name ng-inf1 \\\n    --node-type inf1.2xlarge \\\n    --nodes 2 \\\n    --nodes-min 1 \\\n    --nodes-max 4 \\\n    --ssh-access \\\n    --ssh-public-key your-key \\\n    --with-oidc Note Note the value of the following line of the output. It's used in a later\n                        (optional) step. [9]  adding identity \"arn:aws:iam:: 111122223333 :role/eksctl- inferentia - nodegroup-ng-in -NodeInstanceRole- FI7HIYS3BS09 \" to auth ConfigMap When launching a node group with Inf1 instances, eksctl automatically installs the AWS Neuron Kubernetes\n                    device plugin. This plugin advertises Neuron devices as a system resource to the\n                    Kubernetes scheduler, which can be requested by a container. In addition to the\n                    default Amazon EKS node IAM policies, the Amazon S3 read only access policy is added so\n                    that the sample application, covered in a later step, can load a trained model\n                    from Amazon S3. Make sure that all Pods have started correctly. kubectl get pods -n kube-system Abbreviated output: NAME                                   READY   STATUS    RESTARTS   AGE\n[...]\nneuron-device-plugin-daemonset- 6djhp 1/1     Running   0          5m\nneuron-device-plugin-daemonset- hwjsj 1/1     Running   0          5m (Optional) Deploy a TensorFlow\n                Serving application image A trained model must be compiled to an Inferentia target before it can be deployed on\n            Inferentia instances. To continue, you will need a Neuron optimized TensorFlow model saved in Amazon S3. If you don't already have\n            a SavedModel, please follow the tutorial for creating a Neuron\n                compatible ResNet50 model and upload the resulting SavedModel to S3.\n            ResNet-50 is a popular machine learning model used for image recognition tasks. For more\n            information about compiling Neuron models, see The AWS Inferentia Chip With\n                DLAMI in the AWS Deep Learning AMIs Developer Guide. The sample deployment manifest manages a pre-built inference serving container for\n            TensorFlow provided by AWS Deep Learning Containers. Inside the container is the AWS\n            Neuron Runtime and the TensorFlow Serving application. A complete list of pre-built Deep\n            Learning Containers optimized for Neuron is maintained on GitHub under Available Images . At start-up, the DLC will fetch your model from Amazon S3,\n            launch Neuron TensorFlow Serving with the saved model, and wait for prediction\n            requests. The number of Neuron devices allocated to your serving application can be adjusted by\n            changing the aws.amazon.com/neuron resource in the deployment yaml. Please\n            note that communication between TensorFlow Serving and the Neuron runtime happens over\n            GRPC, which requires passing the IPC_LOCK capability to the\n            container. Add the AmazonS3ReadOnlyAccess IAM policy to the node instance\n                    role that was created in step 1 of Create a cluster . This is necessary so that the\n                    sample application can load a trained model from Amazon S3. aws iam attach-role-policy \\\n    --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess \\\n    --role-name eksctl- inferentia - nodegroup-ng-in -NodeInstanceRole- FI7HIYS3BS09 Create a file named rn50_deployment.yaml with the following\n                    contents. Update the region-code and model path to match your desired settings.\n                    The model name is for identification purposes when a client makes a request to\n                    the TensorFlow server. This example uses a model name to match a sample ResNet50\n                    client script that will be used in a later step for sending prediction requests. aws ecr list-images --repository-name neuron-rtd --registry-id 790709498068 --region us-west-2 kind: Deployment apiVersion: apps/v1 metadata: name: eks-neuron-test labels: app: eks-neuron-test role: master spec: replicas: 2 selector: matchLabels: app: eks-neuron-test role: master template: metadata: labels: app: eks-neuron-test role: master spec: containers: - name: eks-neuron-test image: 763104351884. dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04 command: - /usr/local/bin/entrypoint.sh args: - --port=8500 - --rest_api_port=9000 - --model_name=resnet50_neuron - --model_base_path=s3:// your-bucket-of-models /resnet50_neuron/ ports: - containerPort: 8500 - containerPort: 9000 imagePullPolicy: IfNotPresent env: - name: AWS_REGION value: \"us-east-1\" - name: S3_USE_HTTPS value: \"1\" - name: S3_VERIFY_SSL value: \"0\" - name: S3_ENDPOINT value: s3.us-east-1.amazonaws.com - name: AWS_LOG_LEVEL value: \"3\" resources: limits: cpu: 4 memory: 4Gi aws.amazon.com/neuron: 1 requests: cpu: \"1\" memory: 1Gi securityContext: capabilities: add: - IPC_LOCK Deploy the model. kubectl apply -f rn50_deployment.yaml Create a file named rn50_service.yaml with the following\n                    contents. The HTTP and gRPC ports are opened for accepting prediction\n                    requests. kind: Service apiVersion: v1 metadata: name: eks-neuron-test labels: app: eks-neuron-test spec: type: ClusterIP ports: - name: http-tf-serving port: 8500 targetPort: 8500 - name: grpc-tf-serving port: 9000 targetPort: 9000 selector: app: eks-neuron-test role: master Create a Kubernetes service for your TensorFlow model Serving\n                    application. kubectl apply -f rn50_service.yaml (Optional) Make\n                predictions against your TensorFlow Serving service To test locally, forward the gRPC port to the eks-neuron-test service. kubectl port-forward service/eks-neuron-test 8500:8500 & Create a Python script called tensorflow-model-server-infer.py with the following content. This script runs inference via gRPC, which is\n                    service framework. import numpy as np import grpc import tensorflow as tf from tensorflow.keras.preprocessing import image from tensorflow.keras.applications.resnet50 import preprocess_input from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2_grpc from tensorflow.keras.applications.resnet50 import decode_predictions if __name__ == '__main__' :\n       channel = grpc.insecure_channel( 'localhost:8500' )\n       stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n       img_file = tf.keras.utils.get_file( \"./kitten_small.jpg\" , \"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg\" )\n       img = image.load_img(img_file, target_size=( 224 , 224 ))\n       img_array = preprocess_input(image.img_to_array(img)[ None , ...])\n       request = predict_pb2.PredictRequest()\n       request.model_spec.name = 'resnet50_inf1' request.inputs[ 'input' ].CopyFrom(\n           tf.make_tensor_proto(img_array, shape=img_array.shape))\n       result = stub.Predict(request)\n       prediction = tf.make_ndarray(result.outputs[ 'output' ]) print (decode_predictions(prediction)) Run the script to submit predictions to your service. python3 tensorflow-model-server-infer.py An example output is as follows. [[(u 'n02123045' , u 'tabby' , 0.68817204), (u 'n02127052' , u 'lynx' , 0.12701613), (u 'n02123159' , u 'tiger_cat' , 0.08736559), (u 'n02124075' , u 'Egyptian_cat' , 0.063844085), (u 'n02128757' , u 'snow_leopard' , 0.009240591)]] Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Machine learning training Cluster management"}, {"title": "Amazon Q Documentation", "url": "https://docs.aws.amazon.com/amazonq/", "content": "No main content found."}, {"title": "What is Amazon Q Business? - Amazon Q Business", "url": "https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/quick-create-app.html", "content": "What is Amazon Q Business? PDF RSS Powered by Amazon Bedrock: AWS implements automated\n              abuse detection . Because Amazon Q is built on Amazon Bedrock, users can take full\n            advantage of the controls implemented in Amazon Bedrock to enforce safety, security, and the\n            responsible use of artificial intelligence (AI). Amazon Q Business is a fully managed, generative-AI powered assistant that you can\n    configure to answer questions, provide summaries, generate content, and complete tasks based on\n    your enterprise data. It allows end users to receive immediate, permissions-aware responses from\n    enterprise data sources with citations, for use cases such as IT, HR, and benefits help\n    desks. Amazon Q Business also helps streamline tasks and accelerate problem solving. You can\n    use Amazon Q Business to create and share task automation applications, or perform routine\n    actions like submitting time-off requests and sending meeting invites. Amazon Q Business integrates with services like Amazon Kendra and other\n      supported data sources such as Amazon S3 , Microsoft SharePoint , and Salesforce . Topics Benefits of Amazon Q Business Pricing and availability Accessing Amazon Q Business Related services Are you a first-time Amazon Q Business user? Benefits of Amazon Q Business Some of the benefits of Amazon Q Business include: Accurate and comprehensive answers Amazon Q Business generates comprehensive responses to natural language\n            queries from users by analyzing information across all enterprise content that it has\n            access to. It can avoid incorrect statements by confining its generated responses to\n            existing enterprise data, and provides citations to the sources that it used to generate\n            its response. Simple to deploy and manage Amazon Q Business takes care of the complex task of developing and managing\n            machine learning infrastructure and models so that you can build your chat solution\n            quickly. Amazon Q Business connects to your data and ingests it for processing\n            using its pre-built connectors, document retrievers, document upload\n            capabilities. Configurable and customizable Amazon Q Business provides you with the flexibility of choosing what sources\n            should be used to respond to user queries. You can control whether the responses should\n            only use your enterprise data, or use both enterprise data and model knowledge. Data and application security Amazon Q Business supports access control for your data so that the right\n            users can access the right content. Its responses to questions are based on the content\n            that your end user has permissions to access. You can use AWS IAM Identity Center or AWS Identity and Access Management to\n            manage end user access for Amazon Q Business. Broad connectivity Amazon Q Business offers out-of-the-box connections to multiple supported data sources. Additionally, you can connect Amazon Q to any\n            third-party application using plugins to perform actions and query application\n            data. Pricing and availability Amazon Q Business charges you both for user subscriptions to applications, and for\n      index capacity. For information about what's included in the tiers of user subscriptions and\n      index capacity, see Subscription and index pricing . For pricing information, including examples of charges for index capacity, subscribing and\n      unsubscribing users to Amazon Q Business tiers, upgrading and downgrading Amazon Q Business tiers, and more, see Amazon Q Business Pricing . For a list of regions where Amazon Q Business is currently available, see Supported\n        regions . Accessing Amazon Q Business You can access Amazon Q Business in the following ways in the AWS Regions that\n      it's available in: AWS Management Console You can use the AWS Management Console\u2014a browser-based interface to interact with\n            AWS services\u2014to access the Amazon Q Business console and resources. You\n            can perform most Amazon Q Business tasks using the Amazon Q Business\n            console. Amazon Q Business API To access Amazon Q Business programmatically, you can use the Amazon Q API. For more information, see the Amazon Q Business API\n              Reference . AWS Command Line Interface The AWS Command Line Interface (AWS CLI) is an open source tool. You can use the AWS CLI to interact with\n            AWS services using commands in your command line shell. If you want to build\n            task-based scripts, using the command line can be faster and more convenient than using\n            the console. SDKs AWS SDKs provide language APIs for AWS services to use programmatically. Related services The following are some of the other AWS services that Amazon Q Business integrates\n      with: Amazon Kendra Amazon Kendra is an intelligent search service that uses natural language\n            processing and machine learning algorithms to return specific answers from your data for\n            end user queries. If you're already an Amazon Kendra user, you can use Amazon Kendra as a data retriever for your Amazon Q Business web application. Amazon S3 Amazon S3 is an object storage service. If you're an Amazon S3 user,\n            you can use Amazon S3 as a data source for your Amazon Q Business\n            application. Are you a first-time Amazon Q Business user? If you're a first-time user of Amazon Q Business, we recommend that you read the\n      following sections in order: How it\n            works Introduces Amazon Q Business components and describes how they work to create\n            your Retrieval Augmented Generation (RAG) solution. Key\n            concepts Explains key concepts and important Amazon Q Business terminology. Setting\n            up Explains key concepts and important Amazon Q Business terminology and outlines\n            how to set up Amazon Q Business so that you can begin creating your Amazon Q Business application and web experience. Creating a\n            sample application Explains how to create the Amazon Q Business application integrated with\n            IAM Identity Center. Connecting Amazon Q Business data source\n                connectors Configuration information for specific connectors to use with your Amazon Q Business web experience. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Getting started"}, {"title": "What is Amazon Bedrock Studio? - Amazon Bedrock Studio", "url": "https://docs.aws.amazon.com/bedrock/latest/studio-ug/build-application.html", "content": "Amazon Bedrock Studio is in preview release and is subject to change. Amazon Bedrock Studio is in preview release and is subject to change. What is Amazon Bedrock Studio? PDF RSS Amazon Bedrock Studio is a web app that lets you easily protoype apps that use Amazon Bedrock models and\n  features, without having to set up and use a developer environment. For example, you can use\n  Amazon Bedrock to try a prompt with an Anthropic Claude model without having to write any code.\n  Later, you can use Bedrock Studio to create a prototype app that uses an Amazon Bedrock model and features, such as a\n  Knowledge Base or a Guardrail, again without having to write any code. To use Amazon Bedrock Studio, you must be a member of a workspace. Your organization will provide you with login details.\n  If you don't have login details, contact your administrator. Note If you are administrator and need information about managing an Amazon Bedrock Studio workspace, see Manage Amazon Bedrock Studio in the Amazon Bedrock user\n  guide. Workspaces A workspace in Amazon Bedrock Studio is where you experiment with Amazon Bedrock\n   models and where you can build Amazon Bedrock enabled apps. As an Amazon Bedrock Studio user, you can use a\n   workspace in two types of user mode, Explore or Build . Explore mode Explore mode provides a playground that\n   lets you easily try a model by sending prompts to the model and viewing the responses.\n  For more information, see Explore Amazon Bedrock Studio . Build mode Build mode is where you can create apps that use Amazon Bedrock models. You can create two types\n    of apps, a chat app and a Prompt Flows app . A chat app allows users to communicate with an Amazon Bedrock model\n    through a conversational interface, typically by sending text messages and receiving responses.\n    A Prompt Flows app allows you to link prompts, supported\n    foundational models (FMS), and other units of work, such as a Knowledge Base, together and\n    create generative AI workflows for end-to-end solutions. Apps that you create with Bedrock Studio can integrate the following Amazon Bedrock features. Data sources \u2014 Enrich apps by including context that is received from querying a Knowledge Base or a\n      document. Guardrails \u2014\n      Lets you implement safeguards for your Bedrock Studio app based on your use cases and responsible\n      AI policies. Functions \u2014 Lets\n      a model call a function to access a specific capability when handling a prompt. Prompt Management \u2014 Reusable prompts that you can use in a Prompt Flows app. Within Build mode, you use a project to organize the apps, prompts,\n    and components that you use for a solution. A component is an Amazon Bedrock Knowledge Base,\n    Guardrail, or Function. When you open a project, you can access the App\n     builder and Project details by opening the side navigation menu. The App builder is one way you can access the chat app or Prompt Flows app that you are\n    currently working on. Project details is where you create and manage the apps, components, and\n    prompts that the apps in your project use. For more information, see Organize your work with projects in Amazon Bedrock Studio . If you work on a team, you can collaborate by sharing a project with other team members.\n    For more information, see Share an Amazon Bedrock Studio project . Are you a first-time Amazon Bedrock Studio user? If you're a first-time user of Bedrock Studio, we recommend that you read the following\n   sections in order: Explore Amazon Bedrock Studio \u2013 In this section, you access your\n     Amazon Bedrock Studio workspace for the first time and use Explore mode to experiment with an Amazon Bedrock model. Build a chat app with Amazon Bedrock Studio \u2013 In this section,\n     you use Build mode to create a project in your workspace and create a simple Amazon Bedrock Studio app. Build a Prompt Flows app with Amazon Bedrock Studio \u2013 In this section,\n     you learn how to create a Prompt Flows app. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Explore Amazon Bedrock Studio"}, {"title": "AWS General Reference - AWS General Reference", "url": "https://docs.aws.amazon.com/general/latest/gr/docconventions.html", "content": "AWS General Reference PDF The AWS General Reference provides AWS service endpoint and quota information for Amazon Web Services. Additionally, you can find links to other common topics. Contents AWS security credentials AWS IP address ranges AWS APIs AWS services endpoints and quotas AWS Glossary AWS security credentials When you interact with AWS, you specify your AWS security\n      credentials to verify who you are and whether you have permission to access the\n      resources that you are requesting. AWS uses the security credentials to authenticate and\n      authorize your requests. For more information, see the following resources: AWS security credentials in the IAM User Guide AWS\n        security audit guidelines in the IAM User Guide AWS IP address ranges AWS publishes its current IP address ranges in JSON format. You can download\n      a .json file to view current ranges. The IP address ranges that you bring to AWS through bring your own IP addresses (BYOIP)\n      are not included in the .json file. For more information, see the following resources: AWS IP address ranges in the Amazon VPC User Guide AWS services that support IPv6 in the Amazon VPC User Guide AWS APIs The following pages provide information that is useful when using an AWS API: Retry behavior in the AWS SDKs and Tools Reference Guide Signing AWS API requests in the IAM User Guide AWS services endpoints and quotas You can learn about the endpoints and service quotas in the following pages: AWS service endpoints AWS service quotas Service endpoints and quotas Specifying which AWS Regions your account can use in the AWS Account Management Guide AWS Glossary For the latest AWS terminology, see the AWS Glossary . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions AWS service endpoints"}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=a0d18259-7974-4e8f-8382-0a4be53f4374&topic_url=https://docs.aws.amazon.com/en_us/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html", "content": "No main content found."}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=a0d18259-7974-4e8f-8382-0a4be53f4374&topic_url=https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Document history - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/document-revisions.html", "content": "Document history PDF RSS The following table describes the important changes to this decision guide. For notifications\n  about updates to this guide, you can subscribe to an RSS feed. Change Description Date Guide updated Added AWS Copilot, AWS Batch, and AWS Outposts. Changed capacity, orchestration, and\n          provisioning to compute capacity, orchestration, and vertical solutions. Numerous\n          editorial changes throughout. April 5, 2024 Initial publication Guide first published. April 26, 2023 Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Decision guide"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/pdfs/decision-guides/latest/containers-on-aws-how-to-choose/containers-on-aws-how-to-choose.pdf#choosing-aws-container-service", "content": "No main content found."}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs#intro", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs#understand", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs#consider", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs#choose", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs#use", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Choosing an AWS container service - Choosing an AWS container service", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html?icmpid=docs_homepage_featuredsvcs#explore", "content": "Choosing an AWS container service PDF RSS Taking the first step Purpose Determine which AWS container service is the best fit for your\n                            organization. Last updated April 5, 2024 Covered services Amazon EC2 Amazon ECR Amazon ECS Amazon EKS Amazon Lightsail AWS App Runner AWS Batch AWS Copilot AWS Fargate AWS Lambda AWS Outposts Red Hat OpenShift Service on AWS\n                                        (ROSA) Introduction Containers are a key component of modern application development. They are the\n            standard for organizing compute resources, and managing the content of your application\n            deployments. Containers provide a discrete reproducible compute environment for building software\n            to deploy in the cloud. They also simplify packaging and dependency management. You can\n            use them for everything from orchestrating web applications or very large multi-cluster\n            estates to testing your work and doing a proof of concept on your laptop. This decision guide helps you get started and choose the right AWS container service\n            for your modern application development. This 3\u00bd-minute excerpt is from an 11-minute presentation at re:Invent 2023 by\n                    Umar Saeed, an AWS senior manager and solutions architect. He provides a quick\n                    overview of AWS container choices. Understand Containers offer a number of advantages for packaging, deploying, and running\n    applications: Portability: Benefit from a consistent runtime\n        environment that can run on any platform that supports the container runtime. Scalability: Scale applications up or down, based on\n        demand, with lightweight and easily replicated containers. Consistency: Ensure that the application runs the same\n        way in all environments with a consistent runtime environment. Efficiency: Use fewer resources than traditional\n        virtual machines with lightweight containers. Isolation: Improve security and reliability with\n        containers' process-level isolation, with which applications running in separate containers\n        cannot interfere with each other, improving security and reliability. Agility: Reduce the time that it takes to bring new\n        features or applications to market by quickly packaging and deploying applications. You can think about the universe of AWS container services in three distinct\n    layers: The Compute capacity layer is where your containers\n        actually run. This layer consists of: Amazon Elastic Compute Cloud (Amazon EC2) instances: These instances\n            provide the underlying compute capacity for running containers. You can choose from a\n            wide range of instance types and sizes to match your application requirements. EC2\n            instances can be used as the compute layer for both Amazon ECS and Amazon EKS. AWS Fargate: Fargate is a serverless compute\n            engine for containers with which you can run containers without managing the underlying\n            infrastructure. It removes the need to provision and manage EC2 instances. You can use\n            Fargate with both Amazon ECS and Amazon EKS. AWS Outposts: AWS Outposts is a fully managed service that\n            extends AWS infrastructure and services to your on-premises or hybrid environment.\n            With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data\n            center. The orchestration layer schedules and scales your\n        environment. This layer includes: Amazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container\n            orchestration service that simplifies the deployment, management, and scaling of\n            containerized applications. It supports Docker containers. You can use Amazon ECS to define\n            tasks and services, handle service discovery, and manage the lifecycle of\n            containers. Amazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes\n            service with which you can deploy, manage, and scale containerized applications using\n            Kubernetes. It provides a highly available and secure Kubernetes control plane. Red Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed\n            service with which you can deploy and run Red Hat OpenShift clusters on AWS\n            infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends\n            the capabilities of Kubernetes with additional features and tools for building,\n            deploying, and managing containerized applications. The Vertical solutions layer is a set of vertical\n        integration services that provide higher-level and bundled services that simplify the\n        process of deploying and managing applications. The AWS services in this layer are: AWS App Runner: AWS App Runner is a fully managed service\n            designed to simplify the deployment and use of containerized web applications and APIs.\n            You provide your container image, and App Runner automatically builds, deploys, and scales\n            your application. It handles the provisioning of the underlying compute resources, load\n            balancing, and automatic scaling based on incoming traffic. Amazon Lightsail: Amazon Lightsail is a cloud\n            platform that offers pre-configured virtual private servers (instances) and other\n            resources for running applications. It provides pre-defined configurations for quickly\n            provisioning compute instances, databases, storage, and networking resources.\n            Lightsail supports running containerized applications by provisioning instances with\n            Docker pre-installed, aimed at easing the deployment and management of your\n            containers. AWS Batch: AWS Batch is a fully managed service\n            with which you can run batch computing workloads on AWS. It dynamically provisions the\n            optimal compute resources based on the volume and specific resource requirements of the\n            batch jobs that you submit. It automatically handles job scheduling, resource\n            provisioning, and scaling based on the workload requirements. Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker\n            container registry with which you can store, manage, and deploy Docker container images.\n            It is designed to provide secure and scalable storage for your container images and\n            simplify provisioning containers with the desired images. Note AWS provides a variety of ways to deploy and run containers. One of the first\n      considerations is your preference for either a serverless operational model or a Kubernetes\n      operation model. In practice, most customers use both to varying degrees. The choice of operating model is explored in-depth in the Choosing a\n        modern application strategy decision guide , which is a useful resource for anyone\n      who wants to explore this question further. In addition, the Containers and Serverless Recommendation\n        Guide takes you through the choices to make when choosing your operating\n      model. Consider It's important to choose a container service that aligns to your application requirements\n    and operational preferences. The following section outlines some of the key criteria to consider\n    when choosing a container service, as well as supporting tools and services. Managed service and operation overhead Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Workload characteristics Understanding your workload patterns can help you make architecture choices. Workload\n          patterns can include web applications, API-based microservices, event-driven applications,\n          streaming and messaging, data pipelines, IT automations, and more. Some workloads perform\n          better or are more cost effective in one compute environment versus another type. Application portability Many customers want to ensure that their applications can run in\u2014and be migrated or\n          moved to\u2014a different environment. It's important for them to be able to preserve choice,\n          or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with\n          which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute\n          services rather than others. Organization size and skills The skills of your organization are a major factor when deciding which container\n          services you use. The approach you take can require some investment in DevOps and Site\n          Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy\n          applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some\n          organizations have skills and resources to run and manage a Kubernetes implementation,\n          because they invest in strong SRE teams to manage Kubernetes clusters and find value in\n          the associated skill portability. These teams handle frequent cluster upgrades. For\n          example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited\n          IT team made up of people fulfilling multiple roles, while larger enterprises may support\n          hundreds of workloads in production at once. Ease of deployment Different AWS container services meet unique needs in terms of deployment complexity.\n          Here's how each service is optimized for its own role: AWS App Runner offers the most straightforward path for\n              you to deploy your application on the internet without managing or customizing the\n              underlying infrastructure. Amazon ECS is a good choice if you need more control\n              over the network and security configurations without sacrificing scale or\n              features. Amazon EKS provides flexibility and control over\n              application deployment and orchestration provided by Kubernetes technology. anchor anchor anchor anchor anchor Managed service and operation overhead Workload characteristics Application portability Organization size and skills Ease of deployment Building with containers on AWS uses services with higher levels of abstraction to\n          shift the operational overhead of maintaining infrastructure to AWS. Organizations may\n          choose the cloud to reduce operational cost by using standardized managed services with\n          higher levels of abstraction so that developers and operators can focus on their unique\n          activities that add value, instead of on undifferentiated tasks. Choose Now that you know the criteria by which you are evaluating your container options, you are\n    ready to choose which AWS container services might be a good fit for your organizational\n    requirements. The following table highlights which services are optimized for which circumstances. Use the\n    table to help determine which container services and tools are. Containers category When would you use it? Services Capacity Use when you want to run your containers on self-managed AWS virtual machines or\n            AWS managed compute. AWS Fargate Amazon EC2 AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experience with containers or\n            infrastructure. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibility in where you run your container-based\n            applications. Amazon ECS Anywhere Amazon EKS Anywhere Use You should now have a clear understanding of each AWS container service (and the\n    supporting AWS tools and services) and which one might be the best fit for your organization\n    and use case. To explore how to use and learn more about each of the available AWS container services,\n    we have provided a pathway to explore how each of the services work. The following section\n    provides links to in-depth documentation, hands-on tutorials, and resources to get you\n    started. Capacity Amazon EC2 What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial AWS Fargate Getting started with\n                      AWS Fargate This guide explains the basics of\n                      AWS Fargate, a technology that you can use with Amazon ECS to run containers\n                      without having to manage servers or clusters of Amazon EC2\n                        instances. Explore the guide Getting started with the console using Linux\n                    containers on AWS Fargate Get started with Amazon ECS on\n                      AWS Fargate by using the Fargate launch type for your tasks in the Regions\n                      where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using\n                        the AWS CLI Set up a cluster, register a task\n                      definition, run a Linux task, and perform other common scenarios in Amazon ECS with\n                      the AWS CLI. Explore the guide AWS Outposts Getting started with AWS Outposts Access the complete set of AWS Outposts technical documentation. Explore the\n                      guides What is AWS Outposts? Get an introduction to this fully managed service that extends AWS\n                      infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing anchor anchor anchor Amazon EC2 AWS Fargate AWS Outposts What is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to\n                      the service, but also covers how to get started using it and then provides\n                      in-depth descriptions of key features and how to use them. Explore the\n                        guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify\n                    determines the hardware of the host computer used for your instance. Each\n                    instance type offers different compute, memory, and storage capabilities, and is\n                    grouped in an instance family based on these capabilities. This guide walks you\n                    through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tolerant workload using Amazon EC2 Auto Scaling\n                      with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Orchestration Amazon ECS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Amazon EKS Getting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can\n                      use to run Kubernetes on AWS without needing to install, operate, and\n                      maintain your own Kubernetes control plane or nodes. Explore\n                        the guide Amazon EKS deployment Explore\n                      Amazon EKS deployment options on AWS and learn how to use it to manage a general\n                      containerized application. Explore the guide Amazon EKS Quick Start Reference\n                      Deployment Using a Quick Start reference deployment\n                      guide, get step-by-step instructions for deploying Amazon EKS\n                        clusters. Explore the\n                        guide Amazon EKS workshop Explore practical\n                    exercises to learn about Amazon EKS. Visit the workshop Red Hat OpenShift Service on AWS What is \n                    Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with\n                      Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS\n                      (ROSA). Explore the\n                      guide Why would you use\n                      ROSA? Watch a video to learn when to use Red Hat\n                      OpenShift over standard Kubernetes and explore ROSA in\n                        depth. Watch the video anchor anchor anchor Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Getting started with\n                      Amazon ECS Get an introduction to the tools available to\n                      access Amazon ECS and introductory step-by-step procedures to run\n                      containers. Explore the guide Tutorials for Amazon ECS Learn how\n                      to perform common tasks\u2014including the creation of clusters and VPCs\u2014when using\n                      Amazon ECS. Get started with the tutorials Amazon ECS Workshop Use this\n                      workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container\n                      workflows. Explore the\n                        workshop Deploy Docker containers on\n                      Amazon ECS Learn how to run a Docker-enabled sample\n                      application on an Amazon ECS cluster behind a load balancer, test the sample\n                      application, and delete your resources to avoid charges. Explore the guide Vertical solutions AWS App Runner What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial AWS Lambda What is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute\n                      infrastructure and perform all of the administration of the compute resources,\n                      including server and operating system maintenance, capacity provisioning and\n                      automatic scaling, and logging. Explore the\n                        guide AWS Lambda documentation Work\n                      with AWS Lambda documentation to understand how you can use this service to run\n                      code without provisioning or managing servers and only pay for the compute\n                      time that you consume. Explore the guides Working with Lambda container images\n                        locally Learn how you can use a deployment package to\n                      deploy your function code to Lambda. Lambda supports two types of deployment\n                      packages: container images and .zip file\n                        archives. Explore the guide Amazon Lightsail What is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you\n                      can benefit from it. This guide also includes step-by-step guidance to help\n                      you get started using Lightsail and then configure it to meet your\n                      needs. Explore the guide Creating Lightsail container\n                          service images Learn how to create a\n                      container image on your local machine using a Dockerfile. You can then push it\n                      to your Lightsail container service to deploy it. Explore the guide Amazon Lightsail resource\n                      center Explore Lightsail tutorials, videos, and links\n                      to core concept documentation. Visit the resource center AWS Batch What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the\n                      AWS Cloud. Explore\n                        the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when\n                      using AWS Batch. Explore the guide AWS Batch workshops center Use\n                      these workshops, organized in a progressive manner from beginner to advanced,\n                      to explore and learn AWS Batch. Explore the workshops anchor anchor anchor anchor AWS App Runner AWS Lambda Amazon Lightsail AWS Batch What is AWS App Runner? Learn when to use this service to deploy from source code or a container\n                      image directly to a scalable and secure web application in the\n                      AWS Cloud. Explore the guide Getting started with\n                    AWS App Runner Use this tutorial to configure the source code\n                      and deployment, the service build, and the service runtime to deploy your\n                      application to AWS App Runner. Use the\n                        tutorial Deploy a web app using\n                      AWS App Runner Follow these step-by-step instructions to\n                      deploy a containerized web application using AWS App Runner. Use\n                        the tutorial Tools and services with container support AWS Copilot Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video Amazon ECR Amazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully\n                      managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with\u2014and using\u2014Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi-Region\n                        Architectures Explore key considerations for Amazon ECR\n                      architectures that span across AWS accounts and AWS Regions, and\n                      architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map AWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service\n                      discovery) in the AWS CLI Reference to get the most from this\n                        service. Explore\n                        the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend\n                      services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to\n                      frequently asked questions about AWS Cloud Map. Explore the FAQs anchor anchor anchor AWS Copilot Amazon ECR AWS Cloud Map Getting started with Amazon ECS using AWS\n                        Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS\n                      application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build,\n                      release, and operate production-ready containerized applications on AWS App Runner\n                      and Amazon ECS on AWS Fargate. Explore the\n                        documentation Introduction to Amazon ECS using AWS Copilot\n                        CLI Learn how to deploy your application to Amazon ECS\n                      using AWS Copilot. Watch the\n                      video On-premises Amazon ECS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere Amazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for\n                      Amazon EKS Anywhere. Read the\n                        documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere\n                        pricing. Explore the pricing guide Amazon EKS Anywhere FAQs Get\n                      answers to frequently asked questions about Amazon EKS Anywhere. Explore the\n                      FAQs anchor anchor Amazon ECS Anywhere Amazon EKS Anywhere What is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance ,   such as an on-premises server\n                      or virtual machine (VM), to your Amazon ECS cluster. Explore the guide Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is\n                      based on a model in which you are charged based on the amount of time the\n                      instances you have registered to an Amazon ECS cluster are connected to the ECS\n                      control plane, rounded up to the nearest second. Explore the pricing\n                        guide Amazon ECS Anywhere FAQs Get answers to\n                      frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Explore For your role Developers Solution Architects Professional development Startups Decision\n                  makers For an introduction Docker Kubernetes Breaking a monolith into microservices For a video Containers from the\n                  couch How to containerize\n                  anything! Building a container CI/CD\n                  pipeline Building a container app with\n                  AWS CDK Architecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for\n              containers. Explore solutions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/decision-guides/latest/containers-on-aws-how-to-choose/containers-on-aws-how-to-choose.rss", "content": "No main content found."}, {"title": "Amazon Elastic Compute Cloud Documentation", "url": "https://docs.aws.amazon.com/ec2/index.html", "content": "No main content found."}, {"title": "What is Amazon Elastic Container Registry? - Amazon ECR", "url": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html", "content": "What is Amazon Elastic Container Registry? PDF RSS Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure,\n        scalable, and reliable. Amazon ECR supports private repositories with resource-based permissions\n        using AWS IAM. This is so that specified users or Amazon EC2 instances can access your\n        container repositories and images. You can use your preferred CLI to push, pull, and manage\n        Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts. Note Amazon ECR supports public container image repositories as well. For more information, see What is\n                Amazon ECR Public in the Amazon ECR Public User Guide . The AWS container services team maintains a public roadmap on GitHub. It contains\n        information about what the teams are working on and allows all AWS customers the ability\n        to give direct feedback. For more information, see AWS Containers Roadmap . Features of Amazon ECR Amazon ECR provides the following features: Lifecycle policies help with managing the lifecycle of the images in your\n                    repositories. You define rules that result in the cleaning up of unused images.\n                    You can test rules before applying them to your repository. For more\n                    information, see Automate the cleanup of images by using lifecycle\n            policies in Amazon ECR . Image scanning helps in identifying software vulnerabilities in your container\n                    images. Each repository can be configured to scan on push .\n                    This ensures that each new image pushed to the repository is scanned. You can\n                    then retrieve the results of the image scan. For more information, see Scan images for software vulnerabilities in Amazon ECR . Cross-Region and cross-account replication makes it easier for you to have\n                    your images where you need them. This is configured as a registry setting and is\n                    on a per-Region basis. For more information, see Private registry settings in Amazon ECR . Pull through cache rules provide a way to cache repositories in an upstream\n                    registry in your private Amazon ECR registry. Using a pull through cache rule, Amazon ECR\n                    will periodically reach out to the upstream registry to ensure the cached image\n                    in your Amazon ECR private registry is up to date. For more information, see Sync an upstream registry with an Amazon ECR private registry . How to get started with Amazon ECR If you are using Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), note that the setup for those\n            two services is similar to the setup for Amazon ECR because Amazon ECR is an extension of both\n            services. When using the AWS Command Line Interface with Amazon ECR, use a version of the AWS CLI that supports the\n            latest Amazon ECR features. If you don't see support for an Amazon ECR feature in the AWS CLI,\n            upgrade to the latest version of the AWS CLI. For information about installing the latest\n            version of the AWS CLI, see Install or update to the\n                latest version of the AWS CLI in the AWS Command Line Interface User Guide . To learn how to push a container image to a private Amazon ECR repository using the AWS CLI\n            and Docker, see Moving an image through its lifecycle in\n            Amazon ECR . Pricing for Amazon ECR With Amazon ECR, you only pay for the amount of data you store in your repositories and for\n            the data transfer from your image pushes and pulls. For more information, see Amazon ECR pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Concepts and components"}, {"title": "What is Amazon Elastic Container Service? - Amazon Elastic Container Service", "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html", "content": "What is Amazon Elastic Container Service? PDF RSS Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that helps you easily\n        deploy, manage, and scale containerized applications. As a fully managed service, Amazon ECS\n        comes with AWS configuration and operational best practices built-in. It's integrated with\n        both AWS and third-party tools, such as Amazon Elastic Container Registry and Docker. This integration makes it\n        easier for teams to focus on building the applications, not the environment. You can run and\n        scale your container workloads across AWS Regions in the cloud, and on-premises, without\n        the complexity of managing a control plane. Amazon ECS terminology and components There are three layers in Amazon ECS: Capacity - The infrastructure where your containers run Controller - Deploy and manage your applications that run on the\n                    containers Provisioning - The tools that you can use to interface with the scheduler to\n                    deploy and manage your applications and containers The following diagram shows the Amazon ECS layers. Amazon ECS capacity Amazon ECS capacity is the infrastructure where your containers run. The following is\n                an overview of the capacity options: Amazon EC2 instances in the AWS cloud You choose the instance type, the number of instances, and manage the\n                        capacity. Serverless (AWS Fargate (Fargate)) in the AWS cloud Fargate is a serverless, pay-as-you-go compute engine. With Fargate\n                        you don't need to manage servers, handle capacity planning, or isolate\n                        container workloads for security. On-premises virtual machines (VM) or servers Amazon ECS Anywhere provides support for registering an external instance such\n                        as an on-premises server or virtual machine (VM), to your Amazon ECS cluster. The capacity can be located in any of the following AWS resources: Availability Zones Local Zones Wavelength Zones AWS Regions AWS Outposts Amazon ECS controller The Amazon ECS scheduler is the software that manages your applications. Amazon ECS provisioning There are multiple options for provisioning Amazon ECS: AWS Management Console \u2014 Provides a web\n                        interface that you can use to access your Amazon ECS resources. AWS Command Line Interface (AWS CLI) \u2014 Provides\n                        commands for a broad set of AWS services, including Amazon ECS. It's supported\n                        on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provides\n                        language-specific APIs and takes care of many of the connection details.\n                        These include calculating signatures, handling request retries, and error\n                        handling. For more information, see AWS SDKs . Copilot \u2014 Provides an open-source\n                        tool for developers to build, release, and operate production ready\n                        containerized applications on Amazon ECS. For more information, see Copilot on the GitHub\n                        website. AWS CDK \u2014 Provides an open-source\n                        software development framework that you can use to model and provision your\n                        cloud application resources using familiar programming languages. The AWS CDK\n                        provisions your resources in a safe, repeatable manner through\n                        AWS CloudFormation. Application lifecycle The following diagram shows the application lifecycle and how it works with the Amazon ECS\n            components. You must architect your applications so that they can run on containers . A container is a standardized unit of software\n            development that holds everything that your software application requires to run. This\n            includes relevant code, runtime, system tools, and system libraries. Containers are\n            created from a read-only template that's called an image . Images\n            are typically built from a Dockerfile. A Dockerfile is a plaintext file that contains\n            the instructions for building a container. After they're built, these images are stored\n            in a registry such as Amazon ECR where they can be downloaded from. After you create and store your image, you create an Amazon ECS task definition. A task definition is a blueprint for your application. It is a\n            text file in JSON format that describes the parameters and one or more containers that\n            form your application. For example, you can use it to specify the image and parameters\n            for the operating system, which containers to use, which ports to open for your\n            application, and what data volumes to use with the containers in the task. The specific\n            parameters available for your task definition depend on the needs of your specific\n            application. After you define your task definition, you deploy it as either a service or a task on\n            your cluster. A cluster is a logical grouping of tasks or services\n            that runs on the capacity infrastructure that is registered to a cluster. A task is the instantiation of a task definition within a\n            cluster. You can run a standalone task, or you can run a task as part of a service. You\n            can use an Amazon ECS service to run and maintain your\n            desired number of tasks simultaneously in an Amazon ECS cluster. How it works is that, if any\n            of your tasks fail or stop for any reason, the Amazon ECS service scheduler launches another\n            instance based on your task definition. It does this to replace it and thereby maintain\n            your desired number of tasks in the service. The container agent runs on each container instance within an\n            Amazon ECS cluster. The agent sends information about the current running tasks and resource\n            utilization of your containers to Amazon ECS. It starts and stops tasks whenever it receives\n            a request from Amazon ECS. After you deploy the task or service, you can use any of the following tools to\n            monitor your deployment and application: CloudWatch Runtime Monitoring Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Related information"}, {"title": "What is Amazon EKS? - Amazon EKS", "url": "https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html", "content": "Help improve this page Want to contribute to this user guide? Scroll to the bottom of this page\n   and select Edit this page on GitHub . Your contributions will help make our\n   user guide better for everyone. Help improve this page Want to contribute to this user guide? Scroll to the bottom of this page\n   and select Edit this page on GitHub . Your contributions will help make our\n   user guide better for everyone. What is Amazon EKS? PDF RSS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and\n    maintain your own Kubernetes control plane on Amazon Web Services (AWS). Kubernetes is an open-source system\n    that automates the management, scaling, and deployment of containerized applications. Features of Amazon EKS The following are key features of Amazon EKS: Secure networking and authentication Amazon EKS integrates your Kubernetes workloads with AWS networking and security services. It also integrates with AWS Identity and Access Management (IAM) to\n            provide authentication for your Kubernetes\n            clusters. Easy cluster scaling Amazon EKS enables you to\n            scale your Kubernetes clusters up and down easily based on the demand of your workloads. Amazon EKS\n            supports horizontal Pod\n              autoscaling based on CPU or custom metrics, and cluster autoscaling based on the demand of the entire workload. Managed Kubernetes experience You can make changes to your Kubernetes clusters using eksctl , AWS Management Console , AWS Command Line Interface (AWS CLI) , the API , kubectl , and Terraform . High availability Amazon EKS provides high availability for your control plane across multiple Availability Zones. Integration with AWS services Amazon EKS\n            integrates with other AWS services , providing a\n            comprehensive platform for deploying and managing your containerized applications. You can\n            also more easily troubleshoot your Kubernetes workloads with various observability tools. For details about other features of Amazon EKS, see Amazon EKS features . Get started with Amazon EKS To create your first cluster and its associated resources, see Get started with Amazon EKS . In general, getting started\n      with Amazon EKS involves the following steps. Create a cluster \u2013 Start by creating your\n          cluster using eksctl , AWS Management Console, AWS CLI, or one of the AWS SDKs. Choose your approach to compute resources \u2013\n          Decide between AWS Fargate, Karpenter, managed node groups, and\n          self-managed nodes. Setup \u2013 Set up the necessary controllers,\n          drivers, and services. Deploy workloads \u2013 Tailor your Kubernetes workloads\n          to best utilize the resources and capabilities of your chosen node type. Management \u2013 Oversee your workloads, integrating\n          AWS services to streamline operations and enhance workload performance. You can view\n          information about your workloads using the AWS Management Console. The following diagram shows a basic flow of running Amazon EKS in the cloud. To learn about\n      other Kubernetes deployment options, see Deploy Amazon EKS clusters across cloud and on-premises environments . Pricing for Amazon EKS An Amazon EKS cluster consists of a control plane and the Amazon Elastic Compute Cloud (Amazon EC2) or Fargate compute that you run Pods on. For\n      more information about pricing for the control plane, see Amazon EKS pricing . Both Amazon EC2 and Fargate provide: On-Demand Instances Pay for the instances that you use by the second, with no long-term commitments or\n            upfront payments. For more information, see Amazon EC2 On-Demand Pricing and AWS Fargate\n              Pricing . Savings Plans You can reduce your costs by making a commitment to a consistent amount of usage, in\n            USD per hour, for a term of one or three years. For more\n              information, see Pricing with\n                Savings Plans . You can also use a hybrid pricing model. For\n            example, you can use Savings Plans to serve your regular traffic and scale up your\n            cluster nodes with Spot instances to serve the peak demands. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Common use cases"}, {"title": "Deploy and manage containers on Amazon Lightsail - Amazon Lightsail", "url": "https://docs.aws.amazon.com/lightsail/latest/userguide/amazon-lightsail-container-services.html", "content": "Deploy and manage containers on\n      Amazon Lightsail PDF An Amazon Lightsail container service is a highly scalable compute and networking resource on\n    which you can deploy, run, and manage containers. A container is a standard unit of software\n    that packages code and its dependencies together so the application runs quickly and reliably\n    from one computing environment to another. You can think of your Lightsail container service as a computing environment that lets you\n    run containers on AWS infrastructure by using images that you create on your local machine and\n    push to your service, or images from an online repository, like Amazon ECR Public\n    Gallery. You can also run containers locally, on your local machine, by installing software such as\n    Docker. Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Compute Cloud (Amazon EC2) are other resources within the AWS\n    infrastructure on which you can run containers. For more information, see the Amazon ECS\n      Developer Guide . Contents Containers Lightsail\n          container service elements Lightsail container\n              services Container service capacity\n              (scale and power) Pricing Deployments Deployment\n              versions Container image\n              sources Container service\n            ARN Public endpoints\n              and default domains Custom\n              domains and SSL/TLS certificates Container\n              logs Metrics Use Lightsail\n          container services Containers A container is a standard unit of software that packages code and its dependencies\n      together so the application runs quickly and reliably from one computing environment to\n      another. You could run a container on your development environment, deploy it to your\n      pre-production environment, and then deploy it to your production environment. Your containers\n      will run reliably regardless of whether your development environment is your local machine,\n      your pre-production environment is a physical server in a data center, or your production\n      environment is a virtual private server in the cloud. A container image is a lightweight, standalone, executable package of software that\n      includes everything needed to run an application: code, runtime, system tools, system\n      libraries and settings. Container images become containers at runtime. By containerizing the\n      application and its dependencies, you no longer have to worry about whether your software runs\n      correctly on the operating system and infrastructure that you deploy it on \u2013 you can spend\n      more time focusing on the code. For more information about containers, and container images, see What is a Container? in the Docker documentation . Lightsail container service\n        elements The following are the key elements of Lightsail container services that you should\n      understand before getting started. Lightsail container services A container service is the Lightsail compute resource that you can create in any\n        AWS Region in which Lightsail is available. You can create and delete container services\n        at any time. For more information, see Create Lightsail container\n          services and Delete\n          Lightsail container services . Container service capacity (scale and\n          power) You must choose the following capacity parameters when you first create your container\n        service: Scale \u2014 The number of compute nodes that you want\n            your container workload to run in. Your container workload is copied across the compute\n            nodes of your service. You can specify up to 20 compute nodes for a container service.\n            You pick the scale based on the number of nodes you want powering your service for\n            better availability and higher capacity. Traffic to your containers will be\n            load-balanced across all nodes. Power \u2014 The memory and vCPUs of each node in your\n            container service. The powers that you can choose are Nano (Na), Micro (Mi), Small (Sm),\n            Medium (Md), Large (Lg), and Xlarge (Xl), each with a progressively greater amount of\n            memory and vCPUs. If you specify the scale of your container service as more than 1, then your container\n        workload is copied across the multiple compute nodes of your service. For example, if the\n        scale of your service is 3 and the power is Nano, then there are three copies of your\n        container workload running on three compute resources each with 512 MB of RAM and 0.25\n        vCPUs. The incoming traffic is load-balanced between the three resources. The greater the\n        capacity you specify for your container service, the more traffic it is able to\n        handle. You can dynamically increase the power and scale of your container service at any time\n        without any down-time if you find that it's under-provisioned, or decrease it if you find\n        that it's over-provisioned. Lightsail automatically manages the capacity change along with\n        your current deployment. For more information, see Change the capacity of your\n          container service . Pricing The monthly price of your container service is calculated by multiplying the price of\n        its power with the number of its compute nodes (the scale of your service). For example, a\n        service with a medium power, which has a price of $40 USD, and a scale of 3 compute nodes,\n        will cost $120 USD per month. You are charged for your container service whether it's\n        enabled or disabled, and whether it has a deployment or not. You must delete your container\n        service to stop being charged for it. Each container service, regardless of its configured capacity, includes a monthly data\n        transfer quota of 500 GB. The data transfer quota does not change regardless of the power\n        and scale that you choose for your service. Data transfer out to the internet in excess of\n        the quota will result in an overage charge that varies by AWS Region and starts at $0.09\n        USD per GB. Data transfer in from the internet in excess of the quota does not incur an\n        overage charge. For more information, see the Lightsail pricing page . Deployments You can create a deployment in your Lightsail container service. A deployment is a set\n        of specifications for the container workload that you wish to launch on your service. You can specify the following parameters for each container entry in a\n        deployment: The name of your container that will be launched The source container image to use for your container The command to run when launching your container The environment variables to apply to your container The network ports to open on your container The container in the deployment to make publicly accessible through the default\n            domain of the container service Note Only one container in a deployment can be made publicly accessible for each\n              container service. The following health check parameters will apply to the public endpoint of a deployment\n        after it's launched: The directory path on which to perform a health check. Advanced health check settings, such as interval seconds, timeout seconds, success\n            codes, healthy threshold, and unhealthy threshold. Your container service can have one active deployment at a time, and a deployment can\n        have up to 10 container entries. You can create a deployment at the same time as you create\n        your container service, or you can create it after your service is up and running. For more\n        information, see Create and\n          manage container service deployments . Deployment versions Every deployment that you create in your container service is saved as a deployment\n        version. If you modify the parameters of an existing deployment, the containers are\n        re-deployed to your service and the modified deployment results in a new deployment version.\n        The latest 50 deployment versions for each container service are saved. You can use any of\n        the 50 deployment versions to create a new deployment in the same container service. For\n        more information, see Create\n          and manage container service deployments . Container image sources When you create a deployment, you must specify a source container image for each\n        container entry in your deployment. Immediately after you create your deployment, your\n        container service pulls the images from the sources you specify and uses them to create your\n        containers. The images that you specify can originate from the following sources: A public registry , such as Amazon ECR Public Gallery,\n            or some other public container image registry. For more information\n            about Amazon ECR Public, see What Is\n              Amazon Elastic Container Registry Public? in the Amazon ECR Public User\n              Guide . Images pushed from your local machine to your\n            container service. If you create container images on your local machine, you can push\n            them to your container service to use them when creating a deployment. For more\n            information, see Create\n              container service images and Push and manage container\n              images . Lightsail container services support Linux-based container images. Windows-based\n        container images are currently not supported, but you can run Docker, the AWS Command Line Interface (AWS CLI),\n        and the Lightsail Control (lightsailctl) plugin on Windows to build and push your Linux\n        based images to your Lightsail container service. Container service ARN Amazon Resource Names (ARNs) uniquely identify AWS resources. We require an ARN when\n        you need to specify a resource unambiguously across all of AWS, such as in IAM policies,\n        and API calls. To get the ARN for your container service, use the GetContainerServices Lightsail API action, and specify the name of the container service using the serviceName parameter. Your container service ARN will be listed in the\n        results of that action as shown in the following example. For more information, see GetContainerServices in the Amazon Lightsail API\n        Reference . You'll see output similar to the following: { \"containerServices\" : [ { \"containerServiceName\" : \"container-service-1\" , \"arn\" : \"arn:aws:lightsail: :111122223333:ContainerService/a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\" , \"createdAt\" : \"2024-01-01T00:00:00+00:00\" , \"location\" : { \"availabilityZone\" : \"all\" , \"regionName\" : \"us-west-2\" },\n        .....\n} Public endpoints and default\n          domains When you create a deployment, you can specify the container entry in the deployment that\n        will serve as the public endpoint of your container service. The application on the public\n        endpoint container is publicly accessible on the internet through a randomly generated\n        default domain of your container service. The default domain is formatted as https:// <ServiceName> . <RandomGUID> . <AWSRegion> .cs.amazonlightsail.com ,\n        in which <ServiceName> is the name of your container\n        service, <RandomGUID> is a randomly generated globally\n        unique identifier of your container service in the AWS Region for your Lightsail\n        account, and <AWSRegion> is the AWS Region in which the\n        container service was created. The public endpoint of Lightsail container services\n        supports HTTPS only, and it does not support TCP or UDP traffic. Only one container can be\n        the public endpoint for a service. So make sure that choose the container that is hosting\n        the front-end of your application as the public endpoint while rest of the containers are\n        internally accessible. You can use the default domain of your container service, or you can use your own custom\n        domain (your registered domain name). For more information about using custom domains with\n        your container services, see Enable and manage\n          custom domains for your container services . Private domain All container services also have a private domain that is formatted as <ServiceName> .service.local , in which <ServiceName> is the name of your container service. Use\n        the private domain to access your container service from another one of your Lightsail\n        resources in the same AWS Region as your service. The private domain is the only way to\n        access your container service if you don't specify a public endpoint in the deployment of\n        your service. A default domain is generated for your container service even if you don't\n        specify a public endpoint, but it will show a 404 No Such Service error message\n        when you try to browse to it. To access a specific container using the private domain of your container service, you\n        must specify the open port of the container that will accept your connection request. You do\n        this by formatting the domain of your request as <ServiceName> .service.local: <PortNumber> ,\n        in which <ServiceName> is the name of your container\n        service and <PortNumber> is the open port of the container\n        that you wish to connect to. For example, if you create a deployment on your container\n        service named container-service-1 , and you specify a Redis container with port 6379 open, then you should format the domain of your request as container-service-1 .service.local: 6379 . Custom domains and SSL/TLS\n          certificates You can use up to 4 of your custom domains with your container service instead of using\n        the default domain. For example, you can direct traffic for your custom domain, such as example.com , to the container in your deployment that is labeled as the\n        public endpoint. To use your custom domains with your service, you must first request an SSL/TLS\n        certificate for the domains that you want to use. You must then validate the SSL/TLS\n        certificate by adding a set of CNAME records to the DNS of your domains. After the SSL/TLS\n        certificate is validated, you enable custom domains on your container service by attaching\n        the valid SSL/TLS certificate to your service. For more information see Create SSL/TLS\n          certificates for your Lightsail container services , Validate SSL/TLS\n          certificates for your Lightsail container services , and Enable and manage\n          custom domains for your Lightsail container services . Container logs Every container in your container service generates a log that you can access to\n        diagnose the operation of your containers. The logs provide the stdout and stderr streams of processes that run inside the container. For more\n        information, see View container service logs . Metrics Monitor the metrics of your container service to diagnose issues that may be a result of\n        over-utilization. You can also monitor metrics to help you determine if your service is\n        under-provisioned or over-provisioned. For more information, see View container service\n          metrics . Use Lightsail container\n        services These are the general steps to manage your Lightsail container service if you plan to\n      push container images from your local machine to your service, and use them in your\n      deployment: Create your container service in your Lightsail account. For more information, see Create Lightsail\n            container services . Install software on your local machine that you need to create your own container\n          images and push them to your Lightsail container service. For more information, see For\n          more information, see the following guides: Install software to manage\n                container images for your Lightsail container services Create container images\n                for your Lightsail container services Push and manage\n                container images on your Lightsail container services Create a deployment in your container service that configures and launches your\n          containers. For more information, see Create and manage deployments\n            for your Lightsail container services . View previous deployments for your container service. You can create a new deployment\n          using a previous deployment version. For more information, see View and manage\n            deployment versions of your Lightsail container services . View the logs of containers on your container service. For more information, see View the container\n            logs of your Lightsail container services . Create an SSL/TLS certificate for the domains that you want to use with your\n          containers. For more information, see Create SSL/TLS\n            certificates for your Lightsail container services . Validate the SSL/TLS certificate by adding records to the DNS of your domains. For\n          more information, see Validate SSL/TLS\n            certificates for your Lightsail container services . Enable custom domains by attaching a valid SSL/TLS certificate to your container\n          service. For more information, see Enable and manage\n            custom domains for your Lightsail container services . Monitor the utilization metrics of your container service. For more information, see View container\n            service metrics . (Optional) Scale the capacity of your container service vertically, by increasing its\n          power specification, and horizontally, by increasing its scale specification. For more\n          information, see Change the capacity of your Lightsail container services . Delete your container service if you're not using it to avoid incurring monthly\n          charges. For more information, see Delete Lightsail container\n            services . These are the general steps to manage your Lightsail container service if you plan to\n      use container images from a public registry in your\n      deployment: Create your container service in your Lightsail account. For more information, see Create Lightsail\n            container services . If you plan to use container images from a public registry, find container images from\n          a public registry such as the Amazon ECR Public Gallery. For more information about Amazon ECR Public,\n          see What\n            Is Amazon Elastic Container Registry Public? in the Amazon ECR Public User\n            Guide . Create a deployment in your container service that configures and launches your\n          containers. For more information, see Create and manage deployments\n            for your Lightsail container services . View previous deployments for your container service. You can create a new deployment\n          using a previous deployment version. For more information, see View and manage\n            deployment versions of your Lightsail container services . View the logs of containers on your container service. For more information, see View the container\n            logs of your Lightsail container services . Create an SSL/TLS certificate for the domains that you want to use with your\n          containers. For more information, see Create SSL/TLS\n            certificates for your Lightsail container services . Validate the SSL/TLS certificate by adding records to the DNS of your domains. For\n          more information, see Validate SSL/TLS\n            certificates for your Lightsail container services . Enable custom domains by attaching a valid SSL/TLS certificate to your container\n          service. For more information, see Enable and manage\n            custom domains for your Lightsail container services . Monitor the utilization metrics of your container service. For more information, see View container\n            service metrics . (Optional) Scale the capacity of your container service vertically, by increasing its\n          power specification, and horizontally, by increasing its scale specification. For more\n          information, see Change the capacity of your Lightsail container services . Delete your container service if you're not using it to avoid incurring monthly\n          charges. For more information, see Delete Lightsail container\n            services . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Upload files to bucket Create a container"}, {"title": "What is AWS App Runner? - AWS App Runner", "url": "https://docs.aws.amazon.com/apprunner/latest/dg/what-is-apprunner.html", "content": "What is AWS App Runner? PDF AWS App Runner is an AWS service that provides a fast, simple, and cost-effective way to deploy from source code or a container image directly to a scalable\n    and secure web application in the AWS Cloud. You don't need to learn new technologies, decide which compute service to use, or know how to provision and\n    configure AWS resources. App Runner connects directly to your code or image repository. It provides an automatic integration and delivery pipeline with fully managed operations, high\n    performance, scalability, and security. Who is App Runner for? If you're a developer , you can use App Runner to simplify the process of deploying a new version of your code or image\n      repository. For operations teams , App Runner enables automatic deployments each time a commit is pushed to the code repository or a new container image\n      version is pushed to the image repository. Accessing App Runner You can define and configure your App Runner service deployments using any one of the following interfaces: App Runner console \u2013 Provides a web interface for managing your App Runner services. App Runner API \u2013 Provides a RESTful API for performing App Runner actions. For more information, see AWS App Runner API Reference . AWS Command Line Interface (AWS CLI) \u2013 Provides commands for a broad set of AWS services, including Amazon VPC,\n          and is supported on Windows, macOS, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2013 Provides language-specific APIs and takes care of many of the connection details, such as\n          calculating signatures, handling request retries, and error handling. For more information, see AWS\n            SDKs . Pricing for App Runner App Runner provides a cost-effective way to run your application. You only pay for resources that your App Runner service consumes. Your service scales down to\n      fewer compute instances when request traffic is lower. You have control over scalability settings: the lowest and highest number of provisioned\n      instances, and the highest load an instance handles. For more information about App Runner automatic scaling, see Managing App Runner automatic scaling . For pricing information, see AWS App Runner pricing . What's next Learn how to get started with App Runner in the following topics: Setting up for App Runner \u2013 Complete the prerequisite steps for using App Runner. Getting started with App Runner \u2013 Deploy your first application to App Runner. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Setting up"}, {"title": "What is AWS Batch? - AWS Batch", "url": "https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html", "content": "What is AWS Batch? PDF RSS AWS Batch helps you to run batch computing workloads on the AWS Cloud. Batch computing is a common way for\n  developers, scientists, and engineers to access large amounts of compute resources. AWS Batch removes the\n  undifferentiated heavy lifting of configuring and managing the required infrastructure, similar to traditional batch\n  computing software. This service can efficiently provision resources in response to jobs submitted in order to\n  eliminate capacity constraints, reduce compute costs, and deliver results quickly. As a fully managed service, AWS Batch helps you to run batch computing workloads of any scale. AWS Batch\n  automatically provisions compute resources and optimizes the workload distribution based on the quantity and scale of\n  the workloads. With AWS Batch, there's no need to install or manage batch computing software, so you can focus your time\n  on analyzing results and solving problems. Topics Components of AWS Batch The AWS Batch dashboard Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Components of AWS Batch"}, {"title": "What is Amazon Elastic Container Service? - Amazon Elastic Container Service", "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-aws-copilot-cli.html", "content": "What is Amazon Elastic Container Service? PDF RSS Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that helps you easily\n        deploy, manage, and scale containerized applications. As a fully managed service, Amazon ECS\n        comes with AWS configuration and operational best practices built-in. It's integrated with\n        both AWS and third-party tools, such as Amazon Elastic Container Registry and Docker. This integration makes it\n        easier for teams to focus on building the applications, not the environment. You can run and\n        scale your container workloads across AWS Regions in the cloud, and on-premises, without\n        the complexity of managing a control plane. Amazon ECS terminology and components There are three layers in Amazon ECS: Capacity - The infrastructure where your containers run Controller - Deploy and manage your applications that run on the\n                    containers Provisioning - The tools that you can use to interface with the scheduler to\n                    deploy and manage your applications and containers The following diagram shows the Amazon ECS layers. Amazon ECS capacity Amazon ECS capacity is the infrastructure where your containers run. The following is\n                an overview of the capacity options: Amazon EC2 instances in the AWS cloud You choose the instance type, the number of instances, and manage the\n                        capacity. Serverless (AWS Fargate (Fargate)) in the AWS cloud Fargate is a serverless, pay-as-you-go compute engine. With Fargate\n                        you don't need to manage servers, handle capacity planning, or isolate\n                        container workloads for security. On-premises virtual machines (VM) or servers Amazon ECS Anywhere provides support for registering an external instance such\n                        as an on-premises server or virtual machine (VM), to your Amazon ECS cluster. The capacity can be located in any of the following AWS resources: Availability Zones Local Zones Wavelength Zones AWS Regions AWS Outposts Amazon ECS controller The Amazon ECS scheduler is the software that manages your applications. Amazon ECS provisioning There are multiple options for provisioning Amazon ECS: AWS Management Console \u2014 Provides a web\n                        interface that you can use to access your Amazon ECS resources. AWS Command Line Interface (AWS CLI) \u2014 Provides\n                        commands for a broad set of AWS services, including Amazon ECS. It's supported\n                        on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provides\n                        language-specific APIs and takes care of many of the connection details.\n                        These include calculating signatures, handling request retries, and error\n                        handling. For more information, see AWS SDKs . Copilot \u2014 Provides an open-source\n                        tool for developers to build, release, and operate production ready\n                        containerized applications on Amazon ECS. For more information, see Copilot on the GitHub\n                        website. AWS CDK \u2014 Provides an open-source\n                        software development framework that you can use to model and provision your\n                        cloud application resources using familiar programming languages. The AWS CDK\n                        provisions your resources in a safe, repeatable manner through\n                        AWS CloudFormation. Application lifecycle The following diagram shows the application lifecycle and how it works with the Amazon ECS\n            components. You must architect your applications so that they can run on containers . A container is a standardized unit of software\n            development that holds everything that your software application requires to run. This\n            includes relevant code, runtime, system tools, and system libraries. Containers are\n            created from a read-only template that's called an image . Images\n            are typically built from a Dockerfile. A Dockerfile is a plaintext file that contains\n            the instructions for building a container. After they're built, these images are stored\n            in a registry such as Amazon ECR where they can be downloaded from. After you create and store your image, you create an Amazon ECS task definition. A task definition is a blueprint for your application. It is a\n            text file in JSON format that describes the parameters and one or more containers that\n            form your application. For example, you can use it to specify the image and parameters\n            for the operating system, which containers to use, which ports to open for your\n            application, and what data volumes to use with the containers in the task. The specific\n            parameters available for your task definition depend on the needs of your specific\n            application. After you define your task definition, you deploy it as either a service or a task on\n            your cluster. A cluster is a logical grouping of tasks or services\n            that runs on the capacity infrastructure that is registered to a cluster. A task is the instantiation of a task definition within a\n            cluster. You can run a standalone task, or you can run a task as part of a service. You\n            can use an Amazon ECS service to run and maintain your\n            desired number of tasks simultaneously in an Amazon ECS cluster. How it works is that, if any\n            of your tasks fail or stop for any reason, the Amazon ECS service scheduler launches another\n            instance based on your task definition. It does this to replace it and thereby maintain\n            your desired number of tasks in the service. The container agent runs on each container instance within an\n            Amazon ECS cluster. The agent sends information about the current running tasks and resource\n            utilization of your containers to Amazon ECS. It starts and stops tasks whenever it receives\n            a request from Amazon ECS. After you deploy the task or service, you can use any of the following tools to\n            monitor your deployment and application: CloudWatch Runtime Monitoring Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Related information"}, {"title": "AWS Fargate for Amazon ECS - Amazon Elastic Container Service", "url": "https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html", "content": "AWS Fargate for Amazon ECS PDF RSS AWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage\n        servers or clusters of Amazon EC2 instances. With AWS Fargate, you no longer have to\n        provision, configure, or scale clusters of virtual machines to run containers. This removes\n        the need to choose server types, decide when to scale your clusters, or optimize cluster\n        packing. When you run your tasks and services with the Fargate launch type, you\n        package your application in containers, specify the CPU and memory requirements, define\n        networking and IAM policies, and launch the application. Each Fargate task\n        has its own isolation boundary and does not share the underlying kernel, CPU resources,\n        memory resources, or elastic network interface with another task. You configure your task\n        definitions for Fargate by setting the requiresCompatibilities task definition parameter to FARGATE . For more information, see Launch types . Fargate offers platform versions for Amazon Linux 2 and Microsoft Windows 2019 Server\n        Full and Core editions. Unless otherwise specified, the information on this page applies to\n        all Fargate platforms. This topic describes the different components of Fargate tasks and services,\n        and calls out special considerations for using Fargate with Amazon ECS. For information about the Regions that support Linux containers on\n        Fargate, see Linux containers on AWS Fargate . For information about the Regions that support Windows containers on\n        Fargate, see Windows containers on AWS Fargate . Walkthroughs For information about how to get started using the console, see: Learn how to create an Amazon ECS  Linux task for the Fargate launch type Learn how to create an Amazon ECS  \n            Windows task for the Fargate launch type For information about how to get started using the AWS CLI, see: Creating an Amazon ECS Linux task for the Fargate launch type with the AWS CLI Creating an Amazon ECS Windows \n            task for the Fargate launch type with the AWS CLI Capacity providers The following capacity providers are available: Fargate Fargate Spot - Run interruption tolerant Amazon ECS tasks at a discounted rate\n                    compared to the AWS Fargate price. Fargate Spot runs tasks on spare\n                    compute capacity. When AWS needs the capacity back, your tasks will be\n                    interrupted with a two-minute warning. For more information, see Amazon ECS clusters for the Fargate launch\n                type . Task definitions Tasks that use the Fargate launch type don't support all of the Amazon ECS\n            task definition parameters that are available. Some parameters aren't supported at all,\n            and others behave differently for Fargate tasks. For more information, see Task CPU and memory . Platform versions AWS Fargate platform versions are used to refer to a specific runtime environment for\n        Fargate task infrastructure. It is a combination of the kernel and container\n        runtime versions. You select a platform version when you run a task or when you create a\n        service to maintain a number of identical tasks. New revisions of platform versions are released as the runtime environment evolves, for example, if\n        there are kernel or operating system updates, new features, bug fixes, or security updates.\n        A Fargate platform version is updated by making a new platform version revision. Each task\n        runs on one platform version revision during its\n        lifecycle. If you want to use the latest platform version revision, then you must start a new\n        task. A new task that runs on Fargate always runs on the latest revision\n        of a platform version, ensuring that tasks are always started on secure and patched infrastructure. If a security issue is found that affects an existing platform version, AWS creates a\n        new patched revision of the platform version and retires tasks running on the\n        vulnerable revision. In some cases, you may be notified that your tasks on Fargate\n        have been scheduled for retirement. For more information, see Task retirement and maintenance for AWS Fargate on Amazon ECS . For more information see Fargate platform versions for Amazon ECS . Service load balancing Your Amazon ECS service on AWS Fargate can optionally be configured to use\n            Elastic Load Balancing to distribute traffic evenly across the tasks in your service. Amazon ECS services on AWS Fargate support the Application Load Balancer and Network Load Balancer load balancer\n            types. Application Load Balancers are used to route HTTP/HTTPS (or layer 7) traffic. Network Load Balancers are used to\n            route TCP or UDP (or layer 4) traffic. For more information, see Use load balancing to distribute Amazon ECS service traffic . When you create a target group for these services, you must choose ip as\n            the target type, not instance . This is because tasks that use the awsvpc network mode are associated with an elastic network interface,\n            not an Amazon EC2 instance. For more information, see Use load balancing to distribute Amazon ECS service traffic . Using a Network Load Balancer to route UDP traffic to your Amazon ECS on AWS Fargate tasks is\n            only supported when using platform version 1.4 or later. Usage metrics You can use CloudWatch usage metrics to provide visibility into your accounts usage of\n            resources. Use these metrics to visualize your current service usage on CloudWatch graphs and\n            dashboards. AWS Fargate usage metrics correspond to AWS service quotas. You can configure\n            alarms that alert you when your usage approaches a service quota. For more information\n            about AWS Fargate service quotas, see AWS Fargate service quotas . For more information about AWS Fargate usage metrics, see AWS Fargate usage metrics in the Amazon Elastic Container Service User Guide for AWS Fargate . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Best practices Security considerations for when to use the\n            Fargate launch type"}, {"title": "Create a Lambda function using a container image - AWS Lambda", "url": "https://docs.aws.amazon.com/lambda/latest/dg/images-create.html", "content": "Create a Lambda function using a container image PDF RSS Your AWS Lambda function's code consists of scripts or compiled programs and their dependencies. \nYou use a deployment package to deploy your function code to Lambda. Lambda supports two types of deployment packages: \ncontainer images and .zip file archives. There are three ways to build a container image for a Lambda function: Using an AWS base image for Lambda The AWS base images are preloaded with a language runtime, a runtime interface client to manage the interaction between Lambda and your function code, and a runtime interface emulator for local testing. Using an AWS OS-only base image AWS OS-only base images contain an Amazon Linux distribution and the runtime interface emulator . These images are commonly used to create container images for compiled languages, such as Go and Rust , and for a language or language version that Lambda doesn't provide a base image for, such as Node.js 19. You can also use OS-only base images to implement a custom runtime . To make the image compatible with Lambda, you must include a runtime interface client for your language in the image. Using a non-AWS base image You can use an alternative base image from another container registry, such as Alpine Linux or Debian. You can also use a custom image created by your organization. To make the image compatible with Lambda, you must include a runtime interface client for your language in the image. Tip To reduce the time it takes for Lambda container functions to become active, see Use multi-stage builds in the Docker documentation. To build efficient container images, follow the Best practices for writing Dockerfiles . To create a Lambda function from a container image, build your image locally and upload it to an Amazon Elastic Container Registry (Amazon ECR) repository. Then, specify the repository URI when you create the function. The Amazon ECR repository must be in the same AWS Region as the Lambda function. You can create a function using an image in a different AWS account, as long as the image is in the same Region as the Lambda function. For more information, see Amazon ECR cross-account permissions . Note Lambda does not support Amazon ECR FIPS endpoints for container images. If your repository URI includes ecr-fips , you are using a FIPS endpoint. Example: 111122223333.dkr.ecr-fips.us-east-1.amazonaws.com . This page explains the base image types and requirements for creating Lambda-compatible container images. Note You cannot change the deployment package type (.zip or container image) for an existing function. For example, you cannot convert a container image function to use a .zip file archive. You must create a new function. Topics Requirements Using an AWS base image for Lambda Using an AWS OS-only base image Using a non-AWS base image Runtime interface clients Amazon ECR permissions Function lifecycle Requirements Install the AWS Command Line Interface (AWS CLI) version 2 and the Docker CLI . \n      Additionally, note the following requirements: The container image must implement the Using the Lambda runtime API for custom runtimes . The AWS\n          open-source runtime interface clients implement the API. You can\n          add a runtime interface client to your preferred base image to make it compatible with Lambda. The container image must be able to run on a read-only file system. Your function code can access a\n          writable /tmp directory with between 512 MB and 10,240 MB, in 1-MB increments, of storage. The default Lambda user must be able to read all the files required to run your function code. Lambda\n          follows security best practices by defining a default Linux user with least-privileged permissions. This means that you don't need to specify a USER in your Dockerfile. Verify\n          that your application code does not rely on files that other Linux users are restricted from running. Lambda supports only Linux-based container images. Lambda provides multi-architecture base images. However, the image you build for your function must target\n          only one of the architectures. Lambda does not support functions that use multi-architecture container\n          images. Using an AWS base image for Lambda You can use one of the AWS base images for Lambda to build the container image for your\n      function code. The base images are preloaded with a language runtime and other components\n      required to run a container image on Lambda. You add your function code and dependencies to the\n      base image and then package it as a container image. AWS periodically provides updates to the AWS base images for Lambda. If your Dockerfile includes the\n      image name in the FROM property, your Docker client pulls the latest version of the image from the Amazon ECR repository . To\n      use the updated base image, you must rebuild your container image and update the function code . The Node.js 20, Python 3.12, Java 21, .NET 8, Ruby 3.3, and later base images are based on the Amazon Linux 2023 minimal container image . Earlier base images use Amazon Linux 2. AL2023 provides several advantages over Amazon Linux 2, including a smaller deployment footprint and updated versions of libraries such as glibc . AL2023-based images use microdnf (symlinked as dnf ) as the package manager instead of yum , which is the default package manager in Amazon Linux 2. microdnf is a standalone implementation of dnf . For a list of packages that are included in AL2023-based images, refer to the Minimal Container columns in Comparing packages installed on Amazon Linux 2023 Container Images . For more information about the differences between AL2023 and Amazon Linux 2, see Introducing the Amazon Linux 2023 runtime for AWS Lambda on the AWS\n      Compute Blog. Note To run AL2023-based images locally, including with AWS Serverless Application Model (AWS SAM), you must use Docker version 20.10.10 or later. To build a container image using an AWS base image, choose the instructions for your preferred language: Node.js TypeScript (uses a Node.js base image) Python Java Go .NET Ruby Using an AWS OS-only base image AWS OS-only base images contain an Amazon Linux distribution and the runtime interface emulator . These images are commonly used to create container images for compiled languages, such as Go and Rust , and for a language or language version that Lambda doesn't provide a base image for, such as Node.js 19. You can also use OS-only base images to implement a custom runtime . To make the image compatible with Lambda, you must include a runtime interface client for your language in the image. Tags Runtime Operating system Dockerfile Deprecation al2023 OS-only Runtime Amazon Linux 2023 Dockerfile\n                for OS-only Runtime on GitHub Not scheduled al2 OS-only Runtime Amazon Linux 2 Dockerfile\n                for OS-only Runtime on GitHub Not scheduled Amazon Elastic Container Registry Public Gallery: gallery.ecr.aws/lambda/provided Using a non-AWS base image Lambda\n      supports any image that conforms to one of the following image manifest formats: Docker image manifest V2, schema 2 (used with Docker version 1.10 and newer) Open Container Initiative (OCI) Specifications (v1.0.0 and up) Lambda supports a maximum uncompressed image size of 10 GB, including all layers. Note To make the image compatible with Lambda, you must include a runtime interface client for your language in the image. Runtime interface clients If you use an OS-only base image or an alternative base image, you must include a runtime interface client in your image. The runtime interface client must extend the Using the Lambda runtime API for custom runtimes , which manages the interaction between Lambda and your function code. AWS provides open-source runtime interface clients for the following languages: Node.js Python Java .NET Go Ruby Rust \u2013 The Rust runtime client is an experimental package. It is subject to change and intended only for evaluation purposes. If you're using a language that doesn't have an AWS-provided runtime interface client, you must create your own. Amazon ECR permissions Before you create a Lambda function from a container image, you must build the image locally and upload it to an Amazon ECR repository. When you create the function, specify the Amazon ECR repository URI. Make sure that the permissions for the user or role that creates the function includes GetRepositoryPolicy and SetRepositoryPolicy . For example, use the IAM console to create a role with the following policy: { \"Version\": \"2012-10-17\",\n  \"Statement\": [ { \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:SetRepositoryPolicy\",\n        \"ecr:GetRepositoryPolicy\"\n      ],\n      \"Resource\": \"arn:aws:ecr: us-east-1 :111122223333 :repository/ hello-world \"\n    }\n  ]\n} Amazon ECR repository policies For a function in the same account as the container image in Amazon ECR, you can add ecr:BatchGetImage and ecr:GetDownloadUrlForLayer permissions to your Amazon ECR repository policy. The following example shows the\n        minimum policy: { \"Sid\" : \"LambdaECRImageRetrievalPolicy\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"lambda.amazonaws.com\" }, \"Action\" : [ \"ecr:BatchGetImage\" , \"ecr:GetDownloadUrlForLayer\" ]\n    } For more information about Amazon ECR\n        repository permissions, see Private repository policies in the Amazon Elastic Container Registry User Guide . If the Amazon ECR repository does not include these permissions, Lambda adds ecr:BatchGetImage and ecr:GetDownloadUrlForLayer to the container image repository permissions. Lambda can add these\n        permissions only if the principal calling Lambda has ecr:getRepositoryPolicy and ecr:setRepositoryPolicy permissions. To view or edit your Amazon ECR repository permissions, follow the directions in Setting a private repository policy statement in the Amazon Elastic Container Registry User Guide . Amazon ECR cross-account permissions A different account in the same region can create a function that uses a container image owned by your\n          account. In the following example, your Amazon ECR repository permissions policy needs the following statements to\n          grant access to account number 123456789012. CrossAccountPermission \u2013 Allows account 123456789012 to create and update Lambda\n              functions that use images from this ECR repository. LambdaECRImageCrossAccountRetrievalPolicy \u2013 Lambda will eventually set a\n              function's state to inactive if it is not invoked for an extended period. This statement is required so that\n              Lambda can retrieve the container image for optimization and caching on behalf of the function owned by\n              123456789012. Example \u2014 Add cross-account permission to your repository { \"Version\": \"2012-10-17\",\n  \"Statement\": [ { \"Sid\": \"CrossAccountPermission\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchGetImage\",\n        \"ecr:GetDownloadUrlForLayer\"\n      ],\n      \"Principal\": { \"AWS\": \"arn:aws:iam:: 123456789012 :root\"\n      }\n    }, { \"Sid\": \"LambdaECRImageCrossAccountRetrievalPolicy\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchGetImage\",\n        \"ecr:GetDownloadUrlForLayer\"\n      ],\n      \"Principal\": { \"Service\": \"lambda.amazonaws.com\"\n      },\n      \"Condition\": { \"StringLike\": { \"aws:sourceARN\": \"arn:aws:lambda: us-east-1 : 123456789012 :function:*\"\n        }\n      }\n    }\n  ]\n} To give access to multiple accounts, you add the account IDs to the Principal list in the CrossAccountPermission policy and to the Condition evaluation list in the LambdaECRImageCrossAccountRetrievalPolicy . If you are working with multiple accounts in an AWS Organization, we recommend that you enumerate each\n          account ID in the ECR permissions policy. This approach aligns with the AWS security best practice of setting\n          narrow permissions in IAM policies. In addition to Lambda permissions, the user or role that creates the function must also have BatchGetImage and GetDownloadUrlForLayer permissions. Function lifecycle After you upload a new or updated container image, Lambda optimizes the image before the function can process invocations. The\n      optimization process can take a few seconds. The function remains in the Pending state until the\n      process completes. The function then transitions to the Active state. While the state is Pending , you can invoke the function, but other operations on the function fail. Invocations that\n      occur while an image update is in progress run the code from the previous image. If a function is not invoked for multiple weeks, Lambda reclaims its optimized version, and the function\n      transitions to the Inactive state. To reactivate the function, you must invoke it. Lambda rejects the\n      first invocation and the function enters the Pending state until Lambda re-optimizes the image. The\n      function then returns to the Active state. Lambda periodically fetches the associated container image from the Amazon ECR repository. If the\n      corresponding container image no longer exists on Amazon ECR or permissions are revoked, the function enters the Failed state, and\n      Lambda returns a failure for any function invocations. You can use the Lambda API to get information about a function's state. For more information, see Lambda function states . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions .zip file archives Memory"}, {"title": "AWS Outposts Family Documentation", "url": "https://docs.aws.amazon.com/outposts/", "content": "No main content found."}, {"title": "Get started with ROSA - Red Hat OpenShift Service on AWS", "url": "https://docs.aws.amazon.com/ROSA/latest/userguide/getting-started.html", "content": "Get started with ROSA PDF RSS Red Hat OpenShift Service on AWS (ROSA) is a managed service that you can use to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. You can use the following guides to create your first ROSA cluster, grant user access, deploy your first application, and learn how to revoke user access and delete your cluster. Create a ROSA with HCP cluster using the ROSA CLI - Create your first ROSA with HCP cluster using AWS STS and the ROSA CLI. Create a ROSA classic cluster that uses AWS PrivateLink - Create your first ROSA classic cluster using AWS PrivateLink. Create a ROSA classic cluster using the ROSA CLI - Create your first ROSA classic cluster using AWS STS and the ROSA CLI. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Architecture Set up"}, {"title": "What is AWS Lambda? - AWS Lambda", "url": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html", "content": "What is AWS Lambda? PDF RSS You can use AWS Lambda to run code without provisioning or managing servers. Lambda runs your code\n    on a high-availability compute infrastructure and performs all of the administration of the compute resources,\n    including server and operating system maintenance, capacity provisioning and automatic scaling, and\n    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports. You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume\u2014there is no charge when your code is not running. For more information, see AWS Lambda Pricing . Tip To learn how to build serverless solutions , check out the Serverless Developer Guide . When to use Lambda Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to\n      zero when not in demand. For example, you can use Lambda for: File processing: Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload. Stream processing: Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering. Web applications: Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers. IoT backends: Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests. Mobile backends: Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends. When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a\n      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you\n      cannot log in to compute instances or customize the operating system on provided\n        runtimes. Lambda performs operational and administrative activities on your behalf, including managing\n      capacity, monitoring, and logging your Lambda functions. Key features The following key features help you develop Lambda applications that are scalable, secure, and easily\n      extensible: Environment variables Use environment variables to adjust your function's behavior without updating code. Versions Manage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version. Container images Create a container image for a Lambda function by using an AWS provided base image or an alternative base\n            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning. Layers Package libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code. Lambda extensions Augment your Lambda functions with tools for monitoring, observability, security, and governance. Function URLs Add a dedicated HTTP(S) endpoint to your Lambda function. Response streaming Configure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads. Concurrency and scaling controls Apply fine-grained control over the scaling and responsiveness of your production applications. Code signing Verify that only approved developers publish unaltered, trusted code in your Lambda functions Private networking Create a private network for resources such as databases, cache instances, or internal services. File system access Configure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency. Lambda SnapStart for Java Improve startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Create your first function"}, {"title": "What is Amazon Lightsail? - Amazon Lightsail", "url": "https://docs.aws.amazon.com/lightsail/latest/userguide/what-is-amazon-lightsail.html", "content": "What is Amazon Lightsail? PDF Amazon Lightsail is the easiest way to get started with Amazon Web Services (AWS) for anyone who\n    needs to build websites or web applications. It includes everything you need to launch your\n    project quickly\u2014instances (virtual private servers), container services, managed\n    databases, content delivery network (CDN) distributions, load balancers, SSD-based block\n    storage, static IP addresses, DNS management of registered domains, and resource snapshots\n    (backups)\u2014for a low, predictable monthly price. Lightsail also offers Amazon Lightsail for Research. With Lightsail for Research, academics and researchers can create powerful\n    virtual computers in the AWS Cloud. These virtual computers come with pre-installed research\n    applications, such as RStudio and Scilab. For more information see the Amazon Lightsail for Research\n      User Guide . Topics Features Who is Lightsail for? Access Lightsail Get started Related services Estimates, billing, and cost\n        optimization Features of Lightsail Lightsail provides the following high-level features: Instances Lightsail offers virtual private servers (instances) that are easy to set up and\n            backed by the power and reliability of AWS. You can launch your website, web\n            application, or project in minutes, and manage your instance from the intuitive\n            Lightsail console or API. As you\u2019re creating your instance, you'll click-to-launch a simple operating system\n            (OS), a pre-configured application, or development stack\u2014such as WordPress, Windows,\n            Plesk, LAMP, Nginx, and more. Every Lightsail instance comes with a built-in firewall\n            that you can use to allow or restrict traffic to your instances based on source IP,\n            port, and protocol. Learn\n              more Containers Run and securely access containerized applications in the cloud. A container is a\n            standard unit of software that packages code and its dependencies together so the\n            application runs quickly and reliably from one computing environment to another. Learn more Load balancers Route web traffic across your instances so your websites and applications can\n            accommodate variations in traffic, protected against outages, and deliver a seamless\n            visitor experience. Learn\n              more Managed databases Lightsail offers a fully configured MySQL or PostgreSQL databases plan that\n            includes memory, processing, storage, and transfer allowance. With Lightsail managed\n            databases, you can easily scale your databases independently of your virtual servers,\n            improve application availability, or run standalone databases in the cloud. Learn more Block and object storage Lightsail offers both block and object storage. You can scale your storage quickly\n            and easily with highly available SSD-backed storage for your Linux or Windows virtual\n            server. Learn\n              more With Lightsail Object storage buckets, you can store and retrieve objects, at any\n            time, from anywhere on the internet. You can also host static content on the cloud. Learn more CDN distributions Lightsail enables content delivery network (CDN) distributions, which are built on\n            the same infrastructure as Amazon CloudFront. You can easily distribute your content to a global\n            audience by setting up proxy servers across the world, so that your users can access\n            your website geographically closer to them, thus reducing latency. Learn more Access to AWS services Lightsail uses a focused set of features like instances, managed databases and\n            load balancers to make it easier to get started. But that doesn't mean you're limited to\n            those options \u2013you can integrate your Lightsail project with some of the 90+ other\n            services in AWS through Amazon VPC peering. Learn more For more details about Lightsail, see Amazon Lightsail . Who is Lightsail for? Lightsail is for everyone. You can choose an image for your Lightsail instance that\n      jump starts your project so you don't have to spend as much time installing software or\n      frameworks. If you're an individual developer or hobbyist working on a personal project, Lightsail\n      can help you deploy and manage basic cloud resources. You might also be interested in learning\n      or experimenting with cloud services, such as virtual machines, domains or networking.\n      Lightsail provides a quick way to get started. Lightsail has images with base operating systems, development stacks like LAMP, LEMP\n      (Nginx), and SQL Server Express, and applications like WordPress, Drupal, and Magento. For\n      more detailed information about the software installed on each image, see Choose a\n        Lightsail instance image . As your project grows, you can add block storage disks and attach them to your Lightsail\n      instance. You can take snapshots of these instances and disks and easily create new instances\n      from those snapshots. You can also peer your VPC so that your Lightsail instances can use\n      other AWS resources outside of Lightsail. You can also create a Lightsail load balancer and attach target instances to create a\n      highly available application. You can also configure your load balancer to handle encrypted\n      (HTTPS) traffic, session persistence, health checking, and more. Access Lightsail You can create and manage your Lightsail resources with the following interfaces: Amazon Lightsail console A simple web interface to create and manage Lightsail instances and resources. If\n            you've signed up for an AWS account, you can access the Lightsail console by signing\n            into the AWS Management Console and selecting Lightsail from the console home\n            page. AWS Command Line Interface Enables you to interact with AWS services using commands in your command-line\n            shell. It is supported on Windows, Mac, and Linux. For more information about the AWS CLI\n            , see AWS Command Line Interface User Guide . You can find the Lightsail commands in the Amazon Lightsail API Reference . AWS Tools for PowerShell A set of PowerShell modules that are built on the functionality exposed by the\n            AWS SDK for .NET. The Tools for PowerShell enable you to script operations on your AWS resources from the\n            PowerShell command line. To get started, see the AWS Tools for Windows PowerShell User Guide . You can find the cmdlets\n            for Lightsail, in the AWS Tools for PowerShell Cmdlet Reference . Query API Lightsail provides a Query API. These requests are HTTP or HTTPS requests that use\n            the HTTP verbs GET or POST and a Query parameter named Action . For more\n            information about the API actions for Lightsail, see Actions in the Amazon Lightsail API Reference . AWS SDKs If you prefer to build applications using language-specific APIs instead of\n            submitting a request over HTTP or HTTPS, AWS provides libraries, sample code,\n            tutorials, and other resources for software developers. These libraries provide basic\n            functions that automate tasks such as cryptographically signing your requests, retrying\n            requests, and handling error responses, making it easier for you to get started. For\n            more information, see Tools to\n              Build on AWS . Get started with Lightsail After you set up to\n      use Lightsail, you can walk through Getting started with virtual private servers on\n            Lightsail to launch, connect to, and clean up an instance. Related services You can provision Lightsail resources, such as instances and disks, directly using\n      Lightsail. In addition, you can provision resources using other AWS services, such as the\n      following: Amazon EC2 Provides resizeable computing capacity\u2014literally, servers in Amazon's data\n          centers\u2014that you use to build and host your software systems. To compare Lightsail and\n          Amazon EC2, see Amazon Lightsail or\n            Amazon EC2 . Amazon EC2 Auto Scaling Helps ensure you have the correct number of Amazon EC2 instances available to handle the\n          load for your application. Elastic Load Balancing Automatically distribute incoming application traffic across multiple\n          instances. Amazon Relational Database Service (Amazon RDS) Set up, operate, and scale a managed relational database in the cloud. Amazon Elastic Container Service (Amazon ECS) Deploy, manage, and scale containerized applications on a cluster of Amazon EC2\n          instances. Estimates, billing, and cost\n        optimization To create estimates for your AWS use cases, use the AWS Pricing Calculator . To see your bill, go to the Billing and Cost Management\n        Dashboard in the AWS Billing and Cost Management\n        console . Your bill contains links to usage reports that provide details about your\n      bill. To learn more about AWS account billing, see AWS Billing\n        and Cost Management User Guide . If you have questions concerning AWS billing, accounts, and events, contact AWS Support . You can optimize the cost, security, and performance of your AWS environment using AWS Trusted Advisor . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Set up"}, {"title": "What Is AWS Cloud Map? - AWS Cloud Map", "url": "https://docs.aws.amazon.com/cloud-map/latest/dg/what-is-cloud-map.html", "content": "What Is AWS Cloud Map? PDF RSS AWS Cloud Map is a fully managed solution that you can use to map logical names to the backend services and resources that your\n  applications depend on. It also helps your applications discover resources using one of the AWS SDKs,\n  RESTful API calls, or DNS queries. AWS Cloud Map serves only healthy resources, which can be\n  Amazon DynamoDB (DynamoDB) tables, Amazon Simple Queue Service (Amazon SQS) queues, any higher-level application services that\n  are built using Amazon Elastic Compute Cloud (Amazon EC2) instances or Amazon Elastic Container Service (Amazon ECS) tasks, and more. Components of AWS Cloud Map Namespace To get started, you first create a AWS Cloud Map namespace that functions as a way to group services for an\n      application. A namespace identifies the name that you want to use to locate your resources and\n      also specifies how you want to locate resources: using AWS Cloud Map DiscoverInstances API calls,\n      DNS queries in a VPC, or public DNS queries. In most cases, a namespace contains all the\n      services for an application, such as a billing application. For more information, see AWS Cloud Map namespaces . Service After creating a namespace, you create an AWS Cloud Map service for each type of resource for which you want to use\n      AWS Cloud Map to locate endpoints. For example, you might create services for web servers and\n      database servers. A service is a template that AWS Cloud Map uses when your application adds another resource,\n      such as another web server. If you chose to locate resources using DNS when you created the\n      namespace, a service contains information about the types of records that you want to use to\n      locate the web server. A service also indicates whether you want to check the health of the\n      resource and whether you want to use Amazon Route\u00a053 health checks or a third-party health\n      checker.  For more information, see AWS Cloud Map services . Service instance When your application adds a resource, you can call the AWS Cloud Map RegisterInstance API action in the code, which creates a AWS Cloud Map service instance in a service. The service\n      instance contains information about how your application can locate the resource, whether\n      using DNS or using the AWS Cloud Map DiscoverInstances API\n      action. When your application needs to connect to a resource, it calls DiscoverInstances or utilizes\n      public or private DNS queries by specifying the namespace and service that are associated with\n      the resource. AWS Cloud Map returns information about how to locate one or more resources. If you\n      specified health checking when you created the service, AWS Cloud Map returns only healthy\n      instances.  For more information, see AWS Cloud Map service instances . Accessing AWS Cloud Map You can access AWS Cloud Map in the following ways: AWS Management Console \u2013 The procedures throughout this guide\n     explain how to use the AWS Management Console to perform tasks. AWS SDKs \u2013 If you're using a programming\n     language that AWS provides an SDK for, you can use an SDK to access AWS Cloud Map. SDKs simplify\n     authentication, integrate easily with your development environment, and provide access to\n     AWS Cloud Map commands. For more information, see Tools for Amazon Web Services . AWS Command Line Interface \u2013 For more information, see Get started with the AWS CLI in the AWS Command Line Interface User Guide . AWS Tools for Windows PowerShell \u2013 For more information, see Get started with the AWS Tools for Windows PowerShell in the AWS Tools for Windows PowerShell User Guide . AWS Cloud Map API \u2013 If you're using a programming\n     language that an SDK isn't available for, see the AWS Cloud Map API Reference for information about API actions and\n     about how to make API requests. Note IPv6 Client Support \u2013 As of June 22nd, 2023 in\n      all new regions, any commands sent to AWS Cloud Map from IPv6 clients are routed to a\n      new dualstack endpoint ( servicediscovery.<region>.api.aws ). AWS Cloud Map IPv6 -only networks\n      are reachable for both legacy ( servicediscovery.<region>.amazonaws.com ) and dualstack\n       endpoint s in the following regions that were released prior to June 22nd,\n      2023: US East (Ohio) \u2013 us-east-2 US East (N. Virginia) \u2013 us-east-1 US West (N. California) \u2013 us-west-1 US West (Oregon) \u2013 us-west-2 Africa (Cape Town) \u2013 af-south-1 Asia Pacific (Hong Kong) \u2013 ap-east-1 Asia Pacific (Hyderabad) \u2013 ap-south-2 Asia Pacific (Jakarta) \u2013 ap-southeast-3 Asia Pacific (Melbourne) \u2013 ap-southeast-4 Asia Pacific (Mumbai) \u2013 ap-south-1 Asia Pacific (Osaka) \u2013 ap-northeast-3 Asia Pacific (Seoul) \u2013 ap-northeast-2 Asia Pacific (Singapore) \u2013 ap-southeast-1 Asia Pacific (Sydney) \u2013 ap-southeast-2 Asia Pacific (Tokyo) \u2013 ap-northeast-1 Canada (Central) \u2013 ca-central-1 Europe (Frankfurt) \u2013 eu-central-1 Europe (Ireland) \u2013 eu-west-1 Europe (London) \u2013 eu-west-2 Europe (Milan) \u2013 eu-south-1 Europe (Paris) \u2013 eu-west-3 Europe (Spain) \u2013 eu-south-2 Europe (Stockholm) \u2013 eu-north-1 Europe (Zurich) \u2013 eu-central-2 Middle East (Bahrain) \u2013 me-south-1 Middle East (UAE) \u2013 me-central-1 South America (S\u00e3o Paulo) \u2013 sa-east-1 AWS GovCloud (US-East) \u2013 us-gov-east-1 AWS GovCloud (US-West) \u2013 us-gov-west-1 AWS Identity and Access Management AWS Cloud Map integrates with AWS Identity and Access Management (IAM), a service that your organization can use to do\n   the following actions: Create users and groups under your organization's AWS account Share your AWS account resources among the users in the account in an efficient\n     manner Assign unique security credentials to each user Granularly control user access to services and resources For example, you can use IAM with AWS Cloud Map to control which users in your AWS account\n   can create a new namespace or register instances. For general information about IAM, see the following resources: Identity and Access Management for AWS Cloud Map AWS Identity and Access Management IAM User Guide AWS Cloud Map Pricing AWS Cloud Map pricing is based on resources that you register in the service registry and API\n   calls that you make to discover them. With AWS Cloud Map there are no upfront payments, and you only\n   pay for what you use. Optionally, you can enable DNS-based discovery for the resources with IP addresses. You can\n   also enable health checking for your resources using Amazon Route\u00a053 health checks, whether you're\n   discovering instances using API calls or DNS queries. You will incur additional charges related\n   to Route\u00a053 DNS and health check usage. For more information, see AWS Cloud Map\n    Pricing . AWS Cloud Map and AWS Cloud Compliance For information about AWS Cloud Map compliance with various security compliance regulations and\n   audits standards, see the following pages: AWS Cloud Compliance AWS Services in Scope by\n      Compliance Program Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Get started"}, {"title": "Amazon ECS clusters for the external launch type - Amazon Elastic Container Service", "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-anywhere.html", "content": "Amazon ECS clusters for the external launch type PDF RSS Amazon ECS Anywhere provides support for registering an external instance such as an on-premises server or virtual machine (VM), to your Amazon ECS cluster. External\n        instances are optimized for running applications that generate outbound traffic or process\n        data. If your application requires inbound traffic, the lack of Elastic Load Balancing support makes running\n        these workloads less efficient. Amazon ECS added a new EXTERNAL launch type that you\n        can use to create services or run tasks on your external instances. The following provides a high-level system architecture overview of Amazon ECS Anywhere. Your\n        on-premises server has both the Amazon ECS agent and the SSM agent installed. Supported operating systems and system\n                architectures The following is the list of supported operating systems and system\n            architectures. Amazon Linux 2 Amazon Linux 2023 CentOS 7 CentOS Stream 9 RHEL 7, RHEL 8 \u2014 Neither Docker or RHEL's open package repositories\n                    support installing Docker natively on RHEL. You must ensure that Docker is\n                    installed before you run the install script that's described in this\n                    document. Fedora 32, Fedora 33, Fedora 40 openSUSE Tumbleweed Ubuntu 18, Ubuntu 20, Ubuntu 22, Ubuntu 24 Debian 10 Important Debian 9 Long Term Support (LTS support) ended on June 30, 2022 and is no\n                        longer supported by Amazon ECS Anywhere. Debian 11 Debian 12 SUSE Enterprise Server 15 The x86_64 and ARM64 CPU architectures are\n                    supported. The following Windows operating system versions are supported: Windows Server 2022 Windows Server 2019 Windows Server 2016 Windows Server 20H2 Considerations Before you start using external instances, be aware of the following\n            considerations. You can register an external instance to one cluster at a time. For\n                    instructions on how to register an external instance with a different cluster,\n                    see Deregistering an Amazon ECS external instance . Your external instances require an IAM role that allows them to communicate\n                    with AWS APIs. For more information, see Amazon ECS Anywhere IAM role . Your external instances should not have a preconfigured instance credential\n                    chain defined locally as this will interfere with the registration\n                    script. To send container logs to CloudWatch Logs, make sure that you create and specify a task\n                    execution IAM role in your task definition. When an external instance is registered to a cluster, the ecs.capability.external attribute is associated with the\n                    instance. This attribute identifies the instance as an external instance. You\n                    can add custom attributes to your external instances to use as a task placement\n                    constraint. For more information, see Custom attributes . You can add resource tags to your external instance. For more information, see External container instances . ECS Exec is supported on external instances. For more information, see Monitor Amazon ECS containers with ECS Exec . The following are additional considerations that are specific to networking\n                    with your external instances. For more information, see Networking . Service load balancing isn't supported. Service discovery isn't supported. Tasks that run on external instances must use the bridge , host , or none network modes. The awsvpc network mode isn't supported. There are Amazon ECS service domains in each AWS Region. These service\n                            domains must be allowed to send traffic to your external\n                            instances. The SSM Agent installed on your external instance maintains IAM\n                            credentials that are rotated every 30 minutes using a hardware\n                            fingerprint. If your external instance loses connection to AWS, the\n                            SSM Agent automatically refreshes the credentials after the connection\n                            is re-established. For more information, see Validating on-premises servers and virtual machines using a\n                                hardware fingerprint in the AWS Systems Manager User Guide . The UpdateContainerAgent API isn't supported. For instructions on\n                    how to update the SSM Agent or the Amazon ECS agent on your external instances, see Updating the AWS Systems Manager agent and Amazon ECS container\n            agent on an external instance . Amazon ECS capacity providers aren't supported. To create a service or run a\n                    standalone task on your external instances, use the EXTERNAL launch\n                    type. SELinux isn't supported. Using Amazon EFS volumes or specifying an EFSVolumeConfiguration isn't\n                    supported. Integration with App Mesh isn't supported. If you use the console to create an external instance task definition, you\n                    must create the task definition with the console JSON editor. When you run ECS Anywhere on Windows, you must use your own Windows license on\n                    the on-premises infrastructure. When you use a non Amazon ECS-optimized AMI, run the following commands on the\n                    external container instance to configure rules to use IAM roles for tasks. For\n                    more information, see External instance additional\n                configuration . $ sysctl -w net.ipv4.conf.all.route_localnet= 1 $ iptables -t nat -A PREROUTING -p tcp -d 169.254 . 170.2 - -dport 80 -j DNAT - -to -destination 127.0 . 0.1 : 51679 $ iptables -t nat -A OUTPUT -d 169.254 . 170.2 -p tcp -m tcp - -dport 80 -j REDIRECT - -to -ports 51679 Networking Amazon ECS external instances are optimized for running applications that generate\n                outbound traffic or process data. If your application requires inbound traffic, such\n                as a web service, the lack of Elastic Load Balancing support makes running these workloads less\n                efficient because there isn't support for placing these workloads behind a load\n                balancer. The following are additional considerations that are specific to networking with\n                your external instances. Service load balancing isn't supported. Service discovery isn't supported. Linux tasks that run on external instances must use the bridge , host , or none network\n                        modes. The awsvpc network mode isn't supported. For more information about each network mode, see Choosing a network mode in the Amazon ECS Best Practices\n                            Guide . Windows tasks that run on external instances must use the default network mode. There are Amazon ECS service domains in each Region and must be\n                        allowed to send traffic to your external instances. The SSM Agent installed on your external instance maintains IAM\n                        credentials that are rotated every 30 minutes using a hardware fingerprint.\n                        If your external instance loses connection to AWS, the SSM Agent\n                        automatically refreshes the credentials after the connection is\n                        re-established. For more information, see Validating on-premises servers and virtual machines using a hardware\n                            fingerprint in the AWS Systems Manager User Guide . The following domains are used for communication between the Amazon ECS service and the\n                Amazon ECS agent that's installed on your external instance. Make sure that traffic is\n                allowed and that DNS resolution works. For each endpoint, region represents the Region identifier for an AWS\n                Region that's supported by Amazon ECS, such as us-east-2 for the\n                US East (Ohio) Region. The endpoints for all Regions that you use should be\n                allowed. For the ecs-a and ecs-t endpoints, you should\n                include an asterisk (for example, ecs-a-* ). ecs-a-*. region .amazonaws.com \u2014 This endpoint is used when managing tasks. ecs-t-*. region .amazonaws.com \u2014 This endpoint is used to manage task and container metrics. ecs. region .amazonaws.com \u2014\n                        This is the service endpoint for Amazon ECS. ssm. region .amazonaws.com \u2014\n                        This is the service endpoint for AWS Systems Manager. ec2messages. region .amazonaws.com \u2014 This is the service endpoint that AWS Systems Manager uses to communicate\n                        between the Systems Manager agent and the Systems Manager service in the cloud. ssmmessages. region .amazonaws.com \u2014 This is the service endpoint that is required to create and delete\n                        session channels with the Session Manager service in the cloud. If your tasks require communication with any other AWS services, make\n                        sure that those service endpoints are allowed. Example applications include\n                        using Amazon ECR to pull container images or using CloudWatch for CloudWatch Logs. For more\n                        information, see Service\n                            endpoints in the AWS General\n                        Reference . Amazon FSx for Windows File Server with ECS\n                    Anywhere In order to use the Amazon FSx for Windows File Server with Amazon ECS external instances you\n                must establish a connection between your on-premises data center and the\n                AWS Cloud. For information about the options for connecting your network to your\n                VPC, see Amazon Virtual Private Cloud Connectivity Options . gMSA with ECS Anywhere The following use cases are supported for ECS Anywhere. The Active Directory is in the AWS Cloud - For this configuration, you\n                        create a connection between your on-premises network and the AWS Cloud\n                        using an AWS Direct Connect connection. For information about how to create the\n                        connection, see Amazon Virtual Private Cloud Connectivity Options .You create an Active Directory\n                        in the AWS Cloud. For information about how to get started with AWS Directory Service,\n                        see Setting up\n                            AWS Directory Service in the AWS Directory Service\n                            Administration Guide . You can then join your external\n                        instances to the domain using the AWS Direct Connect connection. For information\n                        about working with gMSA with Amazon ECS, see Learn how to use gMSAs for EC2 Windows containers for Amazon ECS . The Active Directory is in the on-premises data center. - For this\n                        configuration, you join your external instances to the on-premises Active\n                        Directory. You then use the locally available credentials when you run the\n                        Amazon ECS tasks. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Configuring container instances to receive Spot Instance notices Creating a cluster for the\n            External launch type"}, {"title": "Amazon EC2 instance types - Amazon EC2", "url": "https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-types.html", "content": "Amazon EC2 instance types PDF RSS When you launch an EC2 instance, the instance type that you specify\n        determines the hardware of the host computer used for your instance. Each instance type\n        offers different compute, memory, and storage capabilities, and is grouped in an instance\n        family based on these capabilities. Select an instance type based on the requirements of the\n        application or software that you plan to run on your instance. Amazon EC2 dedicates some resources of the host computer, such as CPU, memory, and instance\n        storage, to a particular instance. Amazon EC2 shares other resources of the host computer, such\n        as the network and the disk subsystem, among instances. If each instance on a host computer\n        tries to use as much of one of these shared resources as possible, each receives an equal\n        share of that resource. However, when a resource is underused, an instance can consume a\n        higher share of that resource while it's available. Each instance type provides higher or lower minimum performance from a shared resource.\n        For example, instance types with high I/O performance have a larger allocation of shared resources. \n        Allocating a larger share of shared resources also reduces the variance of I/O performance. \n        For most applications, moderate I/O performance is more than enough. However, for\n        applications that require greater or more consistent I/O performance, consider\n        an instance type with higher I/O performance. Contents Current generation instances Previous generation instances Amazon EC2 instance type naming conventions Amazon EC2 instance type specifications Instances built on the AWS Nitro System Amazon EC2 instance type quotas Current generation instances For the best performance, we recommend that you use the following instance types\n            when you launch new instances. For more information, see Amazon EC2 Instance Types . General purpose: M5 | M5a | M5ad | M5d | M5dn | M5n | M5zn | M6a | M6g | M6gd | M6i | M6id | M6idn | M6in | M7a | M7g | M7gd | M7i | M7i-flex | M8g | Mac1 | Mac2 | Mac2-m1ultra | Mac2-m2 | Mac2-m2pro | T2 | T3 | T3a | T4g Compute optimized: C5 | C5a | C5ad | C5d | C5n | C6a | C6g | C6gd | C6gn | C6i | C6id | C6in | C7a | C7g | C7gd | C7gn | C7i | C7i-flex | C8g Memory optimized: R5 | R5a | R5ad | R5b | R5d | R5dn | R5n | R6a | R6g | R6gd | R6i | R6idn | R6in | R6id | R7a | R7g | R7gd | R7i | R7iz | R8g | U-3tb1 | U-6tb1 | U-9tb1 | U-12tb1 | U-18tb1 | U-24tb1 | U7i-12tb | U7in-16tb | U7in-24tb | U7in-32tb | X1 | X2gd | X2idn | X2iedn | X2iezn | X1e | X8g | z1d Storage optimized: D2 | D3 | D3en | H1 | I3 | I3en | I4g | I4i | Im4gn | Is4gen Accelerated computing: DL1 | DL2q | F1 | G4ad | G4dn | G5 | G5g | G6 | G6e | Gr6 | Inf1 | Inf2 | P2 | P3 | P3dn | P4d | P4de | P5 | P5e | Trn1 | Trn1n | VT1 High-performance computing: Hpc6a | Hpc6id | Hpc7a | Hpc7g Previous generation instances Amazon Web Services offers previous generation instance types for users who have optimized their\n            applications around them and have yet to upgrade. We encourage you to use current generation \n            instance types to get the best performance, but we continue to support the following previous \n            generation instance types. For more information about which current \n                generation instance type would be a suitable upgrade, see Previous Generation Instances . General purpose : A1 | M1 | M2 | M3 | M4 | T1 Compute optimized : C1 | C3 | C4 Memory optimized : R3 | R4 Storage optimized : I2 Accelerated computing : G3 Instance performance Fixed performance instances Fixed performance instances provide fixed CPU resources. These instances can\n                deliver and sustain full CPU performance at any time, and for as long as a workload\n                needs it. If you need consistently high CPU performance for applications such as\n                video encoding, high volume websites, or HPC applications, we recommend that you use\n                fixed performance instances. Burstable performance instances Burstable performance ( T ) instances provide a baseline level of CPU\n                performance with the ability to burst above the baseline. The baseline CPU is\n                designed to meet the needs of the majority of general purpose workloads, such as\n                large-scale micro-services, web servers, small and medium databases, data logging,\n                code repositories, virtual desktops, and development and test environments. The baseline utilization and ability to burst are governed by CPU credits. Each\n            burstable performance instance continuously earns credits when it stays below the CPU\n            baseline, and continuously spends credits when it bursts above the baseline. For more\n            information, see Burstable\n                performance instances in the Amazon EC2 User Guide . Flex instances M7i-flex and C7i-flex instances offer a balance of compute, memory, and network\n                resources, and they provide the most cost-effective way to run a broad spectrum of\n                general purpose applications. These instances provide reliable CPU resources to\n                deliver a baseline CPU performance of 40 percent, which is designed to meet the\n                compute requirements for a majority of general purpose workloads. When more\n                performance is needed, these instances provide the ability to exceed the baseline\n                CPU performance and deliver up to 100 percent CPU performance for 95 percent of the\n                time over a 24-hour window. M7i-flex and C7i-flex instances running at a high CPU utilization that is consistently\n            above the baseline for long periods of time might see a gradual reduction in the maximum\n            burst CPU throughput. For more information, see M7i-flex instances and C7i-flex instances . Pricing For pricing information, see Amazon EC2 Pricing . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Naming conventions"}, {"title": "What is AWS Outposts? - AWS Outposts", "url": "https://docs.aws.amazon.com/outposts/latest/server-userguide/what-is-outposts.html", "content": "What is AWS Outposts? PDF RSS AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and\n    tools to customer premises. By providing local access to AWS managed infrastructure, AWS Outposts\n    enables customers to build and run applications on premises using the same programming\n    interfaces as in AWS Regions, while using local compute and storage resources for lower\n    latency and local data processing needs. An Outpost is a pool of AWS compute and storage capacity deployed at a\n    customer site. AWS operates, monitors, and manages this capacity as part of an AWS Region.\n    You can create subnets on your Outpost and specify them when you create AWS resources such as\n    EC2 instances and subnets. Instances in Outpost subnets communicate with other instances in the\n    AWS Region using private IP addresses, all within the same VPC. Note You can't connect an Outpost to another Outpost or Local Zone that is within the same\n      VPC. For more information, see the AWS Outposts product\n      page . Key concepts These are the key concepts for AWS Outposts. Outpost site \u2013 The customer-managed physical\n          buildings where AWS will install your Outpost. A site must meet the facility,\n          networking, and power requirements for your Outpost. Outpost capacity \u2013 Compute and storage resources\n          available on the Outpost. You can view and manage the capacity for your Outpost from the\n          AWS Outposts console. Outpost equipment \u2013 Physical hardware that\n          provides access to the AWS Outposts service. The hardware includes racks, servers, switches,\n          and cabling owned and managed by AWS. Outposts racks \u2013 An Outpost form factor that is an\n          industry-standard 42U rack. Outposts racks include rack-mountable servers, switches, a network\n          patch panel, a power shelf and blank panels. Outposts servers \u2013 An Outpost form factor that is an\n          industry-standard 1U or 2U server, which can be installed in a standard EIA-310D 19\n          compliant 4 post rack. Outposts servers provide local compute and networking services to sites\n          that have limited space or smaller capacity requirements. Outpost owner \u2013 The account owner for the\n          account that places the AWS Outposts order. After AWS engages with the customer, the owner may\n          include additional points of contact. AWS will communicate with the contacts to clarify\n          orders, installation appointments, and hardware maintenance and replacement. Contact AWS Support Center if the contact information changes. Service link \u2013 Network route that enables\n          communication between your Outpost and its associated AWS Region. Each Outpost is an\n          extension of an Availability Zone and its associated Region. Local gateway (LGW) \u2013 A logical interconnect\n          virtual router that enables communication between an Outposts rack and your on-premises network. Local network interface \u2013 A network interface\n          that enables communication from an Outposts server and your on-premises network. AWS resources on Outposts You can create the following resources on your Outpost to support low-latency workloads\n      that must run in close proximity to on-premises data and applications: Compute Resource type Racks Servers Amazon EC2 instances Yes Yes Amazon ECS\n                clusters Yes Yes Amazon EKS\n                  nodes Yes No Database and analytics Resource type Racks Servers Amazon ElastiCache nodes ( Redis\n                  cluster , Memcached\n                  cluster ) Yes No Amazon EMR\n                clusters Yes No Amazon RDS DB\n                instances Yes No Networking Resource type Racks Servers App Mesh Envoy\n                  proxy Yes Yes Application Load Balancers Yes No Amazon VPC subnets Yes Yes Amazon Route\u00a053 Yes No Storage Resource type Racks Servers Amazon EBS\n                  volumes Yes No Amazon S3\n                  buckets Yes No Other AWS services Service Racks Servers AWS IoT Greengrass Yes Yes Amazon SageMaker Edge Manager Yes Yes Pricing Pricing is based on your order details. When you place an order, you can choose from a\n      variety of Outpost configurations, each providing a combination of Amazon EC2 instance types and\n      storage options. You also choose a contract term and a payment option. Pricing includes the\n      following: Outposts racks - Delivery, installation, infrastructure\n          service maintenance, software patches and upgrades, and rack removal. Outposts servers - Delivery, infrastructure service\n          maintenance, and software patches and upgrades. You are responsible for the installation\n          and packing the server for return. You are billed for shared resources and any data transfer from the AWS Region to the\n      Outpost. You are also billed for data transfers that AWS performs to maintain availability\n      and security. For pricing based on location, configuration, and payment option, see: Outposts racks pricing Outposts servers\n          pricing Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How AWS Outposts works"}, {"title": "Learn how to create and use Amazon ECS resources - Amazon Elastic Container Service", "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started.html", "content": "Learn how to create and use Amazon ECS resources PDF RSS The following guides provide an introduction to the tools available to access Amazon ECS and\n        introductory procedures to run containers. Docker basics takes you through the basic steps\n        to create a Docker container image and upload it to an Amazon ECR private repository. The getting\n        started guides walk you through using the AWS Copilot command line interface and the AWS Management Console to\n        complete the common tasks to run your containers on Amazon ECS and AWS Fargate. Contents Set up Creating a container image Learn how to create a Linux task for the Fargate launch type Learn how to create a \n            Windows task for the Fargate launch type Learn how to create a  Windows task for the EC2 launch type Using the AWS CDK Creating resources using\n            AWS CloudFormation Creating resources using the AWS Copilot CLI Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Related information Set up"}, {"title": "Tutorials for Amazon ECS - Amazon Elastic Container Service", "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-tutorials.html", "content": "Tutorials for Amazon ECS PDF RSS The following tutorials show you how to perform common tasks when using Amazon ECS. You can use any of the following tutorials to deploy tasks on Amazon ECS using the AWS CLI Tutorial overview Learn more Create a Linux task for the Fargate launch type. Creating an Amazon ECS Linux task for the Fargate launch type with the AWS CLI Create a Windows task for the Fargate launch type. Creating an Amazon ECS Windows \n            task for the Fargate launch type with the AWS CLI Create a Linux task for the EC2 launch type. Creating an Amazon ECS task for the EC2 launch type with the AWS CLI You can use any of the following tutorials to learn more about monitoring and\n        logging. Tutorial overview Learn more Set up a simple Lambda function that listens for task events and writes\n                            them out to a CloudWatch Logs log stream. Configuring Amazon ECS to listen for CloudWatch Events events Configure an Amazon EventBridge event rule that only captures task events where\n                            the task has stopped running because one of its essential containers has\n                            terminated. Sending Amazon Simple Notification Service alerts for Amazon ECS task stopped\n            events Concatenate log messages that originally belong to one context but\n                            were split across multiple records or log lines. Concatenating multiline or stack-trace\n                Amazon ECS log messages Deploy Fluent Bit containers on their Windows instances running in\n                            Amazon ECS to stream logs generated by the Windows tasks to Amazon CloudWatch for\n                            centralized logging. Deploying Fluent Bit on\n            Amazon ECS Windows containers You can use any of the following tutorials to learn more about how to use Active Directory authentication with group Managed Service Account on Amazon ECS. Tutorial overview Learn more Use group Managed Service Account with Linux containers on EC2. Using gMSA for EC2 Linux containers\n            on Amazon ECS Use group Managed Service Account with Windows containers on EC2. Learn how to use gMSAs for EC2 Windows containers for Amazon ECS Use group Managed Service Account with Linux containers on Fargate. Using gMSA for Linux\n            containers on Fargate Create a task that runs a Windows container that has credentials to access Active Directory with domainless group Managed Service Account. Using Amazon ECS Windows containers with domainless\n        gMSA using the AWS CLI Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Task and container security best practices Creating a Linux task for the Fargate launch type with the AWS CLI"}, {"title": "Amazon Elastic Container Service - Overview of Deployment Options on AWS", "url": "https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/amazon-elastic-container-service.html", "content": "Amazon Elastic Container Service PDF RSS Amazon Elastic Container Service (Amazon ECS) is a fully managed\n      container orchestration service that supports Docker containers\n      and allows you to easily run applications on a managed cluster.\n      Amazon ECS eliminates the need to install, operate, and scale\n      container management infrastructure, and simplifies the creation\n      of environments with familiar AWS core features like Security\n      Groups , Elastic Load Balancing , and AWS Identity and Access Management (IAM). When running applications on Amazon ECS, you can choose to provide\n      the underlying compute power for your containers with Amazon EC2\n      instances or with AWS Fargate , a serverless compute engine for containers. In\n      either case, Amazon ECS automatically places and scales your\n      containers onto your cluster according to configurations defined\n      by the user. Although Amazon ECS does not create infrastructure\n      components such as Load Balancers or IAM roles on your behalf, the\n      Amazon ECS service provides a number of APIs to simplify the\n      creation and use of these resources in an Amazon ECS cluster. Amazon ECS allows developers to have direct, fine-grained control\n      over all infrastructure components, allowing for the creation of\n      custom application architectures. Additionally, Amazon ECS\n      supports different deployment strategies to update your\n      application container images. Table 4: Amazon ECS deployment features Capability Description Provision Amazon ECS will provision new application container\n                instances and compute resources based on scaling\n                policies and Amazon ECS configurations. Infrastructure\n                resources such as Load Balancers will need to be created\n                outside of Amazon ECS. Refer to Getting\n                Started with Amazon ECS for more details on the\n                types of resources that can be created with Amazon ECS. Configure Amazon ECS supports customization of the compute\n                resources created to run a containerized application, as\n                well as the runtime conditions of the application\n                containers (for example, environment variables, exposed ports,\n                reserved memory/CPU). Customization of underlying\n                compute resources is only available if using Amazon EC2\n                instances. Refer to Creating\n                a Cluster for more details on how to customize an\n                Amazon ECS cluster to run containerized applications. Deploy Amazon ECS supports several deployment strategies for\n                you containerized applications. Refer to Amazon ECS Deployment Types for more details on the\n                types of deployment processes that are supported. Scale Amazon ECS can be used with auto scaling policies to\n                automatically adjust the number of containers running in\n                your Amazon ECS cluster. Refer to Service Auto Scaling for more details on configuring\n                auto scaling for your containerized applications on Amazon ECS. Monitor Amazon ECS supports monitoring compute resources and\n                application containers with CloudWatch. Refer to Monitoring\n                Amazon ECS for more details on the types of\n                monitoring capabilities offered by Amazon ECS. The following diagram illustrates Amazon ECS being used to manage\n      a simple containerized application. In this example,\n      infrastructure components are created outside of Amazon ECS, and\n      Amazon ECS is used to manage the deployment and operation of\n      application containers on the cluster Amazon ECS use case Note Application infrastructure (including Amazon Elastic Container Registry (Amazon ECR) repositories, Amazon ECS configurations, and Load Balancers) \n            is provisioned and managed outside of your Amazon ECS deployment. Amazon ECS manages the deployment of application containers running inside the Amazon ECS service as tasks that are sourced from a container registry like Amazon ECR. Amazon ECS supports multiple container instance types such as Linux and Windows, as well as external instance \n      types such as an on-premises virtual machine (VM) with Amazon ECS Anywhere. Amazon ECS Anywhere Amazon ECS Anywhere allows you to run \n        Amazon ECS tasks anywhere, whether it's on-premises or in other cloud environments. \n        With Amazon ECS Anywhere, you can easily deploy and manage containerized applications across \n        your hybrid infrastructure, while maintaining a consistent operational experience. The \n        service works by extending the Amazon ECS platform to any environment, including on-premises \n        data centers, remote offices, and other cloud environments. It enables you to use the \n        same familiar Amazon ECS APIs and tooling to deploy and manage containers across all of \n        your environments, without having to worry about the underlying infrastructure. Amazon ECS Anywhere uses the Amazon ECS agent to manage the deployment and lifecycle of containers, \n        enabling you to use the same Amazon ECS task definitions and configuration files that \n        you use in the AWS Cloud. This can help to simplify the process of deploying and \n        managing containers across your hybrid infrastructure, and reduce the time and effort r\n        equired for manual configuration and management. With Amazon ECS Anywhere, you can also leverage other AWS services, such as IAM, AWS CloudFormation, and Amazon ECR, to \n        manage your containerized applications. This can help to ensure that your applications are \n        secure, compliant, and integrated with other AWS services. Amazon ECS Anywhere architecture Amazon Elastic Container Service on AWS Outposts Amazon ECS on AWS Outposts is a fully managed AWS service that enables you to run Amazon ECS \n        tasks on-premises, using the same APIs and tooling that you use in the AWS Cloud. With \n        Amazon ECS on AWS Outposts, you can deploy and manage containerized applications in a consistent \n        and familiar way, whether you're running them on-premises or in the cloud. AWS Outposts \n        is a fully managed service that extends AWS infrastructure, services, APIs, and tools \n        to your on-premises environments. With Amazon ECS on AWS Outposts, you can run Amazon ECS tasks on hardware \n        that is dedicated to your organization, without having to worry about the underlying infrastructure. \n        This can help to ensure that your applications are deployed in a secure and compliant manner, \n        while also enabling you to take advantage of the flexibility and scalability of the cloud. Amazon ECS on AWS Outposts works by deploying a set of AWS services and APIs to your on-premises \n        environment, which enables you to run Amazon ECS tasks on dedicated hardware. This \n        includes the Amazon ECS agent, which manages the deployment and lifecycle of containers, \n        and the AWS Outposts infrastructure, which provides a secure and compliant environment \n        for running containerized applications. With Amazon ECS on AWS Outposts, you can use the same \n        Amazon ECS APIs and tooling that you use in the AWS Cloud, making it easy to deploy \n        and manage containerized applications in a consistent and familiar way. This \n        can help to reduce the time and effort required for manual configuration and \n        management, and improve consistency and reliability across your hybrid infrastructure. Amazon ECS \n        on AWS Outposts also integrates with other AWS services, such as IAM, AWS CloudFormation, and Amazon ECR, to manage \n        your containerized applications. This can help to ensure that your applications are \n        secure, compliant, and integrated with other AWS services. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions AWS CodeDeploy Amazon Elastic Kubernetes Service"}, {"title": "Get started with Amazon EKS - Amazon EKS", "url": "https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html", "content": "Help improve this page Want to contribute to this user guide? Scroll to the bottom of this page\n   and select Edit this page on GitHub . Your contributions will help make our\n   user guide better for everyone. Help improve this page Want to contribute to this user guide? Scroll to the bottom of this page\n   and select Edit this page on GitHub . Your contributions will help make our\n   user guide better for everyone. Get started with Amazon EKS PDF RSS Make sure that you are set up to use Amazon EKS before going through the getting started\n        guides. For more information, see Set up to use Amazon EKS . There are two getting started guides available for creating a new Kubernetes cluster with\n        nodes in Amazon EKS: Get started with Amazon EKS \u2013\n            eksctl \u2013 This getting started guide helps you to install all of the required\n                resources to get started with Amazon EKS using eksctl , a simple command line\n                utility for creating and managing Kubernetes clusters on Amazon EKS. At the end of the\n                tutorial, you will have a running Amazon EKS cluster that you can deploy applications to.\n                This is the fastest and simplest way to get started with Amazon EKS. Get started with Amazon EKS \u2013 AWS Management Console and\n            AWS CLI \u2013 This getting started guide helps you to create all of the required\n                resources to get started with Amazon EKS using the AWS Management Console and AWS CLI. At the end of the\n                tutorial, you will have a running Amazon EKS cluster that you can deploy applications to.\n                In this guide, you manually create each resource required for an Amazon EKS cluster. The\n                procedures give you visibility into how each resource is created and how they\n                interact with each other. We also offer the following references: For a collection of hands-on tutorials, see EKS Cluster Setup on AWS Community . For code examples, see Code examples for Amazon EKS using AWS SDKs . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Quickstart Create your first cluster \u2013 eksctl"}, {"title": "Amazon Elastic Kubernetes Service - Overview of Deployment Options on AWS", "url": "https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/amazon-elastic-kubernetes-service.html", "content": "Amazon Elastic Kubernetes Service PDF RSS Amazon Elastic Kubernetes Service (Amazon EKS) is a fully-managed,\n      certified Kubernetes conformant service that simplifies the process of building,\n      securing, operating, and maintaining Kubernetes clusters on AWS.\n      Amazon EKS integrates with core AWS services such as CloudWatch,\n      Auto Scaling Groups, and IAM to provide a seamless experience for\n      monitoring, scaling, and load balancing your containerized\n      applications. Amazon EKS provides a scalable, highly-available control plane for Kubernetes \n      workloads. When you run applications on Amazon EKS, as with Amazon ECS, you can \n      choose to provide the underlying compute power for your containers \n      with Amazon EC2 instances or with AWS Fargate. Amazon VPC Lattice is a fully managed application networking service built directly \n      into the AWS networking infrastructure that you can use to connect, secure, \n      and monitor your services across multiple accounts and virtual private clouds (VPCs). \n      With Amazon EKS, you can leverage VPC Lattice through the use of the AWS Gateway API Controller, an \n      implementation of the Kubernetes Gateway API. Using VPC Lattice, you can set up cross-cluster \n      connectivity with standard Kubernetes semantics in a simple and consistent manner. You can use Amazon EKS with any of the following deployment options: Amazon EKS Distro \u2013 Amazon EKS Distro \n          is a distribution of the same open-source Kubernetes software and dependencies \n          deployed by Amazon EKS in the cloud. Amazon EKS Distro follows the same Kubernetes \n          version release cycle as Amazon EKS and is provided as an open-source project. \n          To learn more, see Amazon EKS Distro . Amazon EKS on AWS Outposts \u2013 AWS Outposts enables \n          native AWS services, infrastructure, and operating models in your on-premises \n          facilities. Amazon EKS on AWS Outposts, you can choose to run extended or local clusters. \n          With extended clusters, the Kubernetes control plane runs in an AWS Region and the \n          nodes run on AWS Outposts. With local clusters, the entire Kubernetes cluster runs locally \n          on AWS Outposts, including both the Kubernetes control plane and nodes. Amazon EKS Anywhere \u2013 \n          Amazon EKS Anywhere is a deployment option for Amazon EKS that enables you to easily create and \n          operate Kubernetes clusters on-premises. Both Amazon EKS and Amazon EKS Anywhere are built on the \n          Amazon EKS Distro. To learn more about Amazon EKS Anywhere, see Running Hybrid Container workloads with Amazon EKS Anywhere , Amazon EKS Anywhere Overview , and Comparing Amazon EKS Anywhere to \n            Amazon EKS . When choosing which deployment options to use for your Kubernetes cluster, consider the following: Table 5: Kubernetes deployment features Feature Amazon EKS Amazon EKS on AWS Outposts Amazon EKS Anywhere Amazon EKS Distro Hardware AWS-supplied AWS-supplied Supplied by you Supplied by you Deployment location AWS Cloud Your data center Your data center Your data center Kubernetes control plane location AWS Cloud AWS Cloud or your data center Your data center Your data center Kubernetes data plane location AWS Cloud Your data center Your data center Your data center Support AWS support AWS support AWS support OSS community support Table 6: Amazon EKS deployment features Capability Description Provision Amazon EKS provisions certain resources to support\n                containerized applications: Load Balancers, if needed Compute resources, or workers (Amazon EKS supports\n                    Windows and Linux) Application Container Instances, or pods Refer to Getting\n                Started with Amazon EKS for more details on\n                Amazon EKS cluster provisioning. Configure Amazon EKS supports customization of the compute\n                resources (workers) if you use Amazon EC2 instances to supply\n                compute power. Amazon EKS also supports customization of the\n                runtime conditions of the application containers\n                (pods). Refer to Worker\n                Nodes and Fargate\n                Pod Configuration documentation for more details. Deploy Amazon EKS supports the same deployment strategies as\n              Kubernetes. See Writing\n              a Kubernetes Deployment Spec -> Strategy for\n              more details. Scale Amazon EKS scales workers with Kubernetes\n              Cluster Autoscaler , and pods with Kubernetes\n              Horizontal Pod Autoscaler and Kubernetes Vertical Pod\n              Autoscaler. Amazon EKS also supports Karpenter , an open \n              source, flexible, high-performance Kubernetes cluster autoscaler to \n              help improve your application availability and cluster efficiency by rapidly \n              launching right-sized compute resources in response to changing application load. Monitor The Amazon EKS control plane logs provide audit and\n                diagnostic information directly to CloudWatch Logs. The\n                Amazon EKS control plane also integrates with AWS CloudTrail to record actions taken in Amazon EKS. Refer to Logging\n                and Monitoring Amazon EKS for more details. Amazon EKS allows organizations to leverage open source Kubernetes\n      tools and plugins, and can be a good choice for organizations\n      migrating to AWS with existing Kubernetes environments. The\n      following diagram illustrates Amazon EKS being used to manage a\n      general containerized application. Amazon EKS use case Amazon EKS Anywhere Amazon EKS Anywhere lets \n        you create and operate Kubernetes clusters on your own infrastructure. Amazon EKS Anywhere \n        builds on the strengths of Amazon EKS Distro and provides open-source software that\u2019s up to \n        date and patched so you can have an on-premises Kubernetes environment that\u2019s more reliable than a self-managed Kubernetes offering. Amazon EKS Anywhere creates a Kubernetes cluster on-premises to a chosen provider. Supported \n        providers include Bare Metal (via Tinkerbell), CloudStack, and vSphere. To manage that \n        cluster, you can run cluster create and delete commands from an Ubuntu or \n        Mac Administrative machine. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Amazon Elastic Container Service AWS App Runner"}, {"title": "What is Red Hat OpenShift Service on AWS? - Red Hat OpenShift Service on AWS", "url": "https://docs.aws.amazon.com/rosa/latest/userguide/what-is-rosa.html", "content": "What is Red Hat OpenShift Service on AWS? PDF RSS Red Hat OpenShift Service on AWS (ROSA) is a managed service that you can use to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS.\nROSA streamlines moving on-premises Red Hat OpenShift workloads to AWS, and offers tight integration with other AWS services. Features ROSA is jointly supported and operated by AWS and Red Hat.\nEach ROSA cluster comes with 24-hour Red Hat site reliability engineer (SRE) support for cluster management, backed by Red Hat\u2019s 99.95% uptime service-level agreement (SLA).\nFor more information about the service\u2019s support model, see Getting ROSA support . ROSA also provides the following features: Red Hat SRE-supported cluster installation, cluster maintenance, and cluster upgrades. AWS service integrations include AWS compute, database, analytics, machine learning, networking, and mobile. Run and scale the Kubernetes control plane across multiple AWS Availability Zones to ensure high availability. Operate clusters using OpenShift APIs and developer productivity tools, including Service Mesh, CodeReady Workspaces, and Serverless. Accessing ROSA You can define and configure your ROSA service deployments using the following interfaces. AWS ROSA console \u2014 Provides a web interface to enable the ROSA subscription and purchase a ROSA software contract. AWS Command Line Interface (AWS CLI) \u2014 Provides commands for a broad set of AWS services and is supported on Windows, macOS, and Linux. For more information, see AWS Command Line Interface . Red Hat OpenShift Red Hat Hybrid Cloud Console \u2014 Provides a web interface to create, update, and manage ROSA clusters, install cluster add-ons, and create and deploy applications to a ROSA cluster. ROSA CLI (rosa) \u2014 Provides commands to create, update, and manage ROSA clusters. OpenShift CLI (oc) \u2014 Provides commands to create applications and manage OpenShift Container Platform projects. Knative CLI (kn) - Provides commands that can be used to interact with OpenShift Serverless components, such as Knative Serving and Eventing. Pipelines CLI (tkn) - Provides commands to interact with OpenShift Pipelines using the terminal. opm CLI - Provides commands that help Operator developers and cluster administrators create and maintain OpenShift Operator catalogs from the terminal. Operator SDK CLI - Provides commands that an Operator developer can use to build, test, and deploy an OpenShift operator. How to get started with ROSA The following summarizes the getting started process for ROSA.\nFor detailed getting started instructions, see Get started with ROSA . AWS Management Console/AWS CLI Configure permissions for AWS services that ROSA relies on to deliver service functionality.\nFor more information, see Prerequisites . Install and configure the latest AWS CLI tool.\nFor more information, see Installing our updating the latest version of the AWS CLI in the AWS CLI User Guide. Enable ROSA in the ROSA console . Red Hat Hybrid Cloud Console/ROSA CLI Download the latest version of the ROSA CLI and OpenShift CLI from the Red Hat Hybrid Cloud Console .\nFor more information, see Getting started with the ROSA CLI in the Red Hat documentation. Create ROSA clusters in the Red Hat Hybrid Cloud Console or with the ROSA CLI. When your cluster is ready, configure an identity provider to grant user access to the cluster. Deploy and manage workloads on your ROSA cluster the same way that you would with any other OpenShift environment. Pricing The total cost of ROSA consists of two components: ROSA service fees and AWS infrastructure fees. For more information about pricing, see Red Hat OpenShift Service on AWS Pricing . ROSA service fees By default, ROSA service fees accrue on demand at an hourly rate per 4 vCPU used by worker nodes.\nService fees are uniform across all supported AWS standard Regions.\nIn addition to the worker node service fee, ROSA with hosted control planes (HCP) clusters incur an hourly cluster fee. ROSA offers 1-year and 3-year service fee contracts that you can purchase for savings on the on-demand service fees for worker nodes.\nFor more information, see Purchasing a ROSA contract . AWS infrastructure fees AWS infrastructure fees apply to the underlying worker nodes, infrastructure nodes, control plane nodes, storage, and network resources hosted on AWS global infrastructure.\nAWS infrastructure fees vary by AWS Region. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Responsibilities"}, {"title": "Getting started with App Runner - AWS App Runner", "url": "https://docs.aws.amazon.com/apprunner/latest/dg/getting-started.html", "content": "Getting started with App Runner PDF AWS App Runner is an AWS service that provides a fast, simple, and cost-effective way to turn an existing container image or source code directly into a\n    running web service in the AWS Cloud. This tutorial covers how you can use AWS App Runner to deploy your application to an App Runner service. It walks through configuring the source code and\n    deployment, the service build, and the service runtime. It also shows how to deploy a code version, make a configuration change, and view logs. Last, the\n    tutorial shows how to clean up the resources that you created while following the tutorial's procedures. Topics Prerequisites Step 1: Create an App Runner service Step 2: Change your service code Step 3: Make a configuration change Step 4: View logs for your service Step 5: Clean up What's next Prerequisites Before you start the tutorial, be sure to take the following actions: Complete the setup steps in Setting up for App Runner . Decide if you'd like to work with either a GitHub repository or a Bitbucket repository. To work with a Bitbucket, first create a Bitbucket account, if you don't already have one. If you're new\n              to Bitbucket, see Getting started with\n                Bitbucket in the Bitbucket Cloud Documentation . To work with GitHub, create a GitHub account, if you don't already have one. If you're new to GitHub, see Getting started with GitHub in the GitHub\n                Docs . Note You can create connections to multiple repository providers from your account. So if you'd like to walk through deploying from both a GitHub\n                and a Bitbucket repository, you can repeat this procedure. The next time through create a new App Runner service and create a new account connection\n                for the other repository provider. Create a repository in your repository provider account. This tutorial uses the repository name python-hello . Create files in\n          the root directory of the repository, with the names and content specified in the following examples. Example requirements.txt pyramid == 2 . 0 Example server.py from wsgiref.simple_server import make_server from pyramid.config import Configurator from pyramid.response import Response import os def hello_world ( request ): name = os.environ.get( 'NAME' ) if name == None or len (name) == 0 :\n        name = \"world\" message = \"Hello, \" + name + \"!\\n\" return Response(message) if __name__ == '__main__' :\n    port = int (os.environ.get( \"PORT\" )) with Configurator() as config:\n        config.add_route( 'hello' , '/' )\n        config.add_view(hello_world, route_name= 'hello' )\n        app = config.make_wsgi_app()\n    server = make_server( '0.0.0.0' , port, app)\n    server.serve_forever() Files for the python-hello example repository Example requirements.txt pyramid == 2 . 0 Example server.py from wsgiref.simple_server import make_server from pyramid.config import Configurator from pyramid.response import Response import os def hello_world ( request ): name = os.environ.get( 'NAME' ) if name == None or len (name) == 0 :\n        name = \"world\" message = \"Hello, \" + name + \"!\\n\" return Response(message) if __name__ == '__main__' :\n    port = int (os.environ.get( \"PORT\" )) with Configurator() as config:\n        config.add_route( 'hello' , '/' )\n        config.add_view(hello_world, route_name= 'hello' )\n        app = config.make_wsgi_app()\n    server = make_server( '0.0.0.0' , port, app)\n    server.serve_forever() Step 1: Create an App Runner service In this step, you create an App Runner service based on the example source code repository that you created on GitHub or Bitbucket as part of Prerequisites . The example contains a simple Python website. These are the main steps you take to create a service: Configure your source code. Configure source deployment. Configure application build. Configure your service. Review and confirm. The following diagram outlines the steps for creating an App Runner service: To create an App Runner service based on a source code repository Configure your source code. Open the App Runner console , and in the Regions list, select your AWS Region. If the AWS account doesn't have any App Runner services yet, the console home page is displayed. Choose Create an App Runner service . If the AWS account has existing services, the Services page with a list of your services is displayed. Choose Create\n      service . On the Source and deployment page, in the Source section, for Repository type ,\n              choose Source code repository . Select a Provider Type . Choose either GitHub or Bitbucket . Next choose Add new . If prompted, provide your GitHub or Bitbucket credentials. Choose the next set of steps based on the Provider type you previously selected. Note The following steps to install the AWS connector for GitHub to your GitHub account are one-time steps. You can reuse the connection for\n                creating multiple App Runner services based on repositories in this account. When you have an existing connection, choose it and skip to\n                repository selection. The same applies to the AWS connector for your Bitbucket account. If you're using both GitHub and Bitbucket as source code repositories for\n                your App Runner services, you'll need to install one AWS Connector for each provider. You can then reuse each connector for creating more App Runner\n                services. For GitHub , follow these steps. On the next screen, enter a Connection Name . If this your first time using GitHub with App Runner, select Install another . In the AWS Connector for GitHub dialog box, if prompted, choose your GitHub account name. If prompted to authorize the AWS Connector for GitHub, choose Authorize AWS Connections . In the Install AWS Connector for GitHub dialog box,Choose Install . Your account name appears as the selected GitHub account/organization . You can now choose a repository in your\n                      account. For Repository , choose the example repository you created, python-hello . For Branch , choose the default branch name of your repository (for example, main ). Leave Source directory with the default value. The directory defaults to the repository root. You stored your\n                      source code in the repository root directory in the previous Prerequisites steps. For Bitbucket , follow these steps. On the next screen, enter a Connection Name . If this your first time using Bitbucket with App Runner, select Install another . In the AWS CodeStar requests access dialog box, you can select your workspace and grant access to AWS CodeStar for\n                      Bitbucket integration. Select your workspace, then select Grant access . Next you'll be redirected to the AWS console. Verify that the Bitbucket application is set to the correct Bitbucket workspace and\n                        select Next . For Repository , choose the example repository you created, python-hello . For Branch , choose the default branch name of your repository (for example, main ). Leave Source directory with the default value. The directory defaults to the repository root. You stored your\n                      source code in the repository root directory in the previous Prerequisites steps. Configure your deployments: In the Deployment settings section, choose Automatic , and then choose Next . Note With automatic deployment, each new commit to your repository source directory automatically deploys a new version of your service. Configure application build. On the Configure build page, for Configuration file , choose Configure all settings\n                here . Provide the following build settings: Runtime \u2013 Choose Python 3 . Build command \u2013 Enter pip install -r requirements.txt . Start command \u2013 Enter python server.py . Port \u2013 Enter 8080 . Choose Next . Note The Python 3 runtime builds a Docker image using a base Python 3 image and your example Python code. It then launches a service that runs a\n            container instance of this image. Configure your service. On the Configure service page, in the Service settings section, enter a service name. Under Environment variables , select Add environment variable . Provide the following\n              values for the environment variable. Source \u2013 Choose Plain text Environment variable name \u2013 NAME Environment variable value \u2013 any name (for example, your first name). Note The example application reads the name you set in this environment variable and displays the name on its webpage. Choose Next . On the Review and create page, verify all the details you've entered, and then choose Create and\n          deploy . If the service is successfully created, the console shows the service dashboard, with a Service overview of the new\n          service. Verify that your service is running. On the service dashboard page, wait until the service Status is Running . Choose the Default domain value\u2014it's the URL to the website of your service. Note To augment the security of your App Runner applications, the *.awsapprunner.com domain is registered in the Public Suffix List (PSL) . For further security, we recommend that you use cookies with a __Host- prefix if you ever need to set sensitive cookies in the default domain name for your App Runner applications. \n    This practice will help to defend your domain against cross-site request\n    forgery attempts (CSRF). For more information see the Set-Cookie page in the Mozilla Developer Network. A webpage displays: Hello, your name ! Step 2: Change your service code In this step, you make a change to your code in the repository source directory. The App Runner CI/CD capability automatically builds and deploys the change to your\n      service. To make a change to your service code Navigate to your example repository. Edit the file named server.py . In the expression assigned to the variable message , change the text Hello to Good morning . Save and commit your changes to the repository. The following steps illustrate changing the service code in a GitHub repository. Navigate to your example GitHub repository. Choose the file name server.py to navigate to that file. Choose Edit this file (the pencil icon). In the expression assigned to the variable message , change the text Hello to Good morning . Choose Commit changes . The new commit starts to deploy for your App Runner service. On the service dashboard page, the service Status changes to Operation in progress . Wait for the deployment to end. On the service dashboard page, the service Status should change back to Running . Verify that the deployment is successful: refresh the browser tab where the webpage of your service is displayed. The page now displays the modified message: Good morning, your name ! Step 3: Make a configuration change In this step, you make a change to the NAME environment variable value, to demonstrate a service configuration change. To change an environment variable value Open the App Runner console , and in the Regions list, select your AWS Region. In the navigation pane, choose Services , and then choose your App Runner service. The console displays the service dashboard with a Service overview . On the service dashboard page, choose the Configuration tab. The console displays your service configuration settings in several sections. In the Configure service section, choose Edit . For the environment variable with the key NAME , change the value to a different name. Choose Apply changes . App Runner starts the update process. On the service dashboard page, the service Status changes to Operation in\n            progress . Wait for the update to end. On the service dashboard page, the service Status should change back to Running . Verify that the update is successful: refresh the browser tab where the webpage of your service is displayed. The page now displays the modified name: Good morning, new name ! Step 4: View logs for your service In this step, you use the App Runner console to view logs for your App Runner service. App Runner streams logs to Amazon CloudWatch Logs (CloudWatch Logs) and displays them on your service's\n      dashboard. For information about App Runner logs, see Viewing App Runner logs streamed to CloudWatch Logs . To view logs for your service Open the App Runner console , and in the Regions list, select your AWS Region. In the navigation pane, choose Services , and then choose your App Runner service. The console displays the service dashboard with a Service overview . On the service dashboard page, choose the Logs tab. The console displays a few types of logs in several sections: Event log \u2013 Activity in the lifecycle of your App Runner service. The console displays the latest events. Deployment logs \u2013 Source repository deployments to your App Runner service. The console displays a separate log\n          stream for each deployment. Application logs \u2013 The output of the web application that's deployed to your App Runner service. The console\n          combines the output from all running instances into a single log stream. To find specific deployments, scope down the deployment log list by entering a search term. You can search for any value that appears in the\n      table. To view a log's content, choose View full log (event log) or the log stream name (deployment and application logs). Choose Download to download a log. For a deployment log stream, select a log stream first. Choose View in CloudWatch to open the CloudWatch console and use its full capabilities to explore your App Runner service logs. For a deployment\n      log stream, select a log stream first. Note The CloudWatch console is particularly useful if you want to view application logs of specific instances instead of the combined application log. Step 5: Clean up You've now learned how to create an App Runner service, view logs, and make some changes. In this step, you delete the service to remove resources that you\n      don't need anymore. To delete your service On the service dashboard page, choose Actions , and then choose Delete service . In the confirmation dialog, enter the requested text, and then choose Delete . Result: The console navigates to the Services page. The service that you just deleted shows a status of DELETING . A short time later it disappears from the list. Consider also deleting the GitHub and Bitbucket connections that you created as part of this tutorial. For more information, see Managing App Runner connections . What's next Now that you've deployed your first App Runner service, learn more in the following topics: App Runner architecture and concepts \u2013 The architecture, main concepts, and AWS resources related to App Runner. Image-based service and Code-based service \u2013 The two types of application source that App Runner can deploy. Developing application code for App Runner \u2013 Things you should know when developing or migrating application code for deployment to App Runner. Using the App Runner console \u2013 Manage and monitor your service using the App Runner console. Managing your App Runner service \u2013 Manage the lifecycle of your App Runner service. Observability for your App Runner service \u2013 Get visibility into your App Runner service operations by monitoring metrics, reading logs, handling events, tracking\n          service action calls, and tracing application events like HTTP calls. App Runner configuration file \u2013 A configuration-based way to specify options for the build and runtime\n          behavior of your App Runner service. The App Runner API \u2013 Use the App Runner application programming interface (API) to create, read, update, and delete App Runner resources. Security in App Runner \u2013 The different ways that AWS and you ensure cloud security while you use App Runner and other services. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Setting up Architecture and concepts"}, {"title": "AWS Lambda Documentation", "url": "https://docs.aws.amazon.com/lambda/", "content": "No main content found."}, {"title": "Build and test Docker images for Lightsail container services - Amazon Lightsail", "url": "https://docs.aws.amazon.com/lightsail/latest/userguide/amazon-lightsail-creating-container-images.html", "content": "Build and test Docker images for\n      Lightsail container services PDF With Docker, you can build, run, test, and deploy distributed applications that are based on\n    containers. Amazon Lightsail container services use Docker container images in deployments to\n    launch containers. In this guide, we show you how to create a container image on your local machine using a\n    Dockerfile. After your image is created, you can then push it to your Lightsail container\n    service to deploy it. To complete the procedures in this guide you should possess a basic understanding of what\n    Docker is and how it works. For more information about Docker, see What is Docker? and the Docker overview . Contents Step 1: Complete the\n          prerequisites Step 2: Create a\n          Dockerfile and build a container image Step 3: Run your new\n          container image (Optional) Step 4: Clean up\n          the containers running on your local machine Next steps after\n          creating container images Step 1: Complete the\n        prerequisites Before you get started, you must install the software required to create containers and\n      then push them to your Lightsail container service. For example, you must install and use\n      Docker to create and build your container images that you can then use with your Lightsail\n      container service. For more information, see Installing software to manage container images for your Amazon Lightsail container\n        services . Step 2: Create a Dockerfile and\n        build a container image Complete the following procedure to create a Dockerfile, and build a mystaticwebsite Docker container image from it. The container image will be for\n      a simple static website hosted on an Apache web server on Ubuntu. Create a mystaticwebsite folder on your local machine where you will\n          store your Dockerfile. Create a Dockerfile in the folder you just created. The Dockerfile does not use a file extension, such as .TXT . The full file\n          name is Dockerfile . Copy one of the following code blocks depending on how you want to configure your\n          container image, and paste it into your Dockerfile: If you want to create a simple static website container\n                image with a Hello World message , then copy the following code block and\n              paste it into your Dockerfile. This code sample uses the Ubuntu 18.04 image. The RUN instructions updates the package caches, and installs and\n              configures Apache, and prints a Hello World message to the web server's document root.\n              The EXPOSE instruction exposes port 80 on the container, and the CMD instruction starts the web server. FROM ubuntu: 18.04 # Install dependencies RUN apt-get update && \\\n apt-get -y install apache2 # Write hello world message RUN echo 'Hello World!' > /var/www/html/index.html # Open port 80 EXPOSE 80 # Start Apache service CMD [ \"/usr/sbin/apache2ctl\" , \"-D\" , \"FOREGROUND\" ] If you want to use your own set of HTML files for your\n                static website container image , create an html folder in the\n              same folder where you store your Dockerfile. Then put your HTML files in that\n              folder. After your HTML files are in the html folder, copy the following code\n              block and paste into to your Dockerfile. This code sample uses the Ubuntu 18.04 image.\n              The RUN instructions updates the package caches, and installs and\n              configures Apache. The COPY instruction copies the contents of the html\n              folder to the web server's document root. The EXPOSE instruction exposes\n              port 80 on the container, and the CMD instruction starts the web\n              server. FROM ubuntu: 18.04 # Install dependencies RUN apt-get update && \\\n apt-get -y install apache2 # Copy html directory files COPY html /var/www/html/ # Open port 80 EXPOSE 80 CMD [ \"/usr/sbin/apache2ctl\" , \"-D\" , \"FOREGROUND\" ] Open a command prompt or terminal window and change the directory to the folder in\n          which you are storing your Dockerfile. Enter the following command to build your container image using the Dockerfile in the\n          folder. This command builds a new Docker container image named mystaticwebsite . docker build -t mystaticwebsite . You should see a message that confirms your image was successfully built. Enter the following command to view the container images on your local machine. docker images --filter reference=mystaticwebsite You should see a result similar to the following example, showing the new container\n          image created. Your newly built container image is ready to be tested by using it to run a new\n          container on your local machine. Continue to the next Step 3: Run your new\n            container image section of this guide. Step 3: Run your new container\n        image Complete the following steps to run the new container image you created. In a command prompt or terminal window, enter the following command to run the\n          container image that you built in the previous Step 2: Create a\n            Dockerfile and build a container image section of this guide. The -p\n            8080:80 option maps the exposed port 80 on the container to port 8080 on your\n          local machine. The -d option specifies that the container should run in\n          detached mode. docker container run -d -p 8080 : 80 --name mystaticwebsite mystaticwebsite:latest Enter the following command to view your running containers. docker container ls - a You should see a result similar to the following example, showing the new running\n          container. To confirm that the container is up and running, open a new browser window and browse\n          to http://localhost:8080 . You should see a message similar to the following\n          example. This confirms that your container is up and running on your local machine. Your newly built container image is ready to be pushed to your Lightsail account so\n          that you can deploy it to your Lightsail container service. For more information, see Pushing and managing container\n            images on your Amazon Lightsail container services . (Optional) Step 4: Clean up the containers\n        running on your local machine Now that you've created a container image that you can push to your Lightsail container\n      service, it's time to clean up the containers that are running on your local machine as a\n      result of following the procedures in this guide. Complete the following steps to clean up the containers running on your local\n      machine: Run the following command to view the containers that are running on your local\n          machine. docker container ls - a You should see a result similar to the following, which lists the names of the\n          containers running on your local machine. Run the following command to remove the running container that you created earlier in\n          this guide. This forces the container to be stopped, and permanently deletes it. docker container rm < ContainerName > --force In the command, replace <ContainerName> with the name of the container you want\n          to stop, and delete. Example: docker container rm mystaticwebsite --force The container that was created as a result of this guide should now be deleted. Next steps after creating container\n        images After you create your container images, push them to your Lightsail container service\n      when you're ready to deploy them. For more information, see Manage Lightsail container service\n        images . Topics Manage container images Install container services plugin ECR private repository access Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Create a container Manage container images"}, {"title": "Best practices for AWS Batch - AWS Batch", "url": "https://docs.aws.amazon.com/batch/latest/userguide/best-practices.html", "content": "Best practices for AWS Batch PDF RSS You can use AWS Batch to run a variety of demanding computational workloads at scale without managing a complex\n  architecture. AWS Batch jobs can be used in a wide range of use cases in areas such as epidemiology, gaming, and machine\n  learning. This topic covers the best practices to consider while using AWS Batch and guidance on how to run and optimize your\n  workloads when using AWS Batch. Topics When to use AWS Batch Checklist to run at scale Optimize containers and AMIs Choose the right compute environment resource Amazon EC2 On-Demand or Amazon EC2 Spot Use Amazon EC2 Spot best practices for AWS Batch Common errors and troubleshooting Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Manage tags using the CLI or API When to use AWS Batch"}, {"title": "Amazon Elastic Container Registry Documentation", "url": "https://docs.aws.amazon.com/ecr/", "content": "No main content found."}, {"title": "AWS Cloud Map Documentation", "url": "https://docs.aws.amazon.com/cloud-map/", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=a0d18259-7974-4e8f-8382-0a4be53f4374&topic_url=https://docs.aws.amazon.com/en_us/decision-guides/latest/containers-on-aws-how-to-choose/choosing-aws-container-service.html", "content": "No main content found."}, {"title": "Amazon Bedrock or Amazon SageMaker? - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html?icmpid=docs_homepage_featuredsvcs", "content": "Amazon Bedrock or Amazon SageMaker? PDF RSS Understand the differences and pick the one that's right for you Purpose Understand the differences between Amazon Bedrock and Amazon SageMaker, and determine which service is\n       the best fit for your needs. Last updated August 21, 2024 Covered services Amazon Bedrock Amazon SageMaker Introduction Amazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and\n    generative AI applications. It\u2019s helpful to understand how these services work together to form\n    a generative AI stack, including: Generative AI-powered services such as Amazon Q, which leverages large language models\n        (LLMs) and other foundation models (FMs). Tools for building applications with LLMs and other FMs, including Amazon Bedrock. Infrastructure for model training and inference, such as Amazon SageMaker and specialized\n        hardware. When considering which generative AI services you want to use, two services are often\n    considered alongside one another: Amazon Bedrock Choose Amazon Bedrock if you primarily need to use pre-trained foundation models for\n        inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is\n        a fully managed service for building generative AI applications with support for popular\n        foundation models, including Anthropic\n          Claude , Cohere Command\n          & Embed , AI21 Labs\n          Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon\n        Titan . Supported FMs are updated on a regular basis. Use Amazon Bedrock to build generative AI applications with security, privacy, and responsible\n        AI\u2014regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API\n        access, so you can use different foundation models, and upgrade to the latest model\n        versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom\n          models . Use Amazon Bedrock Studio (in\n        preview), which is a new SSO-enabled web interface that your developers can use to work with\n        large language models (LLMs) and other foundation models (FMs), collaborate on projects, and\n        iterate on generative AI applications. Amazon SageMaker Amazon SageMaker is a\n        fully managed service designed to help you build, train, and deploy machine learning models\n        at scale. This includes building FMs from scratch, using tools like notebooks, debuggers,\n        profilers,\u00a0pipelines, and MLOps. Consider SageMaker when you have use cases that can bene\ufb01t from\n        extensive training, \ufb01ne-tuning, and customization of foundation models. It can also help you\n        through the potentially challenging task of evaluating which FM is the best \ufb01t for your use\n        case. Use SageMaker\u2019s integrated development environment (IDE) to build, train, and deploy\n        FMs.\u00a0SageMaker offers access to hundreds of pretrained models, including publicly\u00a0available FMs. For more information about how Amazon Bedrock and SageMaker fit into Amazon\u2019s generative AI services and\n    solutions, see the generative AI\n      decision guide . While both Amazon Bedrock and Amazon SageMaker enable the development of ML and generative AI\n    applications, they serve different purposes. This guide will help you understand which of these\n    services is the best fit for your needs, including scenarios in which both services can be used\n    together to build generative AI applications. Here's a high-level view of the key differences between these services to get you started. Category Amazon Bedrock Amazon SageMaker Use Cases Ideal for integration of AI capabilities into\n            applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning\n            expertise Optimized for data scientists, machine learning engineers, and developers Customization You'll primarily use pre-trained models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other\n            services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for\n            building and optimizing models Differences between Amazon Bedrock and SageMaker Let's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker. Use cases Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. Target users Amazon Bedrock and Amazon SageMaker support different targeted users based on their level of expertise\n          and knowledge of machine learning and artificial intelligence. Amazon Bedrock Amazon Bedrock offers a more accessible and straightforward way to integrate AI\n              functionality into your projects. It\u2019s appropriate for a broad audience, which\n              includes developers and businesses, that has limited experience in building and\n              training machine learning models, but wants to use AI to enhance their applications or\n              workflows. Amazon SageMaker SageMaker is predominantly for data scientists, machine learning engineers, and\n              developers who possess the necessary skills and knowledge to build, train, and deploy\n              custom machine learning models. Use SageMaker if you are well-versed in data science and\n              machine learning concepts, and require a platform that provides you with the tools and\n              flexibility to create models tailored to your specific needs. Customization Amazon Bedrock and Amazon SageMaker offer different levels of customization capabilities that you can\n          tailor to your specific needs and expertise. Amazon Bedrock Amazon Bedrock provides pre-trained AI models that you can integrate into applications,\n              with limited customization. You have access to a set of API calls that you use to\n              enter data and receive predictions from these pre-trained models. While this approach\n              drastically simplifies the process of incorporating AI capabilities into applications,\n              it also means that you have less control over the underlying models, unless you\n              customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized\n              for common AI tasks and are designed to work well for a wide range of use cases, but\n              they may not be suitable for highly specialized or niche requirements. Amazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R,\n              Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan\n              Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n              (Oregon) AWS Region. The list of supported FMs is updated on an ongoing\n              basis. Customize models for specific tasks and use cases, including FM fine-tuning\n              and pre-training. Bring your own customized model with custom model\n                import (in preview). Amazon SageMaker Amazon SageMaker provides extensive customization options, giving you full control over the\n              entire machine learning workflow. With SageMaker, you can fine-tune every aspect of your\n              models, from data preprocessing and feature engineering to model architecture and\n              hyperparameter optimization. By using this level of customization, you can create\n              highly specialized models that are tailored to your unique business requirements. SageMaker\n              supports a wide range of popular machine learning frameworks, such as TensorFlow,\n              PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for\n              building and training models. Use Amazon SageMaker JumpStart to evaluate, compare, and select FMs based on\n              pre-defined quality and responsibility. Choose which FM to use with Amazon SageMaker\n                Clarify . Use SageMaker Clarify to create model evaluation jobs, that you use to\n              evaluate and compare model quality and responsibility metrics for text-based\n              foundation models from JumpStart. Generate predictions using Amazon SageMaker Canvas , without needing to\n              write any code.  Use SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n              deploy language models. This blog post describes how you can use them to optimize customer\n              interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock\n              and Amazon SageMaker JumpStart. Pricing Amazon Bedrock and Amazon SageMaker have different pricing\n        models that reflect their target users and the services they\n        provide. Amazon Bedrock Amazon Bedrock employs a simple pricing\n                model based on the number of API calls made to the service. You pay a fixed\n              price per API call, which includes the cost of running the pre-trained models and any\n              associated data processing. This straightforward pricing structure makes it more\n              efficient for you to estimate and control your costs, as you pay only for the actual\n              usage of the service. Amazon Bedrock's pricing model is particularly well-suited for\n              applications with predictable workloads, or for cases where you want more transparency\n              in your AI-related expenses. Amazon SageMaker SageMaker follows a pay-as-you-go pricing\n                model based on the usage of compute resources, storage, and other services\n              consumed during the machine learning process. You\u2019re charged for the instances that\n              you use to build, train, and deploy you models, with prices varying depending on the\n              instance type and size. Additionally, you incur costs for data storage, data transfer,\n              and other associated services like data labeling and model monitoring. This pricing\n              model provides flexibility and allows you to optimize costs based on your specific\n              requirements. However, it also means that costs can vary and may require careful\n              management, especially for resource-intensive projects. Integration Amazon Bedrock and Amazon SageMaker offer different approaches to integrating machine learning models\n          into applications, catering to your specific needs and expertise. Amazon Bedrock Amazon Bedrock simplifies the integration process by providing pre-trained models that you can\n              access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and\n              receive predictions from the models without needing to manage the underlying\n              infrastructure. This approach significantly reduces the complexity and time required\n              to integrate AI capabilities into applications, making it more accessible to\n              developers with limited machine learning expertise. However, this ease of integration\n              comes at the cost of limited customization options, as you\u2019re restricted to the\n              pre-trained models and APIs provided by Amazon Bedrock. Amazon SageMaker SageMaker provides a comprehensive platform for building, training, and deploying custom\n              machine learning models. However, integrating these models into applications requires\n              more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker SDK or\n              API to access the trained models and build the necessary infrastructure to expose them\n              as endpoints. This process involves creating and configuring API Gateway, Lambda\n              functions, and other AWS services to enable communication between the application\n              and the deployed model. While SageMaker provides tools and templates to simplify this\n              process, it still requires a deeper understanding of AWS services and machine\n              learning model deployment. Expertise required Amazon Bedrock and Amazon SageMaker are optimized for different levels of machine learning\n          expertise. Amazon Bedrock Amazon Bedrock is more accessible to a broader range of users, including developers and\n              businesses with limited machine learning expertise. By providing pre-trained models\n              that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away\n              much of the complexity associated with building and deploying machine learning models.\n              You don't need to worry about data preprocessing, model selection, or infrastructure\n              management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus\n              on integrating AI capabilities into your applications without needing to invest\n              significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker If you have deeper expertise in data science and machine learning, SageMaker provides a\n              powerful and flexible platform for building, training, and deploying custom models.\n              While SageMaker aims to simplify the machine learning workflow, it still requires a\n              significant level of technical expertise to take full advantage of its capabilities.\n              You\u2019ll benefit from being proficient in programming languages like Python, along with\n              a deep understanding of machine learning concepts, such as data preprocessing, model\n              selection, and hyperparameter tuning. Additionally, you should be comfortable working\n              with various AWS services and managing the infrastructure required to deploy and\n              integrate their models. As a result, SageMaker may have a steeper learning curve if you\u2019re\n              new to machine learning or have limited experience with AWS. anchor anchor anchor anchor anchor anchor Use cases Target users Customization Pricing Integration Expertise required Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. The choice between Amazon Bedrock and Amazon SageMaker is not always mutually exclusive. In some\n    cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to\n    quickly prototype and deploy a foundation model, and then use SageMaker to further refine and\n    optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker together to optimize\n    customer interaction by working with your own datasets (such as your product FAQs. Ultimately, the decision between Amazon Bedrock and Amazon SageMaker depends on your specific\n    requirements. Evaluating these factors can help you make an informed decision and choose the\n    service that is most suitable for your needs. For more information about Amazon\u2019s generative AI services and solutions, see the generative AI\n      decision guide . Use Now that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker, you can\n    select the service that meets your needs, and use the following information  to help you get\n    started using each of them. Amazon Bedrock What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock\n            Studio? Describes how to use this web app to prototype apps that use Amazon Bedrock models and\n              features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio This blog post describes how you can build applications using a wide array of top\n              performing models. It then explains how to evaluate and share your generative AI apps\n              within Amazon Bedrock Studio. Read the blog Building an app with Amazon Bedrock\n            Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Describes how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Describes how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and\n              RStudio on SageMaker. Explore the guide Get started with Amazon SageMaker\n            JumpStart Explore SageMaker JumpStart solution templates that set up\n          infrastructure for common use cases, and executable example\n          notebooks for machine learning with SageMaker. Explore the guide anchor anchor anchor Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=a0d18259-7974-4e8f-8382-0a4be53f4374&topic_url=https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html?icmpid=docs_homepage_featuredsvcs", "content": "No main content found."}, {"title": "Amazon Bedrock or Amazon SageMaker? - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html", "content": "Amazon Bedrock or Amazon SageMaker? PDF RSS Understand the differences and pick the one that's right for you Purpose Understand the differences between Amazon Bedrock and Amazon SageMaker, and determine which service is\n       the best fit for your needs. Last updated August 21, 2024 Covered services Amazon Bedrock Amazon SageMaker Introduction Amazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and\n    generative AI applications. It\u2019s helpful to understand how these services work together to form\n    a generative AI stack, including: Generative AI-powered services such as Amazon Q, which leverages large language models\n        (LLMs) and other foundation models (FMs). Tools for building applications with LLMs and other FMs, including Amazon Bedrock. Infrastructure for model training and inference, such as Amazon SageMaker and specialized\n        hardware. When considering which generative AI services you want to use, two services are often\n    considered alongside one another: Amazon Bedrock Choose Amazon Bedrock if you primarily need to use pre-trained foundation models for\n        inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is\n        a fully managed service for building generative AI applications with support for popular\n        foundation models, including Anthropic\n          Claude , Cohere Command\n          & Embed , AI21 Labs\n          Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon\n        Titan . Supported FMs are updated on a regular basis. Use Amazon Bedrock to build generative AI applications with security, privacy, and responsible\n        AI\u2014regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API\n        access, so you can use different foundation models, and upgrade to the latest model\n        versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom\n          models . Use Amazon Bedrock Studio (in\n        preview), which is a new SSO-enabled web interface that your developers can use to work with\n        large language models (LLMs) and other foundation models (FMs), collaborate on projects, and\n        iterate on generative AI applications. Amazon SageMaker Amazon SageMaker is a\n        fully managed service designed to help you build, train, and deploy machine learning models\n        at scale. This includes building FMs from scratch, using tools like notebooks, debuggers,\n        profilers,\u00a0pipelines, and MLOps. Consider SageMaker when you have use cases that can bene\ufb01t from\n        extensive training, \ufb01ne-tuning, and customization of foundation models. It can also help you\n        through the potentially challenging task of evaluating which FM is the best \ufb01t for your use\n        case. Use SageMaker\u2019s integrated development environment (IDE) to build, train, and deploy\n        FMs.\u00a0SageMaker offers access to hundreds of pretrained models, including publicly\u00a0available FMs. For more information about how Amazon Bedrock and SageMaker fit into Amazon\u2019s generative AI services and\n    solutions, see the generative AI\n      decision guide . While both Amazon Bedrock and Amazon SageMaker enable the development of ML and generative AI\n    applications, they serve different purposes. This guide will help you understand which of these\n    services is the best fit for your needs, including scenarios in which both services can be used\n    together to build generative AI applications. Here's a high-level view of the key differences between these services to get you started. Category Amazon Bedrock Amazon SageMaker Use Cases Ideal for integration of AI capabilities into\n            applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning\n            expertise Optimized for data scientists, machine learning engineers, and developers Customization You'll primarily use pre-trained models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other\n            services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for\n            building and optimizing models Differences between Amazon Bedrock and SageMaker Let's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker. Use cases Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. Target users Amazon Bedrock and Amazon SageMaker support different targeted users based on their level of expertise\n          and knowledge of machine learning and artificial intelligence. Amazon Bedrock Amazon Bedrock offers a more accessible and straightforward way to integrate AI\n              functionality into your projects. It\u2019s appropriate for a broad audience, which\n              includes developers and businesses, that has limited experience in building and\n              training machine learning models, but wants to use AI to enhance their applications or\n              workflows. Amazon SageMaker SageMaker is predominantly for data scientists, machine learning engineers, and\n              developers who possess the necessary skills and knowledge to build, train, and deploy\n              custom machine learning models. Use SageMaker if you are well-versed in data science and\n              machine learning concepts, and require a platform that provides you with the tools and\n              flexibility to create models tailored to your specific needs. Customization Amazon Bedrock and Amazon SageMaker offer different levels of customization capabilities that you can\n          tailor to your specific needs and expertise. Amazon Bedrock Amazon Bedrock provides pre-trained AI models that you can integrate into applications,\n              with limited customization. You have access to a set of API calls that you use to\n              enter data and receive predictions from these pre-trained models. While this approach\n              drastically simplifies the process of incorporating AI capabilities into applications,\n              it also means that you have less control over the underlying models, unless you\n              customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized\n              for common AI tasks and are designed to work well for a wide range of use cases, but\n              they may not be suitable for highly specialized or niche requirements. Amazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R,\n              Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan\n              Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n              (Oregon) AWS Region. The list of supported FMs is updated on an ongoing\n              basis. Customize models for specific tasks and use cases, including FM fine-tuning\n              and pre-training. Bring your own customized model with custom model\n                import (in preview). Amazon SageMaker Amazon SageMaker provides extensive customization options, giving you full control over the\n              entire machine learning workflow. With SageMaker, you can fine-tune every aspect of your\n              models, from data preprocessing and feature engineering to model architecture and\n              hyperparameter optimization. By using this level of customization, you can create\n              highly specialized models that are tailored to your unique business requirements. SageMaker\n              supports a wide range of popular machine learning frameworks, such as TensorFlow,\n              PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for\n              building and training models. Use Amazon SageMaker JumpStart to evaluate, compare, and select FMs based on\n              pre-defined quality and responsibility. Choose which FM to use with Amazon SageMaker\n                Clarify . Use SageMaker Clarify to create model evaluation jobs, that you use to\n              evaluate and compare model quality and responsibility metrics for text-based\n              foundation models from JumpStart. Generate predictions using Amazon SageMaker Canvas , without needing to\n              write any code.  Use SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n              deploy language models. This blog post describes how you can use them to optimize customer\n              interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock\n              and Amazon SageMaker JumpStart. Pricing Amazon Bedrock and Amazon SageMaker have different pricing\n        models that reflect their target users and the services they\n        provide. Amazon Bedrock Amazon Bedrock employs a simple pricing\n                model based on the number of API calls made to the service. You pay a fixed\n              price per API call, which includes the cost of running the pre-trained models and any\n              associated data processing. This straightforward pricing structure makes it more\n              efficient for you to estimate and control your costs, as you pay only for the actual\n              usage of the service. Amazon Bedrock's pricing model is particularly well-suited for\n              applications with predictable workloads, or for cases where you want more transparency\n              in your AI-related expenses. Amazon SageMaker SageMaker follows a pay-as-you-go pricing\n                model based on the usage of compute resources, storage, and other services\n              consumed during the machine learning process. You\u2019re charged for the instances that\n              you use to build, train, and deploy you models, with prices varying depending on the\n              instance type and size. Additionally, you incur costs for data storage, data transfer,\n              and other associated services like data labeling and model monitoring. This pricing\n              model provides flexibility and allows you to optimize costs based on your specific\n              requirements. However, it also means that costs can vary and may require careful\n              management, especially for resource-intensive projects. Integration Amazon Bedrock and Amazon SageMaker offer different approaches to integrating machine learning models\n          into applications, catering to your specific needs and expertise. Amazon Bedrock Amazon Bedrock simplifies the integration process by providing pre-trained models that you can\n              access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and\n              receive predictions from the models without needing to manage the underlying\n              infrastructure. This approach significantly reduces the complexity and time required\n              to integrate AI capabilities into applications, making it more accessible to\n              developers with limited machine learning expertise. However, this ease of integration\n              comes at the cost of limited customization options, as you\u2019re restricted to the\n              pre-trained models and APIs provided by Amazon Bedrock. Amazon SageMaker SageMaker provides a comprehensive platform for building, training, and deploying custom\n              machine learning models. However, integrating these models into applications requires\n              more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker SDK or\n              API to access the trained models and build the necessary infrastructure to expose them\n              as endpoints. This process involves creating and configuring API Gateway, Lambda\n              functions, and other AWS services to enable communication between the application\n              and the deployed model. While SageMaker provides tools and templates to simplify this\n              process, it still requires a deeper understanding of AWS services and machine\n              learning model deployment. Expertise required Amazon Bedrock and Amazon SageMaker are optimized for different levels of machine learning\n          expertise. Amazon Bedrock Amazon Bedrock is more accessible to a broader range of users, including developers and\n              businesses with limited machine learning expertise. By providing pre-trained models\n              that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away\n              much of the complexity associated with building and deploying machine learning models.\n              You don't need to worry about data preprocessing, model selection, or infrastructure\n              management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus\n              on integrating AI capabilities into your applications without needing to invest\n              significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker If you have deeper expertise in data science and machine learning, SageMaker provides a\n              powerful and flexible platform for building, training, and deploying custom models.\n              While SageMaker aims to simplify the machine learning workflow, it still requires a\n              significant level of technical expertise to take full advantage of its capabilities.\n              You\u2019ll benefit from being proficient in programming languages like Python, along with\n              a deep understanding of machine learning concepts, such as data preprocessing, model\n              selection, and hyperparameter tuning. Additionally, you should be comfortable working\n              with various AWS services and managing the infrastructure required to deploy and\n              integrate their models. As a result, SageMaker may have a steeper learning curve if you\u2019re\n              new to machine learning or have limited experience with AWS. anchor anchor anchor anchor anchor anchor Use cases Target users Customization Pricing Integration Expertise required Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. The choice between Amazon Bedrock and Amazon SageMaker is not always mutually exclusive. In some\n    cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to\n    quickly prototype and deploy a foundation model, and then use SageMaker to further refine and\n    optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker together to optimize\n    customer interaction by working with your own datasets (such as your product FAQs. Ultimately, the decision between Amazon Bedrock and Amazon SageMaker depends on your specific\n    requirements. Evaluating these factors can help you make an informed decision and choose the\n    service that is most suitable for your needs. For more information about Amazon\u2019s generative AI services and solutions, see the generative AI\n      decision guide . Use Now that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker, you can\n    select the service that meets your needs, and use the following information  to help you get\n    started using each of them. Amazon Bedrock What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock\n            Studio? Describes how to use this web app to prototype apps that use Amazon Bedrock models and\n              features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio This blog post describes how you can build applications using a wide array of top\n              performing models. It then explains how to evaluate and share your generative AI apps\n              within Amazon Bedrock Studio. Read the blog Building an app with Amazon Bedrock\n            Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Describes how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Describes how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and\n              RStudio on SageMaker. Explore the guide Get started with Amazon SageMaker\n            JumpStart Explore SageMaker JumpStart solution templates that set up\n          infrastructure for common use cases, and executable example\n          notebooks for machine learning with SageMaker. Explore the guide anchor anchor anchor Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Document history - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/doc-history.html", "content": "Document history PDF RSS The following table describes the important changes to this decision guide. \n  For notifications about updates to this guide, you can subscribe to an RSS feed. Change Description Date Minor updates Minor updates to improve readability. August 21, 2024 Minor updates Minor updates to reflect the latest Amazon Bedrock and Amazon SageMaker features. July 22, 2024 Initial release Initial release of the decision guide. July 11, 2024 Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Decision guide"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/pdfs/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.pdf#bedrock-or-sagemaker", "content": "No main content found."}, {"title": "Amazon Bedrock or Amazon SageMaker? - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html?icmpid=docs_homepage_featuredsvcs#introduction", "content": "Amazon Bedrock or Amazon SageMaker? PDF RSS Understand the differences and pick the one that's right for you Purpose Understand the differences between Amazon Bedrock and Amazon SageMaker, and determine which service is\n       the best fit for your needs. Last updated August 21, 2024 Covered services Amazon Bedrock Amazon SageMaker Introduction Amazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and\n    generative AI applications. It\u2019s helpful to understand how these services work together to form\n    a generative AI stack, including: Generative AI-powered services such as Amazon Q, which leverages large language models\n        (LLMs) and other foundation models (FMs). Tools for building applications with LLMs and other FMs, including Amazon Bedrock. Infrastructure for model training and inference, such as Amazon SageMaker and specialized\n        hardware. When considering which generative AI services you want to use, two services are often\n    considered alongside one another: Amazon Bedrock Choose Amazon Bedrock if you primarily need to use pre-trained foundation models for\n        inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is\n        a fully managed service for building generative AI applications with support for popular\n        foundation models, including Anthropic\n          Claude , Cohere Command\n          & Embed , AI21 Labs\n          Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon\n        Titan . Supported FMs are updated on a regular basis. Use Amazon Bedrock to build generative AI applications with security, privacy, and responsible\n        AI\u2014regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API\n        access, so you can use different foundation models, and upgrade to the latest model\n        versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom\n          models . Use Amazon Bedrock Studio (in\n        preview), which is a new SSO-enabled web interface that your developers can use to work with\n        large language models (LLMs) and other foundation models (FMs), collaborate on projects, and\n        iterate on generative AI applications. Amazon SageMaker Amazon SageMaker is a\n        fully managed service designed to help you build, train, and deploy machine learning models\n        at scale. This includes building FMs from scratch, using tools like notebooks, debuggers,\n        profilers,\u00a0pipelines, and MLOps. Consider SageMaker when you have use cases that can bene\ufb01t from\n        extensive training, \ufb01ne-tuning, and customization of foundation models. It can also help you\n        through the potentially challenging task of evaluating which FM is the best \ufb01t for your use\n        case. Use SageMaker\u2019s integrated development environment (IDE) to build, train, and deploy\n        FMs.\u00a0SageMaker offers access to hundreds of pretrained models, including publicly\u00a0available FMs. For more information about how Amazon Bedrock and SageMaker fit into Amazon\u2019s generative AI services and\n    solutions, see the generative AI\n      decision guide . While both Amazon Bedrock and Amazon SageMaker enable the development of ML and generative AI\n    applications, they serve different purposes. This guide will help you understand which of these\n    services is the best fit for your needs, including scenarios in which both services can be used\n    together to build generative AI applications. Here's a high-level view of the key differences between these services to get you started. Category Amazon Bedrock Amazon SageMaker Use Cases Ideal for integration of AI capabilities into\n            applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning\n            expertise Optimized for data scientists, machine learning engineers, and developers Customization You'll primarily use pre-trained models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other\n            services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for\n            building and optimizing models Differences between Amazon Bedrock and SageMaker Let's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker. Use cases Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. Target users Amazon Bedrock and Amazon SageMaker support different targeted users based on their level of expertise\n          and knowledge of machine learning and artificial intelligence. Amazon Bedrock Amazon Bedrock offers a more accessible and straightforward way to integrate AI\n              functionality into your projects. It\u2019s appropriate for a broad audience, which\n              includes developers and businesses, that has limited experience in building and\n              training machine learning models, but wants to use AI to enhance their applications or\n              workflows. Amazon SageMaker SageMaker is predominantly for data scientists, machine learning engineers, and\n              developers who possess the necessary skills and knowledge to build, train, and deploy\n              custom machine learning models. Use SageMaker if you are well-versed in data science and\n              machine learning concepts, and require a platform that provides you with the tools and\n              flexibility to create models tailored to your specific needs. Customization Amazon Bedrock and Amazon SageMaker offer different levels of customization capabilities that you can\n          tailor to your specific needs and expertise. Amazon Bedrock Amazon Bedrock provides pre-trained AI models that you can integrate into applications,\n              with limited customization. You have access to a set of API calls that you use to\n              enter data and receive predictions from these pre-trained models. While this approach\n              drastically simplifies the process of incorporating AI capabilities into applications,\n              it also means that you have less control over the underlying models, unless you\n              customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized\n              for common AI tasks and are designed to work well for a wide range of use cases, but\n              they may not be suitable for highly specialized or niche requirements. Amazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R,\n              Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan\n              Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n              (Oregon) AWS Region. The list of supported FMs is updated on an ongoing\n              basis. Customize models for specific tasks and use cases, including FM fine-tuning\n              and pre-training. Bring your own customized model with custom model\n                import (in preview). Amazon SageMaker Amazon SageMaker provides extensive customization options, giving you full control over the\n              entire machine learning workflow. With SageMaker, you can fine-tune every aspect of your\n              models, from data preprocessing and feature engineering to model architecture and\n              hyperparameter optimization. By using this level of customization, you can create\n              highly specialized models that are tailored to your unique business requirements. SageMaker\n              supports a wide range of popular machine learning frameworks, such as TensorFlow,\n              PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for\n              building and training models. Use Amazon SageMaker JumpStart to evaluate, compare, and select FMs based on\n              pre-defined quality and responsibility. Choose which FM to use with Amazon SageMaker\n                Clarify . Use SageMaker Clarify to create model evaluation jobs, that you use to\n              evaluate and compare model quality and responsibility metrics for text-based\n              foundation models from JumpStart. Generate predictions using Amazon SageMaker Canvas , without needing to\n              write any code.  Use SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n              deploy language models. This blog post describes how you can use them to optimize customer\n              interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock\n              and Amazon SageMaker JumpStart. Pricing Amazon Bedrock and Amazon SageMaker have different pricing\n        models that reflect their target users and the services they\n        provide. Amazon Bedrock Amazon Bedrock employs a simple pricing\n                model based on the number of API calls made to the service. You pay a fixed\n              price per API call, which includes the cost of running the pre-trained models and any\n              associated data processing. This straightforward pricing structure makes it more\n              efficient for you to estimate and control your costs, as you pay only for the actual\n              usage of the service. Amazon Bedrock's pricing model is particularly well-suited for\n              applications with predictable workloads, or for cases where you want more transparency\n              in your AI-related expenses. Amazon SageMaker SageMaker follows a pay-as-you-go pricing\n                model based on the usage of compute resources, storage, and other services\n              consumed during the machine learning process. You\u2019re charged for the instances that\n              you use to build, train, and deploy you models, with prices varying depending on the\n              instance type and size. Additionally, you incur costs for data storage, data transfer,\n              and other associated services like data labeling and model monitoring. This pricing\n              model provides flexibility and allows you to optimize costs based on your specific\n              requirements. However, it also means that costs can vary and may require careful\n              management, especially for resource-intensive projects. Integration Amazon Bedrock and Amazon SageMaker offer different approaches to integrating machine learning models\n          into applications, catering to your specific needs and expertise. Amazon Bedrock Amazon Bedrock simplifies the integration process by providing pre-trained models that you can\n              access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and\n              receive predictions from the models without needing to manage the underlying\n              infrastructure. This approach significantly reduces the complexity and time required\n              to integrate AI capabilities into applications, making it more accessible to\n              developers with limited machine learning expertise. However, this ease of integration\n              comes at the cost of limited customization options, as you\u2019re restricted to the\n              pre-trained models and APIs provided by Amazon Bedrock. Amazon SageMaker SageMaker provides a comprehensive platform for building, training, and deploying custom\n              machine learning models. However, integrating these models into applications requires\n              more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker SDK or\n              API to access the trained models and build the necessary infrastructure to expose them\n              as endpoints. This process involves creating and configuring API Gateway, Lambda\n              functions, and other AWS services to enable communication between the application\n              and the deployed model. While SageMaker provides tools and templates to simplify this\n              process, it still requires a deeper understanding of AWS services and machine\n              learning model deployment. Expertise required Amazon Bedrock and Amazon SageMaker are optimized for different levels of machine learning\n          expertise. Amazon Bedrock Amazon Bedrock is more accessible to a broader range of users, including developers and\n              businesses with limited machine learning expertise. By providing pre-trained models\n              that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away\n              much of the complexity associated with building and deploying machine learning models.\n              You don't need to worry about data preprocessing, model selection, or infrastructure\n              management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus\n              on integrating AI capabilities into your applications without needing to invest\n              significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker If you have deeper expertise in data science and machine learning, SageMaker provides a\n              powerful and flexible platform for building, training, and deploying custom models.\n              While SageMaker aims to simplify the machine learning workflow, it still requires a\n              significant level of technical expertise to take full advantage of its capabilities.\n              You\u2019ll benefit from being proficient in programming languages like Python, along with\n              a deep understanding of machine learning concepts, such as data preprocessing, model\n              selection, and hyperparameter tuning. Additionally, you should be comfortable working\n              with various AWS services and managing the infrastructure required to deploy and\n              integrate their models. As a result, SageMaker may have a steeper learning curve if you\u2019re\n              new to machine learning or have limited experience with AWS. anchor anchor anchor anchor anchor anchor Use cases Target users Customization Pricing Integration Expertise required Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. The choice between Amazon Bedrock and Amazon SageMaker is not always mutually exclusive. In some\n    cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to\n    quickly prototype and deploy a foundation model, and then use SageMaker to further refine and\n    optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker together to optimize\n    customer interaction by working with your own datasets (such as your product FAQs. Ultimately, the decision between Amazon Bedrock and Amazon SageMaker depends on your specific\n    requirements. Evaluating these factors can help you make an informed decision and choose the\n    service that is most suitable for your needs. For more information about Amazon\u2019s generative AI services and solutions, see the generative AI\n      decision guide . Use Now that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker, you can\n    select the service that meets your needs, and use the following information  to help you get\n    started using each of them. Amazon Bedrock What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock\n            Studio? Describes how to use this web app to prototype apps that use Amazon Bedrock models and\n              features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio This blog post describes how you can build applications using a wide array of top\n              performing models. It then explains how to evaluate and share your generative AI apps\n              within Amazon Bedrock Studio. Read the blog Building an app with Amazon Bedrock\n            Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Describes how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Describes how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and\n              RStudio on SageMaker. Explore the guide Get started with Amazon SageMaker\n            JumpStart Explore SageMaker JumpStart solution templates that set up\n          infrastructure for common use cases, and executable example\n          notebooks for machine learning with SageMaker. Explore the guide anchor anchor anchor Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Amazon Bedrock or Amazon SageMaker? - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html?icmpid=docs_homepage_featuredsvcs#differences", "content": "Amazon Bedrock or Amazon SageMaker? PDF RSS Understand the differences and pick the one that's right for you Purpose Understand the differences between Amazon Bedrock and Amazon SageMaker, and determine which service is\n       the best fit for your needs. Last updated August 21, 2024 Covered services Amazon Bedrock Amazon SageMaker Introduction Amazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and\n    generative AI applications. It\u2019s helpful to understand how these services work together to form\n    a generative AI stack, including: Generative AI-powered services such as Amazon Q, which leverages large language models\n        (LLMs) and other foundation models (FMs). Tools for building applications with LLMs and other FMs, including Amazon Bedrock. Infrastructure for model training and inference, such as Amazon SageMaker and specialized\n        hardware. When considering which generative AI services you want to use, two services are often\n    considered alongside one another: Amazon Bedrock Choose Amazon Bedrock if you primarily need to use pre-trained foundation models for\n        inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is\n        a fully managed service for building generative AI applications with support for popular\n        foundation models, including Anthropic\n          Claude , Cohere Command\n          & Embed , AI21 Labs\n          Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon\n        Titan . Supported FMs are updated on a regular basis. Use Amazon Bedrock to build generative AI applications with security, privacy, and responsible\n        AI\u2014regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API\n        access, so you can use different foundation models, and upgrade to the latest model\n        versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom\n          models . Use Amazon Bedrock Studio (in\n        preview), which is a new SSO-enabled web interface that your developers can use to work with\n        large language models (LLMs) and other foundation models (FMs), collaborate on projects, and\n        iterate on generative AI applications. Amazon SageMaker Amazon SageMaker is a\n        fully managed service designed to help you build, train, and deploy machine learning models\n        at scale. This includes building FMs from scratch, using tools like notebooks, debuggers,\n        profilers,\u00a0pipelines, and MLOps. Consider SageMaker when you have use cases that can bene\ufb01t from\n        extensive training, \ufb01ne-tuning, and customization of foundation models. It can also help you\n        through the potentially challenging task of evaluating which FM is the best \ufb01t for your use\n        case. Use SageMaker\u2019s integrated development environment (IDE) to build, train, and deploy\n        FMs.\u00a0SageMaker offers access to hundreds of pretrained models, including publicly\u00a0available FMs. For more information about how Amazon Bedrock and SageMaker fit into Amazon\u2019s generative AI services and\n    solutions, see the generative AI\n      decision guide . While both Amazon Bedrock and Amazon SageMaker enable the development of ML and generative AI\n    applications, they serve different purposes. This guide will help you understand which of these\n    services is the best fit for your needs, including scenarios in which both services can be used\n    together to build generative AI applications. Here's a high-level view of the key differences between these services to get you started. Category Amazon Bedrock Amazon SageMaker Use Cases Ideal for integration of AI capabilities into\n            applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning\n            expertise Optimized for data scientists, machine learning engineers, and developers Customization You'll primarily use pre-trained models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other\n            services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for\n            building and optimizing models Differences between Amazon Bedrock and SageMaker Let's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker. Use cases Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. Target users Amazon Bedrock and Amazon SageMaker support different targeted users based on their level of expertise\n          and knowledge of machine learning and artificial intelligence. Amazon Bedrock Amazon Bedrock offers a more accessible and straightforward way to integrate AI\n              functionality into your projects. It\u2019s appropriate for a broad audience, which\n              includes developers and businesses, that has limited experience in building and\n              training machine learning models, but wants to use AI to enhance their applications or\n              workflows. Amazon SageMaker SageMaker is predominantly for data scientists, machine learning engineers, and\n              developers who possess the necessary skills and knowledge to build, train, and deploy\n              custom machine learning models. Use SageMaker if you are well-versed in data science and\n              machine learning concepts, and require a platform that provides you with the tools and\n              flexibility to create models tailored to your specific needs. Customization Amazon Bedrock and Amazon SageMaker offer different levels of customization capabilities that you can\n          tailor to your specific needs and expertise. Amazon Bedrock Amazon Bedrock provides pre-trained AI models that you can integrate into applications,\n              with limited customization. You have access to a set of API calls that you use to\n              enter data and receive predictions from these pre-trained models. While this approach\n              drastically simplifies the process of incorporating AI capabilities into applications,\n              it also means that you have less control over the underlying models, unless you\n              customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized\n              for common AI tasks and are designed to work well for a wide range of use cases, but\n              they may not be suitable for highly specialized or niche requirements. Amazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R,\n              Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan\n              Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n              (Oregon) AWS Region. The list of supported FMs is updated on an ongoing\n              basis. Customize models for specific tasks and use cases, including FM fine-tuning\n              and pre-training. Bring your own customized model with custom model\n                import (in preview). Amazon SageMaker Amazon SageMaker provides extensive customization options, giving you full control over the\n              entire machine learning workflow. With SageMaker, you can fine-tune every aspect of your\n              models, from data preprocessing and feature engineering to model architecture and\n              hyperparameter optimization. By using this level of customization, you can create\n              highly specialized models that are tailored to your unique business requirements. SageMaker\n              supports a wide range of popular machine learning frameworks, such as TensorFlow,\n              PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for\n              building and training models. Use Amazon SageMaker JumpStart to evaluate, compare, and select FMs based on\n              pre-defined quality and responsibility. Choose which FM to use with Amazon SageMaker\n                Clarify . Use SageMaker Clarify to create model evaluation jobs, that you use to\n              evaluate and compare model quality and responsibility metrics for text-based\n              foundation models from JumpStart. Generate predictions using Amazon SageMaker Canvas , without needing to\n              write any code.  Use SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n              deploy language models. This blog post describes how you can use them to optimize customer\n              interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock\n              and Amazon SageMaker JumpStart. Pricing Amazon Bedrock and Amazon SageMaker have different pricing\n        models that reflect their target users and the services they\n        provide. Amazon Bedrock Amazon Bedrock employs a simple pricing\n                model based on the number of API calls made to the service. You pay a fixed\n              price per API call, which includes the cost of running the pre-trained models and any\n              associated data processing. This straightforward pricing structure makes it more\n              efficient for you to estimate and control your costs, as you pay only for the actual\n              usage of the service. Amazon Bedrock's pricing model is particularly well-suited for\n              applications with predictable workloads, or for cases where you want more transparency\n              in your AI-related expenses. Amazon SageMaker SageMaker follows a pay-as-you-go pricing\n                model based on the usage of compute resources, storage, and other services\n              consumed during the machine learning process. You\u2019re charged for the instances that\n              you use to build, train, and deploy you models, with prices varying depending on the\n              instance type and size. Additionally, you incur costs for data storage, data transfer,\n              and other associated services like data labeling and model monitoring. This pricing\n              model provides flexibility and allows you to optimize costs based on your specific\n              requirements. However, it also means that costs can vary and may require careful\n              management, especially for resource-intensive projects. Integration Amazon Bedrock and Amazon SageMaker offer different approaches to integrating machine learning models\n          into applications, catering to your specific needs and expertise. Amazon Bedrock Amazon Bedrock simplifies the integration process by providing pre-trained models that you can\n              access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and\n              receive predictions from the models without needing to manage the underlying\n              infrastructure. This approach significantly reduces the complexity and time required\n              to integrate AI capabilities into applications, making it more accessible to\n              developers with limited machine learning expertise. However, this ease of integration\n              comes at the cost of limited customization options, as you\u2019re restricted to the\n              pre-trained models and APIs provided by Amazon Bedrock. Amazon SageMaker SageMaker provides a comprehensive platform for building, training, and deploying custom\n              machine learning models. However, integrating these models into applications requires\n              more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker SDK or\n              API to access the trained models and build the necessary infrastructure to expose them\n              as endpoints. This process involves creating and configuring API Gateway, Lambda\n              functions, and other AWS services to enable communication between the application\n              and the deployed model. While SageMaker provides tools and templates to simplify this\n              process, it still requires a deeper understanding of AWS services and machine\n              learning model deployment. Expertise required Amazon Bedrock and Amazon SageMaker are optimized for different levels of machine learning\n          expertise. Amazon Bedrock Amazon Bedrock is more accessible to a broader range of users, including developers and\n              businesses with limited machine learning expertise. By providing pre-trained models\n              that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away\n              much of the complexity associated with building and deploying machine learning models.\n              You don't need to worry about data preprocessing, model selection, or infrastructure\n              management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus\n              on integrating AI capabilities into your applications without needing to invest\n              significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker If you have deeper expertise in data science and machine learning, SageMaker provides a\n              powerful and flexible platform for building, training, and deploying custom models.\n              While SageMaker aims to simplify the machine learning workflow, it still requires a\n              significant level of technical expertise to take full advantage of its capabilities.\n              You\u2019ll benefit from being proficient in programming languages like Python, along with\n              a deep understanding of machine learning concepts, such as data preprocessing, model\n              selection, and hyperparameter tuning. Additionally, you should be comfortable working\n              with various AWS services and managing the infrastructure required to deploy and\n              integrate their models. As a result, SageMaker may have a steeper learning curve if you\u2019re\n              new to machine learning or have limited experience with AWS. anchor anchor anchor anchor anchor anchor Use cases Target users Customization Pricing Integration Expertise required The choice between Amazon Bedrock and Amazon SageMaker is not always mutually exclusive. In some\n    cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to\n    quickly prototype and deploy a foundation model, and then use SageMaker to further refine and\n    optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker together to optimize\n    customer interaction by working with your own datasets (such as your product FAQs. Ultimately, the decision between Amazon Bedrock and Amazon SageMaker depends on your specific\n    requirements. Evaluating these factors can help you make an informed decision and choose the\n    service that is most suitable for your needs. For more information about Amazon\u2019s generative AI services and solutions, see the generative AI\n      decision guide . Use Now that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker, you can\n    select the service that meets your needs, and use the following information  to help you get\n    started using each of them. Amazon Bedrock What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock\n            Studio? Describes how to use this web app to prototype apps that use Amazon Bedrock models and\n              features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio This blog post describes how you can build applications using a wide array of top\n              performing models. It then explains how to evaluate and share your generative AI apps\n              within Amazon Bedrock Studio. Read the blog Building an app with Amazon Bedrock\n            Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Describes how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Describes how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and\n              RStudio on SageMaker. Explore the guide Get started with Amazon SageMaker\n            JumpStart Explore SageMaker JumpStart solution templates that set up\n          infrastructure for common use cases, and executable example\n          notebooks for machine learning with SageMaker. Explore the guide anchor anchor anchor Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Amazon Bedrock or Amazon SageMaker? - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html?icmpid=docs_homepage_featuredsvcs#use", "content": "Amazon Bedrock or Amazon SageMaker? PDF RSS Understand the differences and pick the one that's right for you Purpose Understand the differences between Amazon Bedrock and Amazon SageMaker, and determine which service is\n       the best fit for your needs. Last updated August 21, 2024 Covered services Amazon Bedrock Amazon SageMaker Introduction Amazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and\n    generative AI applications. It\u2019s helpful to understand how these services work together to form\n    a generative AI stack, including: Generative AI-powered services such as Amazon Q, which leverages large language models\n        (LLMs) and other foundation models (FMs). Tools for building applications with LLMs and other FMs, including Amazon Bedrock. Infrastructure for model training and inference, such as Amazon SageMaker and specialized\n        hardware. When considering which generative AI services you want to use, two services are often\n    considered alongside one another: Amazon Bedrock Choose Amazon Bedrock if you primarily need to use pre-trained foundation models for\n        inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is\n        a fully managed service for building generative AI applications with support for popular\n        foundation models, including Anthropic\n          Claude , Cohere Command\n          & Embed , AI21 Labs\n          Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon\n        Titan . Supported FMs are updated on a regular basis. Use Amazon Bedrock to build generative AI applications with security, privacy, and responsible\n        AI\u2014regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API\n        access, so you can use different foundation models, and upgrade to the latest model\n        versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom\n          models . Use Amazon Bedrock Studio (in\n        preview), which is a new SSO-enabled web interface that your developers can use to work with\n        large language models (LLMs) and other foundation models (FMs), collaborate on projects, and\n        iterate on generative AI applications. Amazon SageMaker Amazon SageMaker is a\n        fully managed service designed to help you build, train, and deploy machine learning models\n        at scale. This includes building FMs from scratch, using tools like notebooks, debuggers,\n        profilers,\u00a0pipelines, and MLOps. Consider SageMaker when you have use cases that can bene\ufb01t from\n        extensive training, \ufb01ne-tuning, and customization of foundation models. It can also help you\n        through the potentially challenging task of evaluating which FM is the best \ufb01t for your use\n        case. Use SageMaker\u2019s integrated development environment (IDE) to build, train, and deploy\n        FMs.\u00a0SageMaker offers access to hundreds of pretrained models, including publicly\u00a0available FMs. For more information about how Amazon Bedrock and SageMaker fit into Amazon\u2019s generative AI services and\n    solutions, see the generative AI\n      decision guide . While both Amazon Bedrock and Amazon SageMaker enable the development of ML and generative AI\n    applications, they serve different purposes. This guide will help you understand which of these\n    services is the best fit for your needs, including scenarios in which both services can be used\n    together to build generative AI applications. Here's a high-level view of the key differences between these services to get you started. Category Amazon Bedrock Amazon SageMaker Use Cases Ideal for integration of AI capabilities into\n            applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning\n            expertise Optimized for data scientists, machine learning engineers, and developers Customization You'll primarily use pre-trained models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other\n            services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for\n            building and optimizing models Differences between Amazon Bedrock and SageMaker Let's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker. Use cases Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. Target users Amazon Bedrock and Amazon SageMaker support different targeted users based on their level of expertise\n          and knowledge of machine learning and artificial intelligence. Amazon Bedrock Amazon Bedrock offers a more accessible and straightforward way to integrate AI\n              functionality into your projects. It\u2019s appropriate for a broad audience, which\n              includes developers and businesses, that has limited experience in building and\n              training machine learning models, but wants to use AI to enhance their applications or\n              workflows. Amazon SageMaker SageMaker is predominantly for data scientists, machine learning engineers, and\n              developers who possess the necessary skills and knowledge to build, train, and deploy\n              custom machine learning models. Use SageMaker if you are well-versed in data science and\n              machine learning concepts, and require a platform that provides you with the tools and\n              flexibility to create models tailored to your specific needs. Customization Amazon Bedrock and Amazon SageMaker offer different levels of customization capabilities that you can\n          tailor to your specific needs and expertise. Amazon Bedrock Amazon Bedrock provides pre-trained AI models that you can integrate into applications,\n              with limited customization. You have access to a set of API calls that you use to\n              enter data and receive predictions from these pre-trained models. While this approach\n              drastically simplifies the process of incorporating AI capabilities into applications,\n              it also means that you have less control over the underlying models, unless you\n              customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized\n              for common AI tasks and are designed to work well for a wide range of use cases, but\n              they may not be suitable for highly specialized or niche requirements. Amazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R,\n              Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan\n              Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n              (Oregon) AWS Region. The list of supported FMs is updated on an ongoing\n              basis. Customize models for specific tasks and use cases, including FM fine-tuning\n              and pre-training. Bring your own customized model with custom model\n                import (in preview). Amazon SageMaker Amazon SageMaker provides extensive customization options, giving you full control over the\n              entire machine learning workflow. With SageMaker, you can fine-tune every aspect of your\n              models, from data preprocessing and feature engineering to model architecture and\n              hyperparameter optimization. By using this level of customization, you can create\n              highly specialized models that are tailored to your unique business requirements. SageMaker\n              supports a wide range of popular machine learning frameworks, such as TensorFlow,\n              PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for\n              building and training models. Use Amazon SageMaker JumpStart to evaluate, compare, and select FMs based on\n              pre-defined quality and responsibility. Choose which FM to use with Amazon SageMaker\n                Clarify . Use SageMaker Clarify to create model evaluation jobs, that you use to\n              evaluate and compare model quality and responsibility metrics for text-based\n              foundation models from JumpStart. Generate predictions using Amazon SageMaker Canvas , without needing to\n              write any code.  Use SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n              deploy language models. This blog post describes how you can use them to optimize customer\n              interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock\n              and Amazon SageMaker JumpStart. Pricing Amazon Bedrock and Amazon SageMaker have different pricing\n        models that reflect their target users and the services they\n        provide. Amazon Bedrock Amazon Bedrock employs a simple pricing\n                model based on the number of API calls made to the service. You pay a fixed\n              price per API call, which includes the cost of running the pre-trained models and any\n              associated data processing. This straightforward pricing structure makes it more\n              efficient for you to estimate and control your costs, as you pay only for the actual\n              usage of the service. Amazon Bedrock's pricing model is particularly well-suited for\n              applications with predictable workloads, or for cases where you want more transparency\n              in your AI-related expenses. Amazon SageMaker SageMaker follows a pay-as-you-go pricing\n                model based on the usage of compute resources, storage, and other services\n              consumed during the machine learning process. You\u2019re charged for the instances that\n              you use to build, train, and deploy you models, with prices varying depending on the\n              instance type and size. Additionally, you incur costs for data storage, data transfer,\n              and other associated services like data labeling and model monitoring. This pricing\n              model provides flexibility and allows you to optimize costs based on your specific\n              requirements. However, it also means that costs can vary and may require careful\n              management, especially for resource-intensive projects. Integration Amazon Bedrock and Amazon SageMaker offer different approaches to integrating machine learning models\n          into applications, catering to your specific needs and expertise. Amazon Bedrock Amazon Bedrock simplifies the integration process by providing pre-trained models that you can\n              access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and\n              receive predictions from the models without needing to manage the underlying\n              infrastructure. This approach significantly reduces the complexity and time required\n              to integrate AI capabilities into applications, making it more accessible to\n              developers with limited machine learning expertise. However, this ease of integration\n              comes at the cost of limited customization options, as you\u2019re restricted to the\n              pre-trained models and APIs provided by Amazon Bedrock. Amazon SageMaker SageMaker provides a comprehensive platform for building, training, and deploying custom\n              machine learning models. However, integrating these models into applications requires\n              more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker SDK or\n              API to access the trained models and build the necessary infrastructure to expose them\n              as endpoints. This process involves creating and configuring API Gateway, Lambda\n              functions, and other AWS services to enable communication between the application\n              and the deployed model. While SageMaker provides tools and templates to simplify this\n              process, it still requires a deeper understanding of AWS services and machine\n              learning model deployment. Expertise required Amazon Bedrock and Amazon SageMaker are optimized for different levels of machine learning\n          expertise. Amazon Bedrock Amazon Bedrock is more accessible to a broader range of users, including developers and\n              businesses with limited machine learning expertise. By providing pre-trained models\n              that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away\n              much of the complexity associated with building and deploying machine learning models.\n              You don't need to worry about data preprocessing, model selection, or infrastructure\n              management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus\n              on integrating AI capabilities into your applications without needing to invest\n              significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker If you have deeper expertise in data science and machine learning, SageMaker provides a\n              powerful and flexible platform for building, training, and deploying custom models.\n              While SageMaker aims to simplify the machine learning workflow, it still requires a\n              significant level of technical expertise to take full advantage of its capabilities.\n              You\u2019ll benefit from being proficient in programming languages like Python, along with\n              a deep understanding of machine learning concepts, such as data preprocessing, model\n              selection, and hyperparameter tuning. Additionally, you should be comfortable working\n              with various AWS services and managing the infrastructure required to deploy and\n              integrate their models. As a result, SageMaker may have a steeper learning curve if you\u2019re\n              new to machine learning or have limited experience with AWS. anchor anchor anchor anchor anchor anchor Use cases Target users Customization Pricing Integration Expertise required Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. The choice between Amazon Bedrock and Amazon SageMaker is not always mutually exclusive. In some\n    cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to\n    quickly prototype and deploy a foundation model, and then use SageMaker to further refine and\n    optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker together to optimize\n    customer interaction by working with your own datasets (such as your product FAQs. Ultimately, the decision between Amazon Bedrock and Amazon SageMaker depends on your specific\n    requirements. Evaluating these factors can help you make an informed decision and choose the\n    service that is most suitable for your needs. For more information about Amazon\u2019s generative AI services and solutions, see the generative AI\n      decision guide . Use Now that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker, you can\n    select the service that meets your needs, and use the following information  to help you get\n    started using each of them. Amazon Bedrock What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock\n            Studio? Describes how to use this web app to prototype apps that use Amazon Bedrock models and\n              features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio This blog post describes how you can build applications using a wide array of top\n              performing models. It then explains how to evaluate and share your generative AI apps\n              within Amazon Bedrock Studio. Read the blog Building an app with Amazon Bedrock\n            Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Describes how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Describes how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and\n              RStudio on SageMaker. Explore the guide Get started with Amazon SageMaker\n            JumpStart Explore SageMaker JumpStart solution templates that set up\n          infrastructure for common use cases, and executable example\n          notebooks for machine learning with SageMaker. Explore the guide anchor anchor anchor Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-sagemaker.rss", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?feedback_destination_id=a0d18259-7974-4e8f-8382-0a4be53f4374&topic_url=https://docs.aws.amazon.com/en_us/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html", "content": "No main content found."}, {"title": "Overview - AWS Setup", "url": "https://docs.aws.amazon.com/SetUp/latest/UserGuide/setup-overview.html", "content": "Overview This guide provides instructions to create a new AWS account and set up your first administrative user in AWS IAM Identity Center following the \n            latest security best practices. An AWS account is required to access AWS services\n            and serves as two basic functions: Container \u2013 An AWS account is \n                a container for all the AWS resources you can create as an AWS customer.\n                When you create an Amazon Simple Storage Service (Amazon S3) bucket or Amazon Relational Database Service (Amazon RDS) database to store\n                your data, or an Amazon Elastic Compute Cloud (Amazon EC2) instance to process your data, you are\n                creating a resource in your account. Every resource is uniquely identified by an\n                Amazon Resource Name (ARN) that includes the account ID of the account that\n                contains or owns the resource. Security boundary \u2013 An AWS account is\n                the basic security boundary for your AWS resources. Resources that you\n                create in your account are available only to users who have credentials for that\n                same account. Among the key resources you can create in your account are identities , such as\n                  IAM users and roles, and federated identities, such as users from your enterprise user\n                    directory, a web identity provider, the IAM Identity Center directory, or any other user that\n                    accesses AWS services by using credentials provided through an identity source. These identities have\n                    credentials that someone can use to sign in, or authenticate to AWS. Identities also have permission policies\n                    that specify what the person who signed in is authorized to do with the\n                    resources in the account. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Terminology"}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=AWS%20Signup%20and%20Signin&topic_url=https://docs.aws.amazon.com/SetUp/latest/UserGuide/setup-overview.html", "content": "No main content found."}, {"title": "AWS Documentation", "url": "https://docs.aws.amazon.com/pdfs/SetUp/latest/UserGuide/setup-guide.pdf#setup-overview", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=AWS%20Signup%20and%20Signin&topic_url=https://docs.aws.amazon.com/en_us/SetUp/latest/UserGuide/setup-overview.html", "content": "No main content found."}, {"title": "Terminology - AWS Setup", "url": "https://docs.aws.amazon.com/SetUp/latest/UserGuide/setup-terminology.html", "content": "Terminology PDF Amazon Web Services (AWS) uses common\n         terminology to describe the sign in process. We recommend you read and understand\n         these terms. Administrator Also referred to as a AWS account administrator or IAM administrator. The\n        administrator, typically Information Technology (IT) personnel, is an individual who\n        oversees an AWS account. Administrators have a higher level of permissions to the\n        AWS account than other members of their organization. Administrators establish and\n        implement settings for the AWS account. They also create IAM or IAM Identity Center users.\n        The administrator provides these users with their access credentials and a sign-in URL to\n        sign in to AWS. Account A standard AWS account contains both your AWS resources and the identities that can\n    access those resources. Accounts are associated with the account owner\u2019s email address and\n    password. Credentials Also referred to as access credentials or security credentials. Credentials are the\n        information that users provide to AWS to sign in and gain access to AWS resources.\n        Credentials can include an email address, a user name, a user defined password, an account\n        ID or alias, a verification code, and a single use multi-factor authentication (MFA) code.\n        In authentication and authorization, a system uses credentials to identify who is making a\n        call and whether to allow the requested access. In AWS, these credentials are typically\n        the access\n        key ID and the secret access\n        key . For more information about credentials, see Understanding and getting your AWS\n          credentials . Note The type of credentials a user must submit depends on their user type. Corporate credentials The credentials that users provide when accessing their corporate network and resources.\n        Your corporate administrator can set up your AWS account to be accessible with the same\n        credentials that you use to access your corporate network and resources. These credentials\n        are provided to you by your administrator or help desk employee. Profile When you sign up for an AWS Builder ID, you create a profile. Your profile includes the contact information you provided and the ability to manage multi-factor authentication\n      (MFA) devices and active sessions. You can also learn more about privacy and how we handle your data in your profile. For more information about your profile and how it relates\n      to an AWS account, see AWS Builder ID and other AWS credentials . User A user is a person or application under an account that makes API calls to AWS\n        products. Each user has a unique name within the AWS account and a set of security\n        credentials that are not shared with others. These credentials are separate from the security\n        credentials for the AWS account. Each user is associated with one and only one\n        AWS account. Root user credentials The root user credentials are the same credentials used to sign in to the AWS Management Console as the\n        root user. For more information on the root user, see Root user . Verification code A verification code verifies your identity during the sign-in process using multi-factor\n        authentication (MFA) . The delivery methods for verification codes vary. They can be sent\n        via text message or email. Check with your administrator for more information. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Overview AWS users and credentials"}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/All products", "content": "No main content found."}, {"title": "AWS Documentation Feedback", "url": "https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=Landing%20Pages&topic_url=https://docs.aws.amazon.com/#/All%20products", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Analytics", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Application Integration", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/AWS Management Console", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Blockchain", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Business Applications", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Cloud Financial Management", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Compute", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Compute HPC", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Containers", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Cryptography & PKI", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Customer Enablement Services", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Database", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Decision Guides", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Developer Tools", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/End User Computing", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Front-End Web & Mobile", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Game Development", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/General Reference", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Internet of Things (IoT)", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Machine Learning", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Management & Governance", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Marketplace", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Media Services", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Migration & Transfer", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Networking & Content Delivery", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Partner Central", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Quantum Computing", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Robotics", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Satellite", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Security, Identity, & Compliance", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Serverless", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#/Storage", "content": "No main content found."}, {"title": "Amazon Augmented AI", "url": "https://docs.aws.amazon.com/augmented-ai/?icmpid=docs_homepage_ml", "content": "No main content found."}, {"title": "Amazon API Gateway Documentation", "url": "https://docs.aws.amazon.com/apigateway/?icmpid=docs_homepage_networking", "content": "No main content found."}, {"title": "Amazon AppFlow Documentation", "url": "https://docs.aws.amazon.com/appflow/?icmpid=docs_homepage_analytics", "content": "No main content found."}, {"title": "Amazon Application Recovery Controller (ARC) Documentation", "url": "https://docs.aws.amazon.com/amazonarc/?icmpid=docs_homepage_networking", "content": "No main content found."}, {"title": "Amazon AppStream 2.0 Documentation", "url": "https://docs.aws.amazon.com/appstream2/?icmpid=docs_homepage_euc", "content": "No main content found."}, {"title": "Amazon Athena Documentation", "url": "https://docs.aws.amazon.com/athena/?icmpid=docs_homepage_analytics", "content": "No main content found."}, {"title": "Amazon RDS and Aurora Documentation", "url": "https://docs.aws.amazon.com/rds/?icmpid=docs_homepage_databases", "content": "No main content found."}, {"title": "Amazon Bedrock Documentation", "url": "https://docs.aws.amazon.com/bedrock/?icmpid=docs_homepage_ml", "content": "No main content found."}, {"title": "Amazon Bedrock or Amazon SageMaker? - Amazon Bedrock or Amazon SageMaker?", "url": "https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html?icmpid=docs_homepage_decision_guides", "content": "Amazon Bedrock or Amazon SageMaker? PDF RSS Understand the differences and pick the one that's right for you Purpose Understand the differences between Amazon Bedrock and Amazon SageMaker, and determine which service is\n       the best fit for your needs. Last updated August 21, 2024 Covered services Amazon Bedrock Amazon SageMaker Introduction Amazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and\n    generative AI applications. It\u2019s helpful to understand how these services work together to form\n    a generative AI stack, including: Generative AI-powered services such as Amazon Q, which leverages large language models\n        (LLMs) and other foundation models (FMs). Tools for building applications with LLMs and other FMs, including Amazon Bedrock. Infrastructure for model training and inference, such as Amazon SageMaker and specialized\n        hardware. When considering which generative AI services you want to use, two services are often\n    considered alongside one another: Amazon Bedrock Choose Amazon Bedrock if you primarily need to use pre-trained foundation models for\n        inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is\n        a fully managed service for building generative AI applications with support for popular\n        foundation models, including Anthropic\n          Claude , Cohere Command\n          & Embed , AI21 Labs\n          Jurassic , Meta Llama , Mistral AI , Stable Diffusion XL and Amazon\n        Titan . Supported FMs are updated on a regular basis. Use Amazon Bedrock to build generative AI applications with security, privacy, and responsible\n        AI\u2014regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API\n        access, so you can use different foundation models, and upgrade to the latest model\n        versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom\n          models . Use Amazon Bedrock Studio (in\n        preview), which is a new SSO-enabled web interface that your developers can use to work with\n        large language models (LLMs) and other foundation models (FMs), collaborate on projects, and\n        iterate on generative AI applications. Amazon SageMaker Amazon SageMaker is a\n        fully managed service designed to help you build, train, and deploy machine learning models\n        at scale. This includes building FMs from scratch, using tools like notebooks, debuggers,\n        profilers,\u00a0pipelines, and MLOps. Consider SageMaker when you have use cases that can bene\ufb01t from\n        extensive training, \ufb01ne-tuning, and customization of foundation models. It can also help you\n        through the potentially challenging task of evaluating which FM is the best \ufb01t for your use\n        case. Use SageMaker\u2019s integrated development environment (IDE) to build, train, and deploy\n        FMs.\u00a0SageMaker offers access to hundreds of pretrained models, including publicly\u00a0available FMs. For more information about how Amazon Bedrock and SageMaker fit into Amazon\u2019s generative AI services and\n    solutions, see the generative AI\n      decision guide . While both Amazon Bedrock and Amazon SageMaker enable the development of ML and generative AI\n    applications, they serve different purposes. This guide will help you understand which of these\n    services is the best fit for your needs, including scenarios in which both services can be used\n    together to build generative AI applications. Here's a high-level view of the key differences between these services to get you started. Category Amazon Bedrock Amazon SageMaker Use Cases Ideal for integration of AI capabilities into\n            applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning\n            expertise Optimized for data scientists, machine learning engineers, and developers Customization You'll primarily use pre-trained models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other\n            services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for\n            building and optimizing models Differences between Amazon Bedrock and SageMaker Let's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker. Use cases Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. Target users Amazon Bedrock and Amazon SageMaker support different targeted users based on their level of expertise\n          and knowledge of machine learning and artificial intelligence. Amazon Bedrock Amazon Bedrock offers a more accessible and straightforward way to integrate AI\n              functionality into your projects. It\u2019s appropriate for a broad audience, which\n              includes developers and businesses, that has limited experience in building and\n              training machine learning models, but wants to use AI to enhance their applications or\n              workflows. Amazon SageMaker SageMaker is predominantly for data scientists, machine learning engineers, and\n              developers who possess the necessary skills and knowledge to build, train, and deploy\n              custom machine learning models. Use SageMaker if you are well-versed in data science and\n              machine learning concepts, and require a platform that provides you with the tools and\n              flexibility to create models tailored to your specific needs. Customization Amazon Bedrock and Amazon SageMaker offer different levels of customization capabilities that you can\n          tailor to your specific needs and expertise. Amazon Bedrock Amazon Bedrock provides pre-trained AI models that you can integrate into applications,\n              with limited customization. You have access to a set of API calls that you use to\n              enter data and receive predictions from these pre-trained models. While this approach\n              drastically simplifies the process of incorporating AI capabilities into applications,\n              it also means that you have less control over the underlying models, unless you\n              customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized\n              for common AI tasks and are designed to work well for a wide range of use cases, but\n              they may not be suitable for highly specialized or niche requirements. Amazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R,\n              Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan\n              Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West\n              (Oregon) AWS Region. The list of supported FMs is updated on an ongoing\n              basis. Customize models for specific tasks and use cases, including FM fine-tuning\n              and pre-training. Bring your own customized model with custom model\n                import (in preview). Amazon SageMaker Amazon SageMaker provides extensive customization options, giving you full control over the\n              entire machine learning workflow. With SageMaker, you can fine-tune every aspect of your\n              models, from data preprocessing and feature engineering to model architecture and\n              hyperparameter optimization. By using this level of customization, you can create\n              highly specialized models that are tailored to your unique business requirements. SageMaker\n              supports a wide range of popular machine learning frameworks, such as TensorFlow,\n              PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for\n              building and training models. Use Amazon SageMaker JumpStart to evaluate, compare, and select FMs based on\n              pre-defined quality and responsibility. Choose which FM to use with Amazon SageMaker\n                Clarify . Use SageMaker Clarify to create model evaluation jobs, that you use to\n              evaluate and compare model quality and responsibility metrics for text-based\n              foundation models from JumpStart. Generate predictions using Amazon SageMaker Canvas , without needing to\n              write any code.  Use SageMaker Canvas in collaboration with Amazon Bedrock to fine-tune and\n              deploy language models. This blog post describes how you can use them to optimize customer\n              interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock\n              and Amazon SageMaker JumpStart. Pricing Amazon Bedrock and Amazon SageMaker have different pricing\n        models that reflect their target users and the services they\n        provide. Amazon Bedrock Amazon Bedrock employs a simple pricing\n                model based on the number of API calls made to the service. You pay a fixed\n              price per API call, which includes the cost of running the pre-trained models and any\n              associated data processing. This straightforward pricing structure makes it more\n              efficient for you to estimate and control your costs, as you pay only for the actual\n              usage of the service. Amazon Bedrock's pricing model is particularly well-suited for\n              applications with predictable workloads, or for cases where you want more transparency\n              in your AI-related expenses. Amazon SageMaker SageMaker follows a pay-as-you-go pricing\n                model based on the usage of compute resources, storage, and other services\n              consumed during the machine learning process. You\u2019re charged for the instances that\n              you use to build, train, and deploy you models, with prices varying depending on the\n              instance type and size. Additionally, you incur costs for data storage, data transfer,\n              and other associated services like data labeling and model monitoring. This pricing\n              model provides flexibility and allows you to optimize costs based on your specific\n              requirements. However, it also means that costs can vary and may require careful\n              management, especially for resource-intensive projects. Integration Amazon Bedrock and Amazon SageMaker offer different approaches to integrating machine learning models\n          into applications, catering to your specific needs and expertise. Amazon Bedrock Amazon Bedrock simplifies the integration process by providing pre-trained models that you can\n              access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and\n              receive predictions from the models without needing to manage the underlying\n              infrastructure. This approach significantly reduces the complexity and time required\n              to integrate AI capabilities into applications, making it more accessible to\n              developers with limited machine learning expertise. However, this ease of integration\n              comes at the cost of limited customization options, as you\u2019re restricted to the\n              pre-trained models and APIs provided by Amazon Bedrock. Amazon SageMaker SageMaker provides a comprehensive platform for building, training, and deploying custom\n              machine learning models. However, integrating these models into applications requires\n              more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker SDK or\n              API to access the trained models and build the necessary infrastructure to expose them\n              as endpoints. This process involves creating and configuring API Gateway, Lambda\n              functions, and other AWS services to enable communication between the application\n              and the deployed model. While SageMaker provides tools and templates to simplify this\n              process, it still requires a deeper understanding of AWS services and machine\n              learning model deployment. Expertise required Amazon Bedrock and Amazon SageMaker are optimized for different levels of machine learning\n          expertise. Amazon Bedrock Amazon Bedrock is more accessible to a broader range of users, including developers and\n              businesses with limited machine learning expertise. By providing pre-trained models\n              that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away\n              much of the complexity associated with building and deploying machine learning models.\n              You don't need to worry about data preprocessing, model selection, or infrastructure\n              management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus\n              on integrating AI capabilities into your applications without needing to invest\n              significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker If you have deeper expertise in data science and machine learning, SageMaker provides a\n              powerful and flexible platform for building, training, and deploying custom models.\n              While SageMaker aims to simplify the machine learning workflow, it still requires a\n              significant level of technical expertise to take full advantage of its capabilities.\n              You\u2019ll benefit from being proficient in programming languages like Python, along with\n              a deep understanding of machine learning concepts, such as data preprocessing, model\n              selection, and hyperparameter tuning. Additionally, you should be comfortable working\n              with various AWS services and managing the infrastructure required to deploy and\n              integrate their models. As a result, SageMaker may have a steeper learning curve if you\u2019re\n              new to machine learning or have limited experience with AWS. anchor anchor anchor anchor anchor anchor Use cases Target users Customization Pricing Integration Expertise required Amazon Bedrock and Amazon SageMaker address different use cases based on your specific requirements\n          and resources. Amazon Bedrock Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI\n              capabilities into your applications without investing heavily in custom model\n              development. For example, a content moderation system for a social media platform\n              could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate\n              text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural\n              language processing capabilities to understand and respond to user inquiries. Amazon Bedrock is\n              particularly useful if you have limited machine learning expertise or resources, as it\n              helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker SageMaker is a good choice for unique or specialized AI/ML needs that require\n              custom-built models. It is ideal for scenarios where off-the-shelf solutions are not\n              sufficient, and you have a need for fine-grained control over the model architecture,\n              training process, and deployment. One example of a scenario that would benefit from\n              using SageMaker would be a healthcare company developing a model to predict patient\n              outcomes based on specific biomarkers. Another example would be a financial\n              institution creating a fraud detection system tailored to their unique data and risk\n              factors.  Additionally, SageMaker is suitable for research and development purposes, where\n              data scientists and machine learning engineers can experiment with different\n              algorithms, hyperparameters, and model architectures. The choice between Amazon Bedrock and Amazon SageMaker is not always mutually exclusive. In some\n    cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to\n    quickly prototype and deploy a foundation model, and then use SageMaker to further refine and\n    optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker together to optimize\n    customer interaction by working with your own datasets (such as your product FAQs. Ultimately, the decision between Amazon Bedrock and Amazon SageMaker depends on your specific\n    requirements. Evaluating these factors can help you make an informed decision and choose the\n    service that is most suitable for your needs. For more information about Amazon\u2019s generative AI services and solutions, see the generative AI\n      decision guide . Use Now that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker, you can\n    select the service that meets your needs, and use the following information  to help you get\n    started using each of them. Amazon Bedrock What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio What is Amazon Bedrock\n            Studio? Describes how to use this web app to prototype apps that use Amazon Bedrock models and\n              features, without having to set up and use a developer environment. Explore the guide Build generative AI applications with Amazon Bedrock Studio This blog post describes how you can build applications using a wide array of top\n              performing models. It then explains how to evaluate and share your generative AI apps\n              within Amazon Bedrock Studio. Read the blog Building an app with Amazon Bedrock\n            Studio Use the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock\n              models and features. You can also use the Build mode to try experiments not supported\n              in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker What is Amazon SageMaker? Describes how you can use this fully managed machine learning (ML) service to build,\n              train, and deploy ML models into a production-ready hosted environment. Explore the guide Get started with Amazon SageMaker Describes how to join an Amazon SageMaker domain, giving you access to Amazon SageMaker Studio and\n              RStudio on SageMaker. Explore the guide Get started with Amazon SageMaker\n            JumpStart Explore SageMaker JumpStart solution templates that set up\n          infrastructure for common use cases, and executable example\n          notebooks for machine learning with SageMaker. Explore the guide anchor anchor anchor Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker What is Amazon Bedrock? Describes how to use this fully managed service to make foundation models (FMs) from\n              Amazon and third parties available for your use through a unified API. Explore the guide Frequently asked questions about Amazon Bedrock Get answers to the most commonly-asked questions about Amazon Bedrock. These include how to\n              use agents, security considerations, details about Amazon Bedrock software development kits\n              (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs Guidance for generating product descriptions with\n                Amazon Bedrock Describes how to use Amazon Bedrock in your solution to automate your product review and\n              approval process for an e-commerce marketplace or retail website. Explore the solution Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Document history"}, {"title": "Amazon Braket Documentation", "url": "https://docs.aws.amazon.com/braket/?icmpid=docs_homepage_quantumcmpt", "content": "No main content found."}, {"title": "Amazon Chime Documentation", "url": "https://docs.aws.amazon.com/chime/?icmpid=docs_homepage_busapp", "content": "No main content found."}, {"title": "Amazon Chime SDK Documentation", "url": "https://docs.aws.amazon.com/chime-sdk/?icmpid=docs_homepage_busapp", "content": "No main content found."}, {"title": "Amazon Cloud Directory Documentation", "url": "https://docs.aws.amazon.com/clouddirectory/?icmpid=docs_homepage_security", "content": "No main content found."}, {"title": "Amazon CloudFront Documentation", "url": "https://docs.aws.amazon.com/cloudfront/?icmpid=docs_homepage_networking", "content": "No main content found."}, {"title": "Amazon CloudSearch Documentation", "url": "https://docs.aws.amazon.com/cloudsearch/?icmpid=docs_homepage_analytics", "content": "No main content found."}, {"title": "Amazon CloudWatch Documentation", "url": "https://docs.aws.amazon.com/cloudwatch/?icmpid=docs_homepage_mgmtgov", "content": "No main content found."}, {"title": "Amazon CodeCatalyst Documentation", "url": "https://docs.aws.amazon.com/codecatalyst?icmpid=docs_homepage_devtools", "content": "No main content found."}, {"title": "Amazon CodeGuru Documentation", "url": "https://docs.aws.amazon.com/codeguru/?icmpid=docs_homepage_ml", "content": "No main content found."}, {"title": "What is the AWS Command Line Interface? - AWS Command Line Interface", "url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html", "content": "What is the AWS Command Line Interface? PDF RSS The AWS Command Line Interface (AWS CLI) is an open source tool that enables you to interact with AWS\n        services using commands in your command-line shell. With minimal configuration, the AWS CLI\n        enables you to start running commands that implement functionality equivalent to that\n        provided by the browser-based AWS Management Console from the command prompt in your terminal\n        program: Linux shells \u2013 Use common shell programs\n                such as bash , zsh , and tcsh to run commands in\n                Linux or macOS. Windows command line \u2013 On Windows, run\n                commands at the Windows command prompt or in PowerShell. Remotely \u2013 Run commands on Amazon Elastic Compute Cloud\n                (Amazon EC2) instances through a remote terminal program such as PuTTY or SSH, or with\n                AWS Systems Manager. All IaaS (infrastructure as a service) AWS administration, management, and access\n        functions in the AWS Management Console are available in the AWS API and AWS CLI. New AWS IaaS features\n        and services provide full AWS Management Console functionality through the API and CLI at launch or\n        within 180 days of launch. The AWS CLI provides direct access to the public APIs of AWS services. You can explore a\n        service's capabilities with the AWS CLI, and develop shell scripts to manage your resources.\n        In addition to the low-level, API-equivalent commands, several AWS services provide\n        customizations for the AWS CLI. Customizations can include higher-level commands that simplify\n        using a service with a complex API. About\n                AWS CLI version 2 The AWS CLI version 2 is the most recent major version of the AWS CLI and supports all of the\n            latest features. Some features introduced in version 2 are not backported to version 1\n            and you must upgrade to access those features. There are some \"breaking\" changes from\n            version 1 that might require you to change your scripts. For a list of breaking changes\n            in version 2, see Migrating from AWS CLI version 1 to AWS CLI version 2 . The AWS CLI version 2 is available to install only as a bundled installer. While you might find\n            it in package managers, these are unsupported and unofficial packages that are not\n            produced or managed by AWS. We recommend that you install the AWS CLI from only the\n            official AWS distribution points, as documented in this guide. To install the AWS CLI version 2, see Installing or updating to the latest version of\n            the AWS CLI . To check the currently installed version, use the following command: $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/1.18.6 For version history, see the AWS CLI version 2\n                Changelog on GitHub . Maintenance and support for SDK major versions For information about maintenance and support for SDK major versions and their underlying\n       dependencies, see the following in the AWS SDKs and Tools Reference Guide : AWS SDKs and tools\n             maintenance policy AWS SDKs and\n             tools version support matrix About Amazon Web Services Amazon Web Services (AWS) is a collection of digital infrastructure \n      services that developers can leverage when developing their applications. \n      The services include computing, storage, database, and application \n      synchronization (messaging and queuing). AWS uses a pay-as-you-go service model.\n      You are charged only for the services that you\u2014or your applications\u2014use. \n      Also, to make AWS more approachable as a platform for prototyping and\n      experimentation, AWS offers a free usage tier. On this tier, services are free below\n      a certain level of usage. \n      For more information about AWS costs and the Free Tier, see AWS Free Tier . \n      To obtain an AWS account, open the AWS home page and then choose Create an AWS Account . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions About the examples"}, {"title": "Installing or updating to the latest version of the AWS CLI - AWS Command Line Interface", "url": "https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html", "content": "Installing or updating to the latest version of\n            the AWS CLI PDF RSS This topic describes how to install or update the latest release of the AWS Command Line Interface (AWS CLI)\n        on supported operating systems. For information on the latest releases of AWS CLI, see the AWS CLI version 2\n            Changelog on GitHub. To install a past release of the AWS CLI, see Installing past releases of the AWS CLI version 2 . For\n        uninstall instructions, see Uninstalling the AWS CLI version 2 . Important AWS CLI versions 1 and 2 use the same aws command name. If you previously\n            installed AWS CLI version 1, see Migrating from AWS CLI version 1 to AWS CLI version 2 . Topics AWS CLI install and update\n                instructions Troubleshooting AWS CLI install and uninstall\n                errors Next steps AWS CLI install and update\n                instructions For installation instructions, expand the section for your operating system. Install and update requirements You must be able to extract or \"unzip\" the downloaded package. If\n                            your operating system doesn't have the built-in unzip command, use an equivalent. The AWS CLI uses glibc , groff , and less . These are included by default in most major\n                            distributions of Linux. We support the AWS CLI on 64-bit versions of recent distributions of\n                            CentOS, Fedora, Ubuntu, Amazon Linux 1, Amazon Linux 2, Amazon Linux 2023, and Linux\n                            ARM. Because AWS doesn't maintain third-party repositories other than snap , we can\u2019t guarantee that they contain the latest\n                            version of the AWS CLI. Install or\n                        update the AWS CLI Warning If this is your first time updating on Amazon Linux, to install the latest\n                        version of the AWS CLI, you must uninstall the pre-installed yum version using the following command: $ sudo yum remove awscli After the yum installation of the AWS CLI is removed,\n                        follow the below Linux install instructions. You can install the AWS CLI by using one of the following methods: The command line installer is\n                                good option for version control, as you can specify the version to\n                                install. This option does not auto-update and you must download a\n                                new installer each time you update to overwrite previous\n                                version. The officially supported snap package is a good option to always have the latest\n                                version of the AWS CLI as snap packages automatically refresh. There\n                                is no built-in support for selecting minor versions of AWS CLI and\n                                therefore is not an optimal install method if your team needs to pin\n                                versions. Command line installer - Linux x86 (64-bit) To update your current installation of AWS CLI, download a new\n                                    installer each time you update to overwrite previous versions.\n                                    Follow these steps from the command line to install the AWS CLI on\n                                    Linux. The following are quick installation steps in a single copy\n                                    and paste group that provide a basic installation. For guided\n                                    instructions, see the steps that follow. Note (Optional) The following command block downloads and installs\n                                                  the AWS CLI without first verifying the integrity of\n                                                  your download. To verify the integrity of your\n                                                  download, use the below step by step\n                                                  instructions. To install the\n                                                AWS CLI, run the following commands. $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install To update your current\n                                                  installation of the AWS CLI, add your\n                                                existing symlink and installer information to\n                                                construct the install command using the --bin-dir , --install-dir , and --update parameters. The following\n                                                command block uses an example symlink of /usr/local/bin and\n                                                example installer location of /usr/local/aws-cli . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update Guided installation steps Download the installation file in one of the following\n                                            ways: Use the curl command \u2013 The -o option specifies the file name\n                                                  that the downloaded package is written to. The\n                                                  options on the following example command write the\n                                                  downloaded file to the current directory with the\n                                                  local name awscliv2.zip . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" Downloading from the\n                                                  URL \u2013 To download the installer\n                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip (Optional) Verifying the\n                                                integrity of your downloaded zip\n                                            file If you chose to manually download the AWS CLI installer\n                                            package .zip in the above steps,\n                                            you can use the following steps to verify the signatures\n                                            by using the GnuPG tool. The AWS CLI installer package .zip files are cryptographically signed using PGP signatures.\n                                            If there is any damage or alteration of the files, this\n                                            verification fails and you should not proceed with\n                                            installation. Download and install the gpg command using your package manager. For more\n                                                  information about GnuPG , see the GnuPG\n                                                  website . To create the public key file, create a text\n                                                  file and paste in the following text. -----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG\nZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx\nPqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G\nTfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz\ngbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk\nC6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG\n94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO\nlrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG\nfYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG\nEEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX\nXDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB\ntCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF\nCwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC\nZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA\n0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI\nyWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz\nVR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck\nbVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF\n0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+\n2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG\n+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs\n19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7\nIRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261\nEycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf\nBfWC9s/USgxchg==\n=ptgS\n-----END PGP PUBLIC KEY BLOCK----- For reference, the following are the details\n                                                  of the public key. Key ID:           A6310ACC4672475C\nType:             RSA\nSize:             4096/4096\nCreated:          2019-09-18\nExpires:          2025-07-24\nUser ID:          AWS CLI Team <aws-cli@amazon.com>\nKey fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Import the AWS CLI public key with the following\n                                                  command, substituting public-key-file-name with the file name of the public key you\n                                                  created. $ gpg --import public -key-file-name gpg: /home/ username /.gnupg/trustdb.gpg: trustdb created\ngpg: key A6310ACC4672475C: public key \"AWS CLI Team <aws-cli@amazon.com>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1 Download the AWS CLI signature file for the\n                                                  package you downloaded. It has the same path and\n                                                  name as the .zip file it corresponds\n                                                  to, but has the extension .sig . In\n                                                  the following examples, we save it to the current\n                                                  directory as a file named awscliv2.sig . For the latest version\n                                                  of the AWS CLI, use the following command\n                                                  block: $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip.sig For a specific version\n                                                  of the AWS CLI, append a hyphen and the\n                                                  version number to the filename. For this example\n                                                  the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip.sig resulting in the following command: $ curl -o awscliv2.sig https://awscli.amazonaws.com/ awscli -exe-linux-x 86 _ 64 - 2 . 0 . 30 .zip.sig For a list of versions, see the AWS CLI version 2 Changelog on GitHub . Verify the signature, passing both the\n                                                  downloaded .sig and .zip file names as parameters to the gpg command. $ gpg --verify awscliv2.sig awscliv2.zip The output should look similar to the\n                                                  following. gpg: Signature made Mon Nov  4 19:00:01 2019 PST\ngpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C\ngpg: Good signature from \"AWS CLI Team <aws-cli@amazon.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Important The warning in the output is expected and\n                                                  doesn't indicate a problem. It occurs because\n                                                  there isn't a chain of trust between your personal\n                                                  PGP key (if you have one) and the AWS CLI PGP key.\n                                                  For more information, see Web of trust . Unzip the installer. If your Linux distribution\n                                            doesn't have a built-in unzip command, use\n                                            an equivalent to unzip it. The following example command\n                                            unzips the package and creates a directory named aws under the current\n                                            directory. $ unzip awscliv2.zip Note When updating from a previous version, the unzip command prompts to overwrite\n                                                existing files. To skip these prompts, such as with\n                                                script automation, use the -u update\n                                                flag for unzip . This flag automatically\n                                                updates existing files and creates new ones as\n                                                needed. $ unzip -u awscliv2.zip Run the install program. The installation command uses\n                                            a file named install in the newly\n                                            unzipped aws directory. By default,\n                                            the files are all installed to /usr/local/aws-cli , and a\n                                            symbolic link is created in /usr/local/bin . The command\n                                            includes sudo to grant write permissions to\n                                            those directories. $ sudo ./aws/install You can install without sudo if you\n                                            specify directories that you already have write\n                                            permissions to. Use the following instructions for the install command to specify the\n                                            installation location: Ensure that the paths you provide to the -i and -b parameters\n                                                  contain no volume name or directory names that\n                                                  contain any space characters or other white space\n                                                  characters. If there is a space, the installation\n                                                  fails. --install-dir or -i \u2013 This option specifies the directory to\n                                                  copy all of the files to. The default value is /usr/local/aws-cli . --bin-dir or -b \u2013 This option specifies that the main aws program in the install directory\n                                                  is symbolically linked to the file aws in the specified path.\n                                                  You must have write permissions to the specified\n                                                  directory. Creating a symlink to a directory that\n                                                  is already in your path eliminates the need to add\n                                                  the install directory to the user's $PATH variable. The default value is /usr/local/bin . $ ./aws/install -i /usr/ local /aws-cli -b /usr/ local /bin Note To update your current installation of the AWS CLI,\n                                                add your existing symlink and installer information\n                                                to construct the install command with\n                                                the --update parameter. $ sudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update To locate the existing symlink and installation\n                                                directory, use the following steps: Use the which command to find\n                                                  your symlink. This gives you the path to use with\n                                                  the --bin-dir parameter. $ which aws /usr/ local /bin /aws Use the ls command to find the\n                                                  directory that your symlink points to. This gives\n                                                  you the path to use with the --install-dir parameter. $ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/ local /aws-cli /v2/current/bin/aws Confirm the installation with the following command. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Command line - Linux ARM To update your current installation of AWS CLI, download a new\n                                    installer each time you update to overwrite previous versions.\n                                    Follow these steps from the command line to install the AWS CLI on\n                                    Linux. The following are quick installation steps in a single copy\n                                    and paste group that provide a basic installation. For guided\n                                    instructions, see the steps that follow. Note (Optional) The following\n                                        command block downloads and installs the AWS CLI without first\n                                        verifying the integrity of your download. To verify the\n                                        integrity of your download, use the below step by step\n                                        instructions. To install the AWS CLI, run the\n                                    following commands. $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install To update your current\n                                        installation of the AWS CLI, add your existing\n                                    symlink and installer information to construct the install command using the --bin-dir , --install-dir , and --update parameters. The following command\n                                    block uses an example symlink of /usr/local/bin and example\n                                    installer location of /usr/local/aws-cli . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update Guided installation steps Download the installation file in one of the following\n                                            ways: Use the curl command \u2013 The -o option specifies the file name\n                                                  that the downloaded package is written to. The\n                                                  options on the following example command write the\n                                                  downloaded file to the current directory with the\n                                                  local name awscliv2.zip . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\" Downloading from the\n                                                  URL \u2013 To download the installer\n                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip (Optional) Verifying the\n                                                integrity of your downloaded zip\n                                            file If you chose to manually download the AWS CLI installer\n                                            package .zip in the above steps,\n                                            you can use the following steps to verify the signatures\n                                            by using the GnuPG tool. The AWS CLI installer package .zip files are cryptographically signed using PGP signatures.\n                                            If there is any damage or alteration of the files, this\n                                            verification fails and you should not proceed with\n                                            installation. Download and install the gpg command using your package manager. For more\n                                                  information about GnuPG , see the GnuPG\n                                                  website . To create the public key file, create a text\n                                                  file and paste in the following text. -----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG\nZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx\nPqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G\nTfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz\ngbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk\nC6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG\n94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO\nlrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG\nfYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG\nEEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX\nXDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB\ntCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF\nCwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC\nZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA\n0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI\nyWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz\nVR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck\nbVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF\n0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+\n2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG\n+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs\n19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7\nIRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261\nEycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf\nBfWC9s/USgxchg==\n=ptgS\n-----END PGP PUBLIC KEY BLOCK----- For reference, the following are the details\n                                                  of the public key. Key ID:           A6310ACC4672475C\nType:             RSA\nSize:             4096/4096\nCreated:          2019-09-18\nExpires:          2025-07-24\nUser ID:          AWS CLI Team <aws-cli@amazon.com>\nKey fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Import the AWS CLI public key with the following\n                                                  command, substituting public-key-file-name with the file name of the public key you\n                                                  created. $ gpg --import public -key-file-name gpg: /home/ username /.gnupg/trustdb.gpg: trustdb created\ngpg: key A6310ACC4672475C: public key \"AWS CLI Team <aws-cli@amazon.com>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1 Download the AWS CLI signature file for the\n                                                  package you downloaded. It has the same path and\n                                                  name as the .zip file it corresponds\n                                                  to, but has the extension .sig . In\n                                                  the following examples, we save it to the current\n                                                  directory as a file named awscliv2.sig . For the latest version\n                                                  of the AWS CLI, use the following command\n                                                  block: $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip.sig For a specific version\n                                                  of the AWS CLI, append a hyphen and the\n                                                  version number to the filename. For this example\n                                                  the filename for version 2.0.30 would be awscli-exe-linux-aarch64-2.0.30.zip.sig resulting in the following command: $ curl -o awscliv2.sig https://awscli.amazonaws.com/ awscli -exe-linux-aarch 64 - 2 . 0 . 30 .zip.sig For a list of versions, see the AWS CLI version 2 Changelog on GitHub . Verify the signature, passing both the\n                                                  downloaded .sig and .zip file names as parameters to the gpg command. $ gpg --verify awscliv2.sig awscliv2.zip The output should look similar to the\n                                                  following. gpg: Signature made Mon Nov  4 19:00:01 2019 PST\ngpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C\ngpg: Good signature from \"AWS CLI Team <aws-cli@amazon.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Important The warning in the output is expected and\n                                                  doesn't indicate a problem. It occurs because\n                                                  there isn't a chain of trust between your personal\n                                                  PGP key (if you have one) and the AWS CLI PGP key.\n                                                  For more information, see Web of trust . Unzip the installer. If your Linux distribution\n                                            doesn't have a built-in unzip command, use\n                                            an equivalent to unzip it. The following example command\n                                            unzips the package and creates a directory named aws under the current\n                                            directory. $ unzip awscliv2.zip Note When updating from a previous version, the unzip command prompts to overwrite\n                                                existing files. To skip these prompts, such as with\n                                                script automation, use the -u update\n                                                flag for unzip . This flag automatically\n                                                updates existing files and creates new ones as\n                                                needed. $ unzip -u awscliv2.zip Run the install program. The installation command uses\n                                            a file named install in the newly\n                                            unzipped aws directory. By default,\n                                            the files are all installed to /usr/local/aws-cli , and a\n                                            symbolic link is created in /usr/local/bin . The command\n                                            includes sudo to grant write permissions to\n                                            those directories. $ sudo ./aws/install You can install without sudo if you\n                                            specify directories that you already have write\n                                            permissions to. Use the following instructions for the install command to specify the\n                                            installation location: Ensure that the paths you provide to the -i and -b parameters\n                                                  contain no volume name or directory names that\n                                                  contain any space characters or other white space\n                                                  characters. If there is a space, the installation\n                                                  fails. --install-dir or -i \u2013 This option specifies the directory to\n                                                  copy all of the files to. The default value is /usr/local/aws-cli . --bin-dir or -b \u2013 This option specifies that the main aws program in the install directory\n                                                  is symbolically linked to the file aws in the specified path.\n                                                  You must have write permissions to the specified\n                                                  directory. Creating a symlink to a directory that\n                                                  is already in your path eliminates the need to add\n                                                  the install directory to the user's $PATH variable. The default value is /usr/local/bin . $ ./aws/install -i /usr/ local /aws-cli -b /usr/ local /bin Note To update your current installation of the AWS CLI,\n                                                add your existing symlink and installer information\n                                                to construct the install command with\n                                                the --update parameter. $ sudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update To locate the existing symlink and installation\n                                                directory, use the following steps: Use the which command to find\n                                                  your symlink. This gives you the path to use with\n                                                  the --bin-dir parameter. $ which aws /usr/ local /bin /aws Use the ls command to find the\n                                                  directory that your symlink points to. This gives\n                                                  you the path to use with the --install-dir parameter. $ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/ local /aws-cli /v2/current/bin/aws Confirm the installation with the following command. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Snap package We provide an official AWS supported version of the AWS CLI on snap . If you want to always have the latest\n                                    version of the AWS CLI installed on your system, a snap package\n                                    provides this for you as it auto-updates. There is no built-in\n                                    support for selecting minor versions of AWS CLI and therefore it\n                                    is not an optimal install method if your team needs to pin\n                                    versions. If you want to install a specific minor version of the\n                                    AWS CLI, we suggest you use the command line installer. If your Linux platform does not already have snap installed, install snap on your platform. For information on installing snap , see Installing the daemon in the Snap\n                                                  documentation . You may need to restart your system so that\n                                                  your PATH variables are updated\n                                                  correctly. If you are having installation issues,\n                                                  follow steps in Fix common issues in the Snap\n                                                  documentation . To verify that snap is installed\n                                                  correctly, run the following command. $ snap version Run the following snap install command\n                                            for the AWS CLI. $ snap install aws-cli --classic Depending on your permissions, you may need to add sudo to the command. $ sudo snap install aws-cli --classic Note To view the snap repository for the AWS CLI,\n                                                including additional snap instructions,\n                                                see the aws-cli page in the Canonical Snapcraft\n                                                  website . Verify that the AWS CLI installed correctly. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If you get an error, see Troubleshooting errors for the AWS CLI . Linux Install and update requirements You must be able to extract or \"unzip\" the downloaded package. If\n                            your operating system doesn't have the built-in unzip command, use an equivalent. The AWS CLI uses glibc , groff , and less . These are included by default in most major\n                            distributions of Linux. We support the AWS CLI on 64-bit versions of recent distributions of\n                            CentOS, Fedora, Ubuntu, Amazon Linux 1, Amazon Linux 2, Amazon Linux 2023, and Linux\n                            ARM. Because AWS doesn't maintain third-party repositories other than snap , we can\u2019t guarantee that they contain the latest\n                            version of the AWS CLI. Install or\n                        update the AWS CLI Warning If this is your first time updating on Amazon Linux, to install the latest\n                        version of the AWS CLI, you must uninstall the pre-installed yum version using the following command: $ sudo yum remove awscli After the yum installation of the AWS CLI is removed,\n                        follow the below Linux install instructions. You can install the AWS CLI by using one of the following methods: The command line installer is\n                                good option for version control, as you can specify the version to\n                                install. This option does not auto-update and you must download a\n                                new installer each time you update to overwrite previous\n                                version. The officially supported snap package is a good option to always have the latest\n                                version of the AWS CLI as snap packages automatically refresh. There\n                                is no built-in support for selecting minor versions of AWS CLI and\n                                therefore is not an optimal install method if your team needs to pin\n                                versions. Command line installer - Linux x86 (64-bit) To update your current installation of AWS CLI, download a new\n                                    installer each time you update to overwrite previous versions.\n                                    Follow these steps from the command line to install the AWS CLI on\n                                    Linux. The following are quick installation steps in a single copy\n                                    and paste group that provide a basic installation. For guided\n                                    instructions, see the steps that follow. Note (Optional) The following command block downloads and installs\n                                                  the AWS CLI without first verifying the integrity of\n                                                  your download. To verify the integrity of your\n                                                  download, use the below step by step\n                                                  instructions. To install the\n                                                AWS CLI, run the following commands. $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install To update your current\n                                                  installation of the AWS CLI, add your\n                                                existing symlink and installer information to\n                                                construct the install command using the --bin-dir , --install-dir , and --update parameters. The following\n                                                command block uses an example symlink of /usr/local/bin and\n                                                example installer location of /usr/local/aws-cli . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update Guided installation steps Download the installation file in one of the following\n                                            ways: Use the curl command \u2013 The -o option specifies the file name\n                                                  that the downloaded package is written to. The\n                                                  options on the following example command write the\n                                                  downloaded file to the current directory with the\n                                                  local name awscliv2.zip . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" Downloading from the\n                                                  URL \u2013 To download the installer\n                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip (Optional) Verifying the\n                                                integrity of your downloaded zip\n                                            file If you chose to manually download the AWS CLI installer\n                                            package .zip in the above steps,\n                                            you can use the following steps to verify the signatures\n                                            by using the GnuPG tool. The AWS CLI installer package .zip files are cryptographically signed using PGP signatures.\n                                            If there is any damage or alteration of the files, this\n                                            verification fails and you should not proceed with\n                                            installation. Download and install the gpg command using your package manager. For more\n                                                  information about GnuPG , see the GnuPG\n                                                  website . To create the public key file, create a text\n                                                  file and paste in the following text. -----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG\nZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx\nPqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G\nTfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz\ngbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk\nC6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG\n94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO\nlrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG\nfYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG\nEEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX\nXDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB\ntCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF\nCwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC\nZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA\n0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI\nyWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz\nVR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck\nbVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF\n0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+\n2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG\n+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs\n19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7\nIRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261\nEycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf\nBfWC9s/USgxchg==\n=ptgS\n-----END PGP PUBLIC KEY BLOCK----- For reference, the following are the details\n                                                  of the public key. Key ID:           A6310ACC4672475C\nType:             RSA\nSize:             4096/4096\nCreated:          2019-09-18\nExpires:          2025-07-24\nUser ID:          AWS CLI Team <aws-cli@amazon.com>\nKey fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Import the AWS CLI public key with the following\n                                                  command, substituting public-key-file-name with the file name of the public key you\n                                                  created. $ gpg --import public -key-file-name gpg: /home/ username /.gnupg/trustdb.gpg: trustdb created\ngpg: key A6310ACC4672475C: public key \"AWS CLI Team <aws-cli@amazon.com>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1 Download the AWS CLI signature file for the\n                                                  package you downloaded. It has the same path and\n                                                  name as the .zip file it corresponds\n                                                  to, but has the extension .sig . In\n                                                  the following examples, we save it to the current\n                                                  directory as a file named awscliv2.sig . For the latest version\n                                                  of the AWS CLI, use the following command\n                                                  block: $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip.sig For a specific version\n                                                  of the AWS CLI, append a hyphen and the\n                                                  version number to the filename. For this example\n                                                  the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip.sig resulting in the following command: $ curl -o awscliv2.sig https://awscli.amazonaws.com/ awscli -exe-linux-x 86 _ 64 - 2 . 0 . 30 .zip.sig For a list of versions, see the AWS CLI version 2 Changelog on GitHub . Verify the signature, passing both the\n                                                  downloaded .sig and .zip file names as parameters to the gpg command. $ gpg --verify awscliv2.sig awscliv2.zip The output should look similar to the\n                                                  following. gpg: Signature made Mon Nov  4 19:00:01 2019 PST\ngpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C\ngpg: Good signature from \"AWS CLI Team <aws-cli@amazon.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Important The warning in the output is expected and\n                                                  doesn't indicate a problem. It occurs because\n                                                  there isn't a chain of trust between your personal\n                                                  PGP key (if you have one) and the AWS CLI PGP key.\n                                                  For more information, see Web of trust . Unzip the installer. If your Linux distribution\n                                            doesn't have a built-in unzip command, use\n                                            an equivalent to unzip it. The following example command\n                                            unzips the package and creates a directory named aws under the current\n                                            directory. $ unzip awscliv2.zip Note When updating from a previous version, the unzip command prompts to overwrite\n                                                existing files. To skip these prompts, such as with\n                                                script automation, use the -u update\n                                                flag for unzip . This flag automatically\n                                                updates existing files and creates new ones as\n                                                needed. $ unzip -u awscliv2.zip Run the install program. The installation command uses\n                                            a file named install in the newly\n                                            unzipped aws directory. By default,\n                                            the files are all installed to /usr/local/aws-cli , and a\n                                            symbolic link is created in /usr/local/bin . The command\n                                            includes sudo to grant write permissions to\n                                            those directories. $ sudo ./aws/install You can install without sudo if you\n                                            specify directories that you already have write\n                                            permissions to. Use the following instructions for the install command to specify the\n                                            installation location: Ensure that the paths you provide to the -i and -b parameters\n                                                  contain no volume name or directory names that\n                                                  contain any space characters or other white space\n                                                  characters. If there is a space, the installation\n                                                  fails. --install-dir or -i \u2013 This option specifies the directory to\n                                                  copy all of the files to. The default value is /usr/local/aws-cli . --bin-dir or -b \u2013 This option specifies that the main aws program in the install directory\n                                                  is symbolically linked to the file aws in the specified path.\n                                                  You must have write permissions to the specified\n                                                  directory. Creating a symlink to a directory that\n                                                  is already in your path eliminates the need to add\n                                                  the install directory to the user's $PATH variable. The default value is /usr/local/bin . $ ./aws/install -i /usr/ local /aws-cli -b /usr/ local /bin Note To update your current installation of the AWS CLI,\n                                                add your existing symlink and installer information\n                                                to construct the install command with\n                                                the --update parameter. $ sudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update To locate the existing symlink and installation\n                                                directory, use the following steps: Use the which command to find\n                                                  your symlink. This gives you the path to use with\n                                                  the --bin-dir parameter. $ which aws /usr/ local /bin /aws Use the ls command to find the\n                                                  directory that your symlink points to. This gives\n                                                  you the path to use with the --install-dir parameter. $ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/ local /aws-cli /v2/current/bin/aws Confirm the installation with the following command. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Command line - Linux ARM To update your current installation of AWS CLI, download a new\n                                    installer each time you update to overwrite previous versions.\n                                    Follow these steps from the command line to install the AWS CLI on\n                                    Linux. The following are quick installation steps in a single copy\n                                    and paste group that provide a basic installation. For guided\n                                    instructions, see the steps that follow. Note (Optional) The following\n                                        command block downloads and installs the AWS CLI without first\n                                        verifying the integrity of your download. To verify the\n                                        integrity of your download, use the below step by step\n                                        instructions. To install the AWS CLI, run the\n                                    following commands. $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install To update your current\n                                        installation of the AWS CLI, add your existing\n                                    symlink and installer information to construct the install command using the --bin-dir , --install-dir , and --update parameters. The following command\n                                    block uses an example symlink of /usr/local/bin and example\n                                    installer location of /usr/local/aws-cli . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update Guided installation steps Download the installation file in one of the following\n                                            ways: Use the curl command \u2013 The -o option specifies the file name\n                                                  that the downloaded package is written to. The\n                                                  options on the following example command write the\n                                                  downloaded file to the current directory with the\n                                                  local name awscliv2.zip . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\" Downloading from the\n                                                  URL \u2013 To download the installer\n                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip (Optional) Verifying the\n                                                integrity of your downloaded zip\n                                            file If you chose to manually download the AWS CLI installer\n                                            package .zip in the above steps,\n                                            you can use the following steps to verify the signatures\n                                            by using the GnuPG tool. The AWS CLI installer package .zip files are cryptographically signed using PGP signatures.\n                                            If there is any damage or alteration of the files, this\n                                            verification fails and you should not proceed with\n                                            installation. Download and install the gpg command using your package manager. For more\n                                                  information about GnuPG , see the GnuPG\n                                                  website . To create the public key file, create a text\n                                                  file and paste in the following text. -----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG\nZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx\nPqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G\nTfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz\ngbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk\nC6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG\n94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO\nlrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG\nfYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG\nEEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX\nXDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB\ntCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF\nCwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC\nZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA\n0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI\nyWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz\nVR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck\nbVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF\n0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+\n2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG\n+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs\n19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7\nIRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261\nEycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf\nBfWC9s/USgxchg==\n=ptgS\n-----END PGP PUBLIC KEY BLOCK----- For reference, the following are the details\n                                                  of the public key. Key ID:           A6310ACC4672475C\nType:             RSA\nSize:             4096/4096\nCreated:          2019-09-18\nExpires:          2025-07-24\nUser ID:          AWS CLI Team <aws-cli@amazon.com>\nKey fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Import the AWS CLI public key with the following\n                                                  command, substituting public-key-file-name with the file name of the public key you\n                                                  created. $ gpg --import public -key-file-name gpg: /home/ username /.gnupg/trustdb.gpg: trustdb created\ngpg: key A6310ACC4672475C: public key \"AWS CLI Team <aws-cli@amazon.com>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1 Download the AWS CLI signature file for the\n                                                  package you downloaded. It has the same path and\n                                                  name as the .zip file it corresponds\n                                                  to, but has the extension .sig . In\n                                                  the following examples, we save it to the current\n                                                  directory as a file named awscliv2.sig . For the latest version\n                                                  of the AWS CLI, use the following command\n                                                  block: $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip.sig For a specific version\n                                                  of the AWS CLI, append a hyphen and the\n                                                  version number to the filename. For this example\n                                                  the filename for version 2.0.30 would be awscli-exe-linux-aarch64-2.0.30.zip.sig resulting in the following command: $ curl -o awscliv2.sig https://awscli.amazonaws.com/ awscli -exe-linux-aarch 64 - 2 . 0 . 30 .zip.sig For a list of versions, see the AWS CLI version 2 Changelog on GitHub . Verify the signature, passing both the\n                                                  downloaded .sig and .zip file names as parameters to the gpg command. $ gpg --verify awscliv2.sig awscliv2.zip The output should look similar to the\n                                                  following. gpg: Signature made Mon Nov  4 19:00:01 2019 PST\ngpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C\ngpg: Good signature from \"AWS CLI Team <aws-cli@amazon.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Important The warning in the output is expected and\n                                                  doesn't indicate a problem. It occurs because\n                                                  there isn't a chain of trust between your personal\n                                                  PGP key (if you have one) and the AWS CLI PGP key.\n                                                  For more information, see Web of trust . Unzip the installer. If your Linux distribution\n                                            doesn't have a built-in unzip command, use\n                                            an equivalent to unzip it. The following example command\n                                            unzips the package and creates a directory named aws under the current\n                                            directory. $ unzip awscliv2.zip Note When updating from a previous version, the unzip command prompts to overwrite\n                                                existing files. To skip these prompts, such as with\n                                                script automation, use the -u update\n                                                flag for unzip . This flag automatically\n                                                updates existing files and creates new ones as\n                                                needed. $ unzip -u awscliv2.zip Run the install program. The installation command uses\n                                            a file named install in the newly\n                                            unzipped aws directory. By default,\n                                            the files are all installed to /usr/local/aws-cli , and a\n                                            symbolic link is created in /usr/local/bin . The command\n                                            includes sudo to grant write permissions to\n                                            those directories. $ sudo ./aws/install You can install without sudo if you\n                                            specify directories that you already have write\n                                            permissions to. Use the following instructions for the install command to specify the\n                                            installation location: Ensure that the paths you provide to the -i and -b parameters\n                                                  contain no volume name or directory names that\n                                                  contain any space characters or other white space\n                                                  characters. If there is a space, the installation\n                                                  fails. --install-dir or -i \u2013 This option specifies the directory to\n                                                  copy all of the files to. The default value is /usr/local/aws-cli . --bin-dir or -b \u2013 This option specifies that the main aws program in the install directory\n                                                  is symbolically linked to the file aws in the specified path.\n                                                  You must have write permissions to the specified\n                                                  directory. Creating a symlink to a directory that\n                                                  is already in your path eliminates the need to add\n                                                  the install directory to the user's $PATH variable. The default value is /usr/local/bin . $ ./aws/install -i /usr/ local /aws-cli -b /usr/ local /bin Note To update your current installation of the AWS CLI,\n                                                add your existing symlink and installer information\n                                                to construct the install command with\n                                                the --update parameter. $ sudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update To locate the existing symlink and installation\n                                                directory, use the following steps: Use the which command to find\n                                                  your symlink. This gives you the path to use with\n                                                  the --bin-dir parameter. $ which aws /usr/ local /bin /aws Use the ls command to find the\n                                                  directory that your symlink points to. This gives\n                                                  you the path to use with the --install-dir parameter. $ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/ local /aws-cli /v2/current/bin/aws Confirm the installation with the following command. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Snap package We provide an official AWS supported version of the AWS CLI on snap . If you want to always have the latest\n                                    version of the AWS CLI installed on your system, a snap package\n                                    provides this for you as it auto-updates. There is no built-in\n                                    support for selecting minor versions of AWS CLI and therefore it\n                                    is not an optimal install method if your team needs to pin\n                                    versions. If you want to install a specific minor version of the\n                                    AWS CLI, we suggest you use the command line installer. If your Linux platform does not already have snap installed, install snap on your platform. For information on installing snap , see Installing the daemon in the Snap\n                                                  documentation . You may need to restart your system so that\n                                                  your PATH variables are updated\n                                                  correctly. If you are having installation issues,\n                                                  follow steps in Fix common issues in the Snap\n                                                  documentation . To verify that snap is installed\n                                                  correctly, run the following command. $ snap version Run the following snap install command\n                                            for the AWS CLI. $ snap install aws-cli --classic Depending on your permissions, you may need to add sudo to the command. $ sudo snap install aws-cli --classic Note To view the snap repository for the AWS CLI,\n                                                including additional snap instructions,\n                                                see the aws-cli page in the Canonical Snapcraft\n                                                  website . Verify that the AWS CLI installed correctly. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If you get an error, see Troubleshooting errors for the AWS CLI . anchor anchor anchor Command line installer - Linux x86 (64-bit) Command line - Linux ARM Snap package To update your current installation of AWS CLI, download a new\n                                    installer each time you update to overwrite previous versions.\n                                    Follow these steps from the command line to install the AWS CLI on\n                                    Linux. The following are quick installation steps in a single copy\n                                    and paste group that provide a basic installation. For guided\n                                    instructions, see the steps that follow. Note (Optional) The following command block downloads and installs\n                                                  the AWS CLI without first verifying the integrity of\n                                                  your download. To verify the integrity of your\n                                                  download, use the below step by step\n                                                  instructions. To install the\n                                                AWS CLI, run the following commands. $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install To update your current\n                                                  installation of the AWS CLI, add your\n                                                existing symlink and installer information to\n                                                construct the install command using the --bin-dir , --install-dir , and --update parameters. The following\n                                                command block uses an example symlink of /usr/local/bin and\n                                                example installer location of /usr/local/aws-cli . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update Guided installation steps Download the installation file in one of the following\n                                            ways: Use the curl command \u2013 The -o option specifies the file name\n                                                  that the downloaded package is written to. The\n                                                  options on the following example command write the\n                                                  downloaded file to the current directory with the\n                                                  local name awscliv2.zip . $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" Downloading from the\n                                                  URL \u2013 To download the installer\n                                                  with your browser, use the following URL: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip (Optional) Verifying the\n                                                integrity of your downloaded zip\n                                            file If you chose to manually download the AWS CLI installer\n                                            package .zip in the above steps,\n                                            you can use the following steps to verify the signatures\n                                            by using the GnuPG tool. The AWS CLI installer package .zip files are cryptographically signed using PGP signatures.\n                                            If there is any damage or alteration of the files, this\n                                            verification fails and you should not proceed with\n                                            installation. Download and install the gpg command using your package manager. For more\n                                                  information about GnuPG , see the GnuPG\n                                                  website . To create the public key file, create a text\n                                                  file and paste in the following text. -----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBF2Cr7UBEADJZHcgusOJl7ENSyumXh85z0TRV0xJorM2B/JL0kHOyigQluUG\nZMLhENaG0bYatdrKP+3H91lvK050pXwnO/R7fB/FSTouki4ciIx5OuLlnJZIxSzx\nPqGl0mkxImLNbGWoi6Lto0LYxqHN2iQtzlwTVmq9733zd3XfcXrZ3+LblHAgEt5G\nTfNxEKJ8soPLyWmwDH6HWCnjZ/aIQRBTIQ05uVeEoYxSh6wOai7ss/KveoSNBbYz\ngbdzoqI2Y8cgH2nbfgp3DSasaLZEdCSsIsK1u05CinE7k2qZ7KgKAUIcT/cR/grk\nC6VwsnDU0OUCideXcQ8WeHutqvgZH1JgKDbznoIzeQHJD238GEu+eKhRHcz8/jeG\n94zkcgJOz3KbZGYMiTh277Fvj9zzvZsbMBCedV1BTg3TqgvdX4bdkhf5cH+7NtWO\nlrFj6UwAsGukBTAOxC0l/dnSmZhJ7Z1KmEWilro/gOrjtOxqRQutlIqG22TaqoPG\nfYVN+en3Zwbt97kcgZDwqbuykNt64oZWc4XKCa3mprEGC3IbJTBFqglXmZ7l9ywG\nEEUJYOlb2XrSuPWml39beWdKM8kzr1OjnlOm6+lpTRCBfo0wa9F8YZRhHPAkwKkX\nXDeOGpWRj4ohOx0d2GWkyV5xyN14p2tQOCdOODmz80yUTgRpPVQUtOEhXQARAQAB\ntCFBV1MgQ0xJIFRlYW0gPGF3cy1jbGlAYW1hem9uLmNvbT6JAlQEEwEIAD4CGwMF\nCwkIBwIGFQoJCAsCBBYCAwECHgECF4AWIQT7Xbd/1cEYuAURraimMQrMRnJHXAUC\nZqFYbwUJCv/cOgAKCRCmMQrMRnJHXKYuEAC+wtZ611qQtOl0t5spM9SWZuszbcyA\n0xBAJq2pncnp6wdCOkuAPu4/R3UCIoD2C49MkLj9Y0Yvue8CCF6OIJ8L+fKBv2DI\nyWZGmHL0p9wa/X8NCKQrKxK1gq5PuCzi3f3SqwfbZuZGeK/ubnmtttWXpUtuU/Iz\nVR0u/0sAy3j4uTGKh2cX7XnZbSqgJhUk9H324mIJiSwzvw1Ker6xtH/LwdBeJCck\nbVBdh3LZis4zuD4IZeBO1vRvjot3Oq4xadUv5RSPATg7T1kivrtLCnwvqc6L4LnF\n0OkNysk94L3LQSHyQW2kQS1cVwr+yGUSiSp+VvMbAobAapmMJWP6e/dKyAUGIX6+\n2waLdbBs2U7MXznx/2ayCLPH7qCY9cenbdj5JhG9ibVvFWqqhSo22B/URQE/CMrG\n+3xXwtHEBoMyWEATr1tWwn2yyQGbkUGANneSDFiTFeoQvKNyyCFTFO1F2XKCcuDs\n19nj34PE2TJilTG2QRlMr4D0NgwLLAMg2Los1CK6nXWnImYHKuaKS9LVaCoC8vu7\nIRBik1NX6SjrQnftk0M9dY+s0ZbAN1gbdjZ8H3qlbl/4TxMdr87m8LP4FZIIo261\nEycv34pVkCePZiP+dgamEiQJ7IL4ZArio9mv6HbDGV6mLY45+l6/0EzCwkI5IyIf\nBfWC9s/USgxchg==\n=ptgS\n-----END PGP PUBLIC KEY BLOCK----- For reference, the following are the details\n                                                  of the public key. Key ID:           A6310ACC4672475C\nType:             RSA\nSize:             4096/4096\nCreated:          2019-09-18\nExpires:          2025-07-24\nUser ID:          AWS CLI Team <aws-cli@amazon.com>\nKey fingerprint:  FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Import the AWS CLI public key with the following\n                                                  command, substituting public-key-file-name with the file name of the public key you\n                                                  created. $ gpg --import public -key-file-name gpg: /home/ username /.gnupg/trustdb.gpg: trustdb created\ngpg: key A6310ACC4672475C: public key \"AWS CLI Team <aws-cli@amazon.com>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1 Download the AWS CLI signature file for the\n                                                  package you downloaded. It has the same path and\n                                                  name as the .zip file it corresponds\n                                                  to, but has the extension .sig . In\n                                                  the following examples, we save it to the current\n                                                  directory as a file named awscliv2.sig . For the latest version\n                                                  of the AWS CLI, use the following command\n                                                  block: $ curl -o awscliv2.sig https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip.sig For a specific version\n                                                  of the AWS CLI, append a hyphen and the\n                                                  version number to the filename. For this example\n                                                  the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip.sig resulting in the following command: $ curl -o awscliv2.sig https://awscli.amazonaws.com/ awscli -exe-linux-x 86 _ 64 - 2 . 0 . 30 .zip.sig For a list of versions, see the AWS CLI version 2 Changelog on GitHub . Verify the signature, passing both the\n                                                  downloaded .sig and .zip file names as parameters to the gpg command. $ gpg --verify awscliv2.sig awscliv2.zip The output should look similar to the\n                                                  following. gpg: Signature made Mon Nov  4 19:00:01 2019 PST\ngpg:                using RSA key FB5D B77F D5C1 18B8 0511 ADA8 A631 0ACC 4672 475C\ngpg: Good signature from \"AWS CLI Team <aws-cli@amazon.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: FB5D B77F D5C1 18B8 0511  ADA8 A631 0ACC 4672 475C Important The warning in the output is expected and\n                                                  doesn't indicate a problem. It occurs because\n                                                  there isn't a chain of trust between your personal\n                                                  PGP key (if you have one) and the AWS CLI PGP key.\n                                                  For more information, see Web of trust . Unzip the installer. If your Linux distribution\n                                            doesn't have a built-in unzip command, use\n                                            an equivalent to unzip it. The following example command\n                                            unzips the package and creates a directory named aws under the current\n                                            directory. $ unzip awscliv2.zip Note When updating from a previous version, the unzip command prompts to overwrite\n                                                existing files. To skip these prompts, such as with\n                                                script automation, use the -u update\n                                                flag for unzip . This flag automatically\n                                                updates existing files and creates new ones as\n                                                needed. $ unzip -u awscliv2.zip Run the install program. The installation command uses\n                                            a file named install in the newly\n                                            unzipped aws directory. By default,\n                                            the files are all installed to /usr/local/aws-cli , and a\n                                            symbolic link is created in /usr/local/bin . The command\n                                            includes sudo to grant write permissions to\n                                            those directories. $ sudo ./aws/install You can install without sudo if you\n                                            specify directories that you already have write\n                                            permissions to. Use the following instructions for the install command to specify the\n                                            installation location: Ensure that the paths you provide to the -i and -b parameters\n                                                  contain no volume name or directory names that\n                                                  contain any space characters or other white space\n                                                  characters. If there is a space, the installation\n                                                  fails. --install-dir or -i \u2013 This option specifies the directory to\n                                                  copy all of the files to. The default value is /usr/local/aws-cli . --bin-dir or -b \u2013 This option specifies that the main aws program in the install directory\n                                                  is symbolically linked to the file aws in the specified path.\n                                                  You must have write permissions to the specified\n                                                  directory. Creating a symlink to a directory that\n                                                  is already in your path eliminates the need to add\n                                                  the install directory to the user's $PATH variable. The default value is /usr/local/bin . $ ./aws/install -i /usr/ local /aws-cli -b /usr/ local /bin Note To update your current installation of the AWS CLI,\n                                                add your existing symlink and installer information\n                                                to construct the install command with\n                                                the --update parameter. $ sudo ./aws/install --bin-dir /usr/ local /bin --install-dir /usr/ local /aws-cli --update To locate the existing symlink and installation\n                                                directory, use the following steps: Use the which command to find\n                                                  your symlink. This gives you the path to use with\n                                                  the --bin-dir parameter. $ which aws /usr/ local /bin /aws Use the ls command to find the\n                                                  directory that your symlink points to. This gives\n                                                  you the path to use with the --install-dir parameter. $ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -> /usr/ local /aws-cli /v2/current/bin/aws Confirm the installation with the following command. $ aws --version aws-cli/2.17.20 Python/3.11.6 Linux/5.10.205-195.807.amzn2.x86_64 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Install and update\n                            requirements We support the AWS CLI on macOS versions 10.15 and later. For more\n                                information, see macOS support policy updates for the AWS CLI v2 on the AWS Developer Tools\n                                Blog . Because AWS doesn't maintain third-party repositories, we can\u2019t\n                                guarantee that they contain the latest version of the AWS CLI. Install or update the\n                            AWS CLI If you are updating to the latest version, use the same installation\n                        method that you used in your current version. You can install the AWS CLI on\n                        macOS in the following ways. GUI installer The following steps show how to install the latest version\n                                        of the AWS CLI by using the standard macOS user interface and\n                                        your browser. In your browser, download the macOS pkg file: https://awscli.amazonaws.com/AWSCLIV2.pkg Run your downloaded file and follow the on-screen\n                                            instructions. You can choose to install the AWS CLI in the\n                                            following ways: For all users on the\n                                                  computer (requires sudo ) You can install to any folder, or choose the\n                                                  recommended default folder of /usr/local/aws-cli . The installer automatically creates a\n                                                  symlink at /usr/local/bin/aws that\n                                                  links to the main program in the installation\n                                                  folder you chose. For only the current\n                                                  user (doesn't require sudo ) You can install to any folder to which you\n                                                  have write permission. Due to standard user permissions, after the\n                                                  installer finishes, you must manually create a\n                                                  symlink file in your $PATH that\n                                                  points to the aws and aws_completer programs by using the\n                                                  following commands at the command prompt. If your $PATH includes a folder you can write\n                                                  to, you can run the following command without sudo if you specify that folder as\n                                                  the target's path. If you don't have a writable\n                                                  folder in your $PATH , you must use sudo in the commands to get\n                                                  permissions to write to the specified target\n                                                  folder. The default location for a symlink is /usr/local/bin/ . $ sudo ln -s / folder/installed /aws-cli/aws / usr/local/bin /aws $ sudo ln -s / folder/installed /aws-cli/aws_completer / usr/local/bin /aws_completer Note You can view debug logs for the installation by\n                                                pressing Cmd+L anywhere in the installer. This opens a log pane\n                                                that enables you to filter and save the log. The log\n                                                file is also automatically saved to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Command line installer - All users If you have sudo permissions, you can install the\n                                    AWS CLI for all users on the computer. We provide the steps in one\n                                    easy to copy and paste group. See the descriptions of each line\n                                    in the following steps. $ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" $ sudo installer -pkg AWSCLIV2.pkg -target / Guided installation instructions Download the file using the curl command.\n                                            The -o option specifies the file name that\n                                            the downloaded package is written to. In this example,\n                                            the file is written to AWSCLIV2.pkg in the\n                                            current folder. $ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" Run the standard macOS installer program,\n                                            specifying the downloaded .pkg file\n                                            as the source. Use the -pkg parameter to\n                                            specify the name of the package to install, and the -target / parameter for which drive to\n                                            install the package to. The files are installed to /usr/local/aws-cli , and a\n                                            symlink is automatically created in /usr/local/bin . You must\n                                            include sudo on the command to grant write\n                                            permissions to those folders. $ sudo installer -pkg ./AWSCLIV2.pkg -target / After installation is complete, debug logs are written\n                                            to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Command line - Current user To specify which folder the AWS CLI is installed to, you\n                                            must create an XML file with any file name. This file is\n                                            an XML-formatted file that looks like the following\n                                            example. Leave all values as shown, except you must\n                                            replace the path /Users/myusername in line\n                                            9 with the path to the folder you want the AWS CLI\n                                            installed to. The folder must\n                                                already exist, or the command fails. The\n                                            following XML example, named choices.xml , specifies\n                                            the installer to install the AWS CLI in the folder /Users/myusername , where it\n                                            creates a folder named aws-cli . <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\" > < plist version = \"1.0\" > < array > < dict > < key > choiceAttribute </ key > < string > customLocation </ string > < key > attributeSetting </ key > < string > /Users/myusername </ string > < key > choiceIdentifier </ key > < string > default </ string > </ dict > </ array > </ plist > Download the pkg installer using\n                                            the curl command. The -o option specifies the file name that the downloaded\n                                            package is written to. In this example, the file is\n                                            written to AWSCLIV2.pkg in the current folder. $ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" Run the standard macOS installer program\n                                            with the following options: Specify the name of the package to install by\n                                                  using the -pkg parameter. Specify installing to a current user only by\n                                                  setting the -target parameter to CurrentUserHomeDirectory . Specify the path (relative to the current\n                                                  folder) and name of the XML file that you created\n                                                  in the -applyChoiceChangesXML parameter. The following example installs the AWS CLI in the folder /Users/myusername/aws-cli . $ installer -pkg AWSCLIV2.pkg \\\n            -target CurrentUserHomeDirectory \\\n            -applyChoiceChangesXML choices.xml Because standard user permissions typically don't\n                                            allow writing to folders in your $PATH , the\n                                            installer in this mode doesn't try to add the symlinks\n                                            to the aws and aws_completer programs. For the AWS CLI to run correctly, you must\n                                            manually create the symlinks after the installer\n                                            finishes. If your $PATH includes a folder\n                                            you can write to and you specify the folder as the\n                                            target's path, you can run the following command without sudo . If you don't have a writable\n                                            folder in your $PATH , you must use sudo for permissions to write to the\n                                            specified target folder. The default location for a\n                                            symlink is /usr/local/bin/ . Replace folder/installed with the path to your\n                                            AWS CLI installation. $ sudo ln -s / folder/installed /aws-cli/aws / usr/local/bin /aws $ sudo ln -s / folder/installed /aws-cli/aws_completer / usr/local/bin /aws_completer After installation is complete, debug logs are written\n                                            to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . macOS Install and update\n                            requirements We support the AWS CLI on macOS versions 10.15 and later. For more\n                                information, see macOS support policy updates for the AWS CLI v2 on the AWS Developer Tools\n                                Blog . Because AWS doesn't maintain third-party repositories, we can\u2019t\n                                guarantee that they contain the latest version of the AWS CLI. Install or update the\n                            AWS CLI If you are updating to the latest version, use the same installation\n                        method that you used in your current version. You can install the AWS CLI on\n                        macOS in the following ways. GUI installer The following steps show how to install the latest version\n                                        of the AWS CLI by using the standard macOS user interface and\n                                        your browser. In your browser, download the macOS pkg file: https://awscli.amazonaws.com/AWSCLIV2.pkg Run your downloaded file and follow the on-screen\n                                            instructions. You can choose to install the AWS CLI in the\n                                            following ways: For all users on the\n                                                  computer (requires sudo ) You can install to any folder, or choose the\n                                                  recommended default folder of /usr/local/aws-cli . The installer automatically creates a\n                                                  symlink at /usr/local/bin/aws that\n                                                  links to the main program in the installation\n                                                  folder you chose. For only the current\n                                                  user (doesn't require sudo ) You can install to any folder to which you\n                                                  have write permission. Due to standard user permissions, after the\n                                                  installer finishes, you must manually create a\n                                                  symlink file in your $PATH that\n                                                  points to the aws and aws_completer programs by using the\n                                                  following commands at the command prompt. If your $PATH includes a folder you can write\n                                                  to, you can run the following command without sudo if you specify that folder as\n                                                  the target's path. If you don't have a writable\n                                                  folder in your $PATH , you must use sudo in the commands to get\n                                                  permissions to write to the specified target\n                                                  folder. The default location for a symlink is /usr/local/bin/ . $ sudo ln -s / folder/installed /aws-cli/aws / usr/local/bin /aws $ sudo ln -s / folder/installed /aws-cli/aws_completer / usr/local/bin /aws_completer Note You can view debug logs for the installation by\n                                                pressing Cmd+L anywhere in the installer. This opens a log pane\n                                                that enables you to filter and save the log. The log\n                                                file is also automatically saved to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Command line installer - All users If you have sudo permissions, you can install the\n                                    AWS CLI for all users on the computer. We provide the steps in one\n                                    easy to copy and paste group. See the descriptions of each line\n                                    in the following steps. $ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" $ sudo installer -pkg AWSCLIV2.pkg -target / Guided installation instructions Download the file using the curl command.\n                                            The -o option specifies the file name that\n                                            the downloaded package is written to. In this example,\n                                            the file is written to AWSCLIV2.pkg in the\n                                            current folder. $ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" Run the standard macOS installer program,\n                                            specifying the downloaded .pkg file\n                                            as the source. Use the -pkg parameter to\n                                            specify the name of the package to install, and the -target / parameter for which drive to\n                                            install the package to. The files are installed to /usr/local/aws-cli , and a\n                                            symlink is automatically created in /usr/local/bin . You must\n                                            include sudo on the command to grant write\n                                            permissions to those folders. $ sudo installer -pkg ./AWSCLIV2.pkg -target / After installation is complete, debug logs are written\n                                            to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Command line - Current user To specify which folder the AWS CLI is installed to, you\n                                            must create an XML file with any file name. This file is\n                                            an XML-formatted file that looks like the following\n                                            example. Leave all values as shown, except you must\n                                            replace the path /Users/myusername in line\n                                            9 with the path to the folder you want the AWS CLI\n                                            installed to. The folder must\n                                                already exist, or the command fails. The\n                                            following XML example, named choices.xml , specifies\n                                            the installer to install the AWS CLI in the folder /Users/myusername , where it\n                                            creates a folder named aws-cli . <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\" > < plist version = \"1.0\" > < array > < dict > < key > choiceAttribute </ key > < string > customLocation </ string > < key > attributeSetting </ key > < string > /Users/myusername </ string > < key > choiceIdentifier </ key > < string > default </ string > </ dict > </ array > </ plist > Download the pkg installer using\n                                            the curl command. The -o option specifies the file name that the downloaded\n                                            package is written to. In this example, the file is\n                                            written to AWSCLIV2.pkg in the current folder. $ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" Run the standard macOS installer program\n                                            with the following options: Specify the name of the package to install by\n                                                  using the -pkg parameter. Specify installing to a current user only by\n                                                  setting the -target parameter to CurrentUserHomeDirectory . Specify the path (relative to the current\n                                                  folder) and name of the XML file that you created\n                                                  in the -applyChoiceChangesXML parameter. The following example installs the AWS CLI in the folder /Users/myusername/aws-cli . $ installer -pkg AWSCLIV2.pkg \\\n            -target CurrentUserHomeDirectory \\\n            -applyChoiceChangesXML choices.xml Because standard user permissions typically don't\n                                            allow writing to folders in your $PATH , the\n                                            installer in this mode doesn't try to add the symlinks\n                                            to the aws and aws_completer programs. For the AWS CLI to run correctly, you must\n                                            manually create the symlinks after the installer\n                                            finishes. If your $PATH includes a folder\n                                            you can write to and you specify the folder as the\n                                            target's path, you can run the following command without sudo . If you don't have a writable\n                                            folder in your $PATH , you must use sudo for permissions to write to the\n                                            specified target folder. The default location for a\n                                            symlink is /usr/local/bin/ . Replace folder/installed with the path to your\n                                            AWS CLI installation. $ sudo ln -s / folder/installed /aws-cli/aws / usr/local/bin /aws $ sudo ln -s / folder/installed /aws-cli/aws_completer / usr/local/bin /aws_completer After installation is complete, debug logs are written\n                                            to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . anchor anchor anchor GUI installer Command line installer - All users Command line - Current user The following steps show how to install the latest version\n                                        of the AWS CLI by using the standard macOS user interface and\n                                        your browser. In your browser, download the macOS pkg file: https://awscli.amazonaws.com/AWSCLIV2.pkg Run your downloaded file and follow the on-screen\n                                            instructions. You can choose to install the AWS CLI in the\n                                            following ways: For all users on the\n                                                  computer (requires sudo ) You can install to any folder, or choose the\n                                                  recommended default folder of /usr/local/aws-cli . The installer automatically creates a\n                                                  symlink at /usr/local/bin/aws that\n                                                  links to the main program in the installation\n                                                  folder you chose. For only the current\n                                                  user (doesn't require sudo ) You can install to any folder to which you\n                                                  have write permission. Due to standard user permissions, after the\n                                                  installer finishes, you must manually create a\n                                                  symlink file in your $PATH that\n                                                  points to the aws and aws_completer programs by using the\n                                                  following commands at the command prompt. If your $PATH includes a folder you can write\n                                                  to, you can run the following command without sudo if you specify that folder as\n                                                  the target's path. If you don't have a writable\n                                                  folder in your $PATH , you must use sudo in the commands to get\n                                                  permissions to write to the specified target\n                                                  folder. The default location for a symlink is /usr/local/bin/ . $ sudo ln -s / folder/installed /aws-cli/aws / usr/local/bin /aws $ sudo ln -s / folder/installed /aws-cli/aws_completer / usr/local/bin /aws_completer Note You can view debug logs for the installation by\n                                                pressing Cmd+L anywhere in the installer. This opens a log pane\n                                                that enables you to filter and save the log. The log\n                                                file is also automatically saved to /var/log/install.log . To verify that the shell can find and run the aws command in your $PATH ,\n                                            use the following commands. $ which aws /usr/local/bin/aws $ aws --version aws-cli/2.17.20 Python/3.11.6 Darwin/23.3.0 botocore/2.4.5 If the aws command cannot be found, you\n                                            might need to restart your terminal or follow the\n                                            troubleshooting in Troubleshooting errors for the AWS CLI . Install and update\n                            requirements We support the AWS CLI on Microsoft-supported versions of 64-bit\n                                Windows. Admin rights to install software Install or update the\n                            AWS CLI To update your current installation of AWS CLI on Windows, download a new\n                        installer each time you update to overwrite previous versions. AWS CLI is\n                        updated regularly. To see when the latest version was released, see the AWS CLI version 2 Changelog on GitHub . Download and run the AWS CLI MSI installer for Windows\n                                (64-bit): https://awscli.amazonaws.com/AWSCLIV2.msi Alternatively, you can run the msiexec command to run\n                                the MSI installer. C:\\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi For various parameters that can be used with msiexec ,\n                                see msiexec on the Microsoft\n                                    Docs website. For example, you can use the /qn flag for a silent installation. C:\\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi /qn To confirm the installation, open the Start menu, search for cmd to open a command prompt window,\n                                and at the command prompt use the aws --version command. C:\\> aws --version aws-cli/2.17.20 Python/3.11.6 Windows/10 exe/AMD64 prompt/off If Windows is unable to find the program, you might need to close\n                                and reopen the command prompt window to refresh the path, or follow\n                                the troubleshooting in Troubleshooting errors for the AWS CLI . Windows Install and update\n                            requirements We support the AWS CLI on Microsoft-supported versions of 64-bit\n                                Windows. Admin rights to install software Install or update the\n                            AWS CLI To update your current installation of AWS CLI on Windows, download a new\n                        installer each time you update to overwrite previous versions. AWS CLI is\n                        updated regularly. To see when the latest version was released, see the AWS CLI version 2 Changelog on GitHub . Download and run the AWS CLI MSI installer for Windows\n                                (64-bit): https://awscli.amazonaws.com/AWSCLIV2.msi Alternatively, you can run the msiexec command to run\n                                the MSI installer. C:\\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi For various parameters that can be used with msiexec ,\n                                see msiexec on the Microsoft\n                                    Docs website. For example, you can use the /qn flag for a silent installation. C:\\> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi /qn To confirm the installation, open the Start menu, search for cmd to open a command prompt window,\n                                and at the command prompt use the aws --version command. C:\\> aws --version aws-cli/2.17.20 Python/3.11.6 Windows/10 exe/AMD64 prompt/off If Windows is unable to find the program, you might need to close\n                                and reopen the command prompt window to refresh the path, or follow\n                                the troubleshooting in Troubleshooting errors for the AWS CLI . Troubleshooting AWS CLI install and uninstall\n                errors If you come across issues after installing or uninstalling the AWS CLI, see Troubleshooting errors for the AWS CLI for troubleshooting steps. For the most\n            relevant troubleshooting steps, see Command not found errors , The \"aws --version\" command\n                returns a different version than you installed , and The \"aws --version\" command returns a\n                version after uninstalling the AWS CLI . Next steps After you successfully install the AWS CLI, you can safely delete your downloaded\n            installer files. After completing the steps in Prerequisites to use the AWS CLI version 2 and installing the AWS CLI, you should perform a Setting up the AWS CLI . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Prerequisites Past releases"}, {"title": "Configuring settings for the AWS CLI - AWS Command Line Interface", "url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html", "content": "Configuring settings for the AWS CLI PDF RSS This section explains how to configure the settings that the AWS Command Line Interface (AWS CLI) uses to\n        interact with AWS. These include the following: Credentials identify who is calling the API.\n                Access credentials are used to encrypt the request to the AWS servers to confirm\n                your identity and retrieve associated permissions policies. These permissions\n                determine the actions you can perform. For information on setting up your\n                credentials, see Authentication and access credentials for the\n      AWS CLI . Other configuration details to tell the AWS CLI how\n                to process requests, such as the default output format and the default AWS\n                Region. Note AWS requires that all incoming requests are cryptographically signed. The AWS CLI does\n            this for you. The \"signature\" includes a date/time stamp. Therefore, you must ensure\n            that your computer's date and time are set correctly. If you don't, and the date/time in\n            the signature is too far off of the date/time recognized by the AWS service, AWS\n            rejects the request. Configuration and credentials precedence Credentials and configuration settings are located in multiple places, such as the\n            system or user environment variables, local AWS configuration files, or explicitly\n            declared on the command line as a parameter. Certain locations take precedence over\n            others. The AWS CLI credentials and configuration settings take precedence in the\n            following order: Command line\n        options \u2013 Overrides settings in any other location, such as the --region , --output , and --profile parameters. Environment\n        variables \u2013 You can store values in your system's environment\n      variables. Assume role \u2013 Assume the permissions of an IAM role through configuration or the aws sts assume-role command. Assume role with web\n          identity \u2013 Assume the permissions of an IAM role using web\n      identity through configuration or the aws sts assume-role command. AWS IAM Identity Center \u2013 The\n      IAM Identity Center configuration settings stored in the config file are updated when you run\n      the aws configure sso command. Credentials are then authenticated when you run\n      the aws sso login command. The config file is located at ~/.aws/config on Linux or macOS, or at C:\\Users\\ USERNAME \\.aws\\config on Windows. Credentials\n        file \u2013 The credentials and config file are\n      updated when you run the command aws configure . The credentials file\n      is located at ~/.aws/credentials on Linux or macOS, or at C:\\Users\\ USERNAME \\.aws\\credentials on\n      Windows. Custom\n          process \u2013 Get your credentials from an external source. Configuration\n        file \u2013 The credentials and config file are\n      updated when you run the command aws configure . The config file is\n      located at ~/.aws/config on Linux or macOS, or at C:\\Users\\ USERNAME \\.aws\\config on\n      Windows. Container\n          credentials \u2013 You can associate an IAM role with each of your\n      Amazon Elastic Container Service (Amazon ECS) task definitions. Temporary credentials for that role are then available to\n      that task's containers. For more information, see IAM Roles for Tasks in the Amazon Elastic Container Service Developer Guide . Amazon EC2\n          instance profile credentials \u2013 You can associate an IAM role\n      with each of your Amazon Elastic Compute Cloud (Amazon EC2) instances. Temporary credentials for that role are then\n      available to code running in the instance. The credentials are delivered through the Amazon EC2\n      metadata service. For more information, see IAM Roles for Amazon EC2 in the Amazon EC2 User Guide and Using Instance\n        Profiles in the IAM User Guide . Additional topics in this section Configuration and credential file settings in the\n            AWS CLI Configuring environment variables for the\n            AWS CLI Command line options in the AWS CLI Configuring command completion in the AWS CLI AWS CLI retries in the AWS CLI Using an HTTP proxy for the AWS CLI Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Setup Configuration and credential file settings in the\n            AWS CLI"}, {"title": "AWS Cloud Development Kit Documentation", "url": "https://docs.aws.amazon.com/cdk/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for C++ Documentation", "url": "https://docs.aws.amazon.com/sdk-for-cpp/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for Go Documentation", "url": "https://docs.aws.amazon.com/sdk-for-go/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for Java Documentation", "url": "https://docs.aws.amazon.com/sdk-for-java/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for JavaScript Documentation", "url": "https://docs.aws.amazon.com/sdk-for-javascript/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for Kotlin Documentation", "url": "https://docs.aws.amazon.com/sdk-for-kotlin/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for .NET documentation", "url": "https://docs.aws.amazon.com/sdk-for-net/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for PHP Documentation", "url": "https://docs.aws.amazon.com/sdk-for-php/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for Python (Boto3) Documentation", "url": "https://docs.aws.amazon.com/pythonsdk/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for Rust Documentation", "url": "https://docs.aws.amazon.com/sdk-for-rust/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS SDK for Swift Documentation", "url": "https://docs.aws.amazon.com/sdk-for-swift/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS Toolkit for Microsoft Azure DevOps Documentation", "url": "https://docs.aws.amazon.com/vsts/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS Tools for PowerShell Documentation", "url": "https://docs.aws.amazon.com/powershell/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS Toolkit for Visual Studio Documentation", "url": "https://docs.aws.amazon.com/aws-toolkit-visual-studio/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "AWS Toolkit for Visual Studio Code Documentation", "url": "https://docs.aws.amazon.com/toolkit-for-vscode/?icmpid=docs_homepage_sdktoolkits", "content": "No main content found."}, {"title": "What is the code example library? - AWS SDK Code Examples", "url": "https://docs.aws.amazon.com/code-library/latest/ug/what-is-code-library.html", "content": "There are more AWS SDK examples available in the AWS Doc SDK Examples GitHub repo. There are more AWS SDK examples available in the AWS Doc SDK Examples GitHub repo. What is the code example library? PDF The code example library is a collection of code examples that show you how to use AWS software development kits (SDKs) with AWS. The examples are organized by AWS service and by AWS SDK. The same examples can be found in each section. Code examples by service \u2013 A list of AWS services that contain examples of how to use each service with AWS SDKs. \n    Use this section if you know which service you want to use. Code examples by SDK \u2013 A list of AWS SDKs that contain examples of how to use AWS services with each SDK. \n    Use this section if you know which SDK you want to use. Within each section, the examples are divided into the following categories: Basics are code examples that show you how to perform the essential operations within a service. Actions are code excerpts that show you how to call individual service functions. Scenarios are code examples that show you how to accomplish specific tasks by\n      calling multiple functions within a service or combined with other AWS services. Additional example categories are defined for some services. These code examples show you something specialized about a service or integration. All of the examples in this library can also be found in the AWS Code Examples GitHub repository .\n  The GitHub repository also contains instructions on how to set up, run, and test the examples. Using AWS services with an AWS SDK AWS software development kits (SDKs) are available for many popular programming languages. Each SDK provides an API, code examples, and documentation that \n        make it easier for developers to build applications in their preferred language. SDK documentation Code examples AWS SDK for C++ AWS SDK for C++ code examples AWS CLI AWS CLI code examples AWS SDK for Go AWS SDK for Go code examples AWS SDK for Java AWS SDK for Java code examples AWS SDK for JavaScript AWS SDK for JavaScript code examples AWS SDK for Kotlin AWS SDK for Kotlin code examples AWS SDK for .NET AWS SDK for .NET code examples AWS SDK for PHP AWS SDK for PHP code examples AWS Tools for PowerShell Tools for PowerShell code examples AWS SDK for Python (Boto3) AWS SDK for Python (Boto3) code examples AWS SDK for Ruby AWS SDK for Ruby code examples AWS SDK for Rust AWS SDK for Rust code examples AWS SDK for SAP ABAP AWS SDK for SAP ABAP code examples AWS SDK for Swift AWS SDK for Swift code examples Example availability Can't find what you need? Request a code example by using the Provide feedback link at the bottom of this page. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Code examples by service"}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#welcome", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#products", "content": "No main content found."}, {"title": "Welcome to AWS Documentation", "url": "https://docs.aws.amazon.com/#developer_resources", "content": "No main content found."}]